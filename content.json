{"meta":{"title":"TwinTower Tech","subtitle":"人工智能 | AI","description":"梦想.自信.成就","author":"双子塔科技","url":"https://steven-cheng-com.github.io","root":"/"},"pages":[{"title":"关于我们","date":"2019-03-06T14:08:40.000Z","updated":"2022-05-01T06:40:13.370Z","comments":true,"path":"about/index.html","permalink":"https://steven-cheng-com.github.io/about/index.html","excerpt":"","text":"我们是谁双子塔科技于2021年8月成立于武汉，我们是一家由技术和人文驱动的科技公司。目前，我们提供互联网+留学、人工智能+取名等服务，希望通过大数据和人工智能为每一位顾客提供最合理最优质的解决方案。 联系方式 地址: 中国湖北省武汉市武昌区和平大道1004号 电话: +86-135-5469-3349 Email: mystudy.max@gmail.com 微信: Twin-Tower-Tech 微信公众号: 一分钟英文 小红书: 一分钟英文 B站: 一分钟英文 YouTube: 一分钟英文 微信公众号: 大马留学僧 小红书: 大马留学僧 B站: 大马留学僧 YouTube: 大马留学僧"},{"title":"服务","date":"2022-01-01T11:20:07.000Z","updated":"2022-05-01T06:31:48.675Z","comments":true,"path":"service/index.html","permalink":"https://steven-cheng-com.github.io/service/index.html","excerpt":"","text":"1.人工智能取名服务简介 客户只需要提供中文姓名、性别和寓意，我们双子塔科技就可以通过AI模型和NLP语料数据提供智能取名服务，定制个性小众化的英文名，并提供英文名相对应的中文译名、名字解释、来源、其他写法和同名名人。 序号 英文名字 中文译名 名字解释 来源 其他写法 同名名人 1 Celine 席琳 Celine的中文译名为席琳，有天堂、天空的意思，一般用作女生名字。 Celine源自德文 Selene Celine Yeung (杨鎧凝) 2 Hanna 汉娜 Hanna的中文译名为汉娜，有和蔼可亲、上帝是恩慈的意思，一般用作女生名。 源自希伯来语 Hana,Hannah Hanna Kuk (菊梓乔) 3 Phoebe 菲比 Phoebe的中文译名为菲比，有明亮、耀眼的意思，一般用作女生名字。 Phoebe源自希腊文 Bee,Fee,Fifi Phoebe Chow (周梓盈) 更多内容 最新视频 联系方式 微信：Twin-Tower-Tech 微信公众号: 一分钟英文 小红书: 一分钟英文 B站: 一分钟英文 YouTube: 一分钟英文 2.互联网+留学服务简介 我们与马来西亚StuMax研究院（StuMax Academy）于2020年12月1日创立了学习大师（StudyMax）教育平台，之后于2021年8月11日又在中国武汉成立了双子塔科技公司。目前，StuMax Academy主要负责提供学校数据以及联系学校等，双子塔科技主要负责在中国提供留学招生、留学咨询、代理申请等服务，通过大数据和人工智能为每一位学生匹配最优的学习方案和教育模式。 学校资料 目前提供留学服务的国家包括但不限于：澳大利亚、加拿大、马来西亚、新加坡、英国、美国等国家。 留学国家 澳大利亚留学 加拿大留学 马来西亚留学 新加坡留学 英国留学 美国留学 课程资料 目前提供留学服务的课程项目包括：大学预科文凭、本科学位、硕士学位、博士学位等。 课程项目 大学预科文凭 本科学位 硕士学位 博士学位 成功案例 截至2022年4月，留学申请的成功案例已超过100多例，涵盖本科、硕士研究生和博士研究生。 感谢留言 本科案例 硕士案例 博士案例 服务形式 1）使用AI预测被名校录取的概率通过输入你的一些基础信息（例如，学校水平、学科专业、在校成绩、工作经验、学术成果、课外活动、语言成绩等），让AI（Artificial Intelligence）模型预测你被名校或许某某学校录取的概率，帮你提前规划留学，挑选合适的录取机会大的学校，减少无用功。 2）使用数据科学帮你分析背景另外，借助数据科学（Data Science）技术的帮助， 可以全面分析学生的背景，从而凸显学生的优势，弥补缺陷，深度挖掘学生的潜力，争取拿到理想学校的入场券。 3）结合预测和数据分析帮你决策结合AI模型获得的录取概率和数据科学分析的结果，在进一步完善学生各方面的背景之后，我们帮学生做出最佳的留学决策。 4）互联网+在线服务通过互联网各平台提供每周7天24h实时在线的服务模式，以低碳环保、高效快速、专业靠谱的服务方式，帮助莘莘学子实现留学梦想！ 联系方式 Email: mystudy.max@gmail.com 电话: +86-13554693349 微信：Twin-Tower-Tech 微信公众号: 大马留学僧 小红书: 大马留学僧 B站: 大马留学僧 YouTube: 大马留学僧"}],"posts":[{"title":"欧美10大男歌手的英文名都有什么含义？","slug":"欧美10大男歌手的英文名都有什么含义？","date":"2022-05-01T07:22:27.000Z","updated":"2022-05-01T07:28:37.001Z","comments":true,"path":"2022/05/01/欧美10大男歌手的英文名都有什么含义？/","link":"","permalink":"https://steven-cheng-com.github.io/2022/05/01/%E6%AC%A7%E7%BE%8E10%E5%A4%A7%E7%94%B7%E6%AD%8C%E6%89%8B%E7%9A%84%E8%8B%B1%E6%96%87%E5%90%8D%E9%83%BD%E6%9C%89%E4%BB%80%E4%B9%88%E5%90%AB%E4%B9%89%EF%BC%9F/","excerpt":"","text":"&nbsp; Twin-Tower Tech 喜欢听英文歌的你，肯定多少了解过一些著名的欧美男歌手。例如，Michael Jackson、Elvis Presle、Justin Bieber、Bruno Mars等，今天将介绍欧美10大男歌手，以及他们的英文名所代表的含义。不过，有些歌手可能比较有年代感。 视频","categories":[{"name":"取名","slug":"取名","permalink":"https://steven-cheng-com.github.io/categories/%E5%8F%96%E5%90%8D/"}],"tags":[{"name":"英语","slug":"英语","permalink":"https://steven-cheng-com.github.io/tags/%E8%8B%B1%E8%AF%AD/"},{"name":"英文名字","slug":"英文名字","permalink":"https://steven-cheng-com.github.io/tags/%E8%8B%B1%E6%96%87%E5%90%8D%E5%AD%97/"},{"name":"取名","slug":"取名","permalink":"https://steven-cheng-com.github.io/tags/%E5%8F%96%E5%90%8D/"}],"author":"Twin-Tower Tech"},{"title":"欧美10大女歌手的英文名都有什么含义？","slug":"欧美10大女歌手的英文名都有什么含义？","date":"2022-05-01T07:01:42.000Z","updated":"2022-05-01T07:14:32.114Z","comments":true,"path":"2022/05/01/欧美10大女歌手的英文名都有什么含义？/","link":"","permalink":"https://steven-cheng-com.github.io/2022/05/01/%E6%AC%A7%E7%BE%8E10%E5%A4%A7%E5%A5%B3%E6%AD%8C%E6%89%8B%E7%9A%84%E8%8B%B1%E6%96%87%E5%90%8D%E9%83%BD%E6%9C%89%E4%BB%80%E4%B9%88%E5%90%AB%E4%B9%89%EF%BC%9F/","excerpt":"","text":"&nbsp; Twin-Tower Tech 视频","categories":[{"name":"取名","slug":"取名","permalink":"https://steven-cheng-com.github.io/categories/%E5%8F%96%E5%90%8D/"}],"tags":[{"name":"英语","slug":"英语","permalink":"https://steven-cheng-com.github.io/tags/%E8%8B%B1%E8%AF%AD/"},{"name":"英文名字","slug":"英文名字","permalink":"https://steven-cheng-com.github.io/tags/%E8%8B%B1%E6%96%87%E5%90%8D%E5%AD%97/"},{"name":"取名","slug":"取名","permalink":"https://steven-cheng-com.github.io/tags/%E5%8F%96%E5%90%8D/"}],"author":"Twin-Tower Tech"},{"title":"Blackpink成员的英文名你都了解吗？","slug":"Blackpink成员的英文名你都了解吗？","date":"2022-03-09T09:49:31.000Z","updated":"2022-03-09T09:52:51.479Z","comments":true,"path":"2022/03/09/Blackpink成员的英文名你都了解吗？/","link":"","permalink":"https://steven-cheng-com.github.io/2022/03/09/Blackpink%E6%88%90%E5%91%98%E7%9A%84%E8%8B%B1%E6%96%87%E5%90%8D%E4%BD%A0%E9%83%BD%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F/","excerpt":"","text":"&nbsp; Twin-Tower Tech 1.Roseanne英文名字:Roseanne 中文译名:罗森娜名字含义:Roseanne有玫瑰、浪漫、优雅的意思，一般用作女生名字，源自英式英语，在《1968美国婴儿名字排行榜》中排名前1000。 2.Lisa英文名字:Lisa 中文译名:丽莎名字含义:Lisa有奉献的意思，一般用作女生名字，源自希伯来语，在《2019美国婴儿名字排行榜》中排名前1000。 3.Jennie英文名字：Jennie 中文译名:詹妮名字含义：有美貌、白浪的意思，一般用作女生名字，源自英式英语，在《1997美国婴儿名字排行榜》中排名前1000。","categories":[{"name":"取名","slug":"取名","permalink":"https://steven-cheng-com.github.io/categories/%E5%8F%96%E5%90%8D/"}],"tags":[{"name":"英语","slug":"英语","permalink":"https://steven-cheng-com.github.io/tags/%E8%8B%B1%E8%AF%AD/"},{"name":"英文名字","slug":"英文名字","permalink":"https://steven-cheng-com.github.io/tags/%E8%8B%B1%E6%96%87%E5%90%8D%E5%AD%97/"}],"author":"Twin-Tower Tech"},{"title":"免费学习人工智能最好的10个网站","slug":"免费学习人工智能最好的10个网站","date":"2021-12-27T12:18:32.000Z","updated":"2022-01-27T12:28:17.436Z","comments":true,"path":"2021/12/27/免费学习人工智能最好的10个网站/","link":"","permalink":"https://steven-cheng-com.github.io/2021/12/27/%E5%85%8D%E8%B4%B9%E5%AD%A6%E4%B9%A0%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%9C%80%E5%A5%BD%E7%9A%8410%E4%B8%AA%E7%BD%91%E7%AB%99/","excerpt":"","text":"&nbsp; Twin-Tower Tech 很多人对人工智能非常感兴趣，但是却不知道无从学起。接下来要介绍的10个网站，绝对是免费学习人工智能全世界最好最火的网站。 1.Analytics VidhyaAnalytics Vidhya为数据分析和数据科学的专业人员提供了一个基于社区的知识平台。该平台的目标是成为一个完整的门户网站，可满足数据科学专业人员的所有知识和职业需求。 2.KaggleGoogle LLC的子公司Kaggle是一个由数据科学家和机器学习从业人员组成的在线社区。Kaggle允许用户查找和发布数据集，在基于Web的数据科学环境中探索和构建模型，与其他数据科学家和机器学习工程师合作，并参加竞赛以解决数据科学挑战。 3.CourseraCoursera是由斯坦福大学计算机科学教授Andrew Ng和Daphne Koller于2012年创立的美国大规模开放在线课程提供商，提供大规模开放在线课程，包括专业、学位和大师课程。 4.UdacityUdacity是由塞巴斯蒂安·特伦（Sebastian Thrun）、大卫·斯塔文斯（David Stavens）和迈克·索科尔斯基（Mike Sokolsky）创立的美国营利性教育组织，提供大规模的在线公开课程。 5.DatacampDataCamp课程易于学习，对初学者友好且结构合理。对于那些以前没有数据科学或分析经验的人，这些课程是极好的学习选择。但是，对于具有丰富的R和Python编程经验的学生来说，这些课程可能太基础了。 6.edXedX是由哈佛大学和麻省理工学院创建的美国大规模开放在线课程提供商。它为全世界的学生团体提供涵盖广泛学科的在线大学水平课程，其中包括一些免费课程。它还根据人们如何使用其平台对学习进行研究。 7.UdemyUdemy是一家面向专业成年人和学生的美国大型开放式在线课程提供商。它由Eren Bali、Gagan Biyani和Oktay Caglar于2010年5月成立。截至2020年1月，该平台拥有超过3500万名学生和57,000名讲师，教授超过65种语言。 8.KDNuggetsKDnuggets是AI、分析、大数据、数据挖掘、数据科学和机器学习方面的领先网站，由Gregory Piatetsky-Shapiro和Matthew Mayo创立。 9.R-bloggersR-Blogger是关于授权博主以授权其他R语言用户的在线学习网站。R-Bloggers是博客的内容聚合者，这些博客作者写了有关R（语言）的博客。该网站可帮助R语言博客作者和用户联系并关注“ R 博客圈”。 10.FutureLearnFutureLearn是一个成立于2012年12月的数字教育平台，由英国The Open University和SEEK Ltd 共同拥有。FutureLearn是一个大规模开放式在线课程（MOOC）学习平台，截至2020年6月，该网站包括175个英国和国际合作伙伴，包括非大学合作伙伴。 最后以上10个在线网站，都是免费学习人工智能相关课程全球最火的10大网站。不要再睡了，赶紧起床学习！","categories":[{"name":"AI","slug":"AI","permalink":"https://steven-cheng-com.github.io/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://steven-cheng-com.github.io/tags/AI/"},{"name":"人工智能","slug":"人工智能","permalink":"https://steven-cheng-com.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"免费课程","slug":"免费课程","permalink":"https://steven-cheng-com.github.io/tags/%E5%85%8D%E8%B4%B9%E8%AF%BE%E7%A8%8B/"},{"name":"网站","slug":"网站","permalink":"https://steven-cheng-com.github.io/tags/%E7%BD%91%E7%AB%99/"}],"author":"Twin-Tower Tech"},{"title":"史上被滥用最多次的10个英文名！如何拥有个性而小众的英文名？","slug":"史上被滥用最多次的10个英文名！如何拥有个性而小众的英文名？","date":"2021-12-12T10:29:24.000Z","updated":"2022-01-27T12:23:21.940Z","comments":true,"path":"2021/12/12/史上被滥用最多次的10个英文名！如何拥有个性而小众的英文名？/","link":"","permalink":"https://steven-cheng-com.github.io/2021/12/12/%E5%8F%B2%E4%B8%8A%E8%A2%AB%E6%BB%A5%E7%94%A8%E6%9C%80%E5%A4%9A%E6%AC%A1%E7%9A%8410%E4%B8%AA%E8%8B%B1%E6%96%87%E5%90%8D%EF%BC%81%E5%A6%82%E4%BD%95%E6%8B%A5%E6%9C%89%E4%B8%AA%E6%80%A7%E8%80%8C%E5%B0%8F%E4%BC%97%E7%9A%84%E8%8B%B1%E6%96%87%E5%90%8D%EF%BC%9F/","excerpt":"","text":"&nbsp; Twin-Tower Tech 随着时代的发展，人类的生活方式也在慢慢走向个性化和定制化，重复而撞衫的东西会引起很多人的尴尬，很多人都想拥有属于自己的个性化的服装、发型和口味，也有不少人梦想拥有独特的人生和经历，而今天我们主要是讨论100年来被最多重复使用的10个英文名字。如果你的名字很不幸也在其中，那么可以考虑换一个个性而小众的英文名，可以在出国旅游、留学或者与外国人交流时使用。 史上被滥用最多次的10个英文名下表列出了过去100年（1921年至2020年）出生的男婴和女婴使用的10个最受欢迎的名字。这些久经流行的名字来自一个包含176,490,003名男性出生和171,530,100名女性出生的数据库。 不过，这些流行的英文名不一定每年都流行。例如，过去100年最受欢迎的男性婴儿名是James，出现了470万次左右；但2020年，James这个英文名在男性婴儿名的欢迎度排名中，仅排名第6。而100年来最受欢迎的女性婴儿英文名Mary没有出现在2020年最受欢迎的10大女性婴儿英文名中。 如何拥有个性而小众的英文名？ 如果你的英文名很不幸也在上面的名单中，或者你的英文名经常与其他人重名，那么你可以考虑一下，定制一个符合你的风格和希望的个性化的英文名字，除了突出你的独特之外，也是带着美好的寓意。 接下来分别举个男性和女性取英文名的例子。假如一个名叫“王二小”的男生，想取一个个性化的英文名。如果想要有“高大上”的寓意，可以取名为“Eliseo”；如果想要有“聪明”的寓意，可以取名为“Zell”，如果想要有“有钱”的寓意，可以取名为“Ryker”，如果想要有“帅”的寓意，可以取名为“Aaron”。 假如一个名叫“李翠翠”的女生，想取一个个性化的英文名。如果想要有“美丽”的寓意，可以取名为“Giselle”；如果想要有“幸福”的寓意，可以取名为“Zoya”，如果想要有“年轻”的寓意，可以取名为“Julian”，如果想要有“聪明”的寓意，可以取名为“Sophiel”。 上面只是列举了两个简单的例子，如果你也想定制一个个性化而小众的英文名，不妨扫扫下面的图片，我们会根据你提供的中文名、性别和希望的寓意，帮你智能化匹配和定制多个独特而深刻的英文名。 总之我们的取名系统使用的是人工智能NLP领域最新的深度神经网络算法，而且会定期更新和补充更多的姓名数据库，从而不断训练出更加精准的取名模型，为更多人取上更合适的英文名。","categories":[{"name":"AI","slug":"AI","permalink":"https://steven-cheng-com.github.io/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://steven-cheng-com.github.io/tags/AI/"},{"name":"人工智能","slug":"人工智能","permalink":"https://steven-cheng-com.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"取名字","slug":"取名字","permalink":"https://steven-cheng-com.github.io/tags/%E5%8F%96%E5%90%8D%E5%AD%97/"}],"author":"Twin-Tower Tech"},{"title":"Table Detection with Detectron2 & Mask R-CNN","slug":"table-detection-with-detectron2-n-maskrcnn","date":"2020-11-22T04:13:38.000Z","updated":"2021-12-12T07:56:36.005Z","comments":true,"path":"2020/11/22/table-detection-with-detectron2-n-maskrcnn/","link":"","permalink":"https://steven-cheng-com.github.io/2020/11/22/table-detection-with-detectron2-n-maskrcnn/","excerpt":"","text":"&nbsp; Steven Cheng Intro Detectron2 is Facebook AI Research’s new software system that implements state-of-the-art object detection algorithms. It is a ground-up rewrite of the previous version, Detectron, and it originates from Mask R-CNN. Table detection is a crucial step in many document analysis applications as tables are used for presenting essential information to the reader in a structured manner. It is a hard problem due to varying layouts and encodings of the tables. Researchers have proposed numerous techniques for table detection based on layout analysis of documents. Most of these techniques fail to generalize because they rely on hand engineered features which are not robust to layout variations. In this post, we propose a detectron2 based method for table detection. Why use detectron2? It is powered by the PyTorch deep learning framework. It Include more features such as panoptic segmentation, Densepose, Cascade R-CNN, rotated bounding boxes, PointRend, DeepLab, etc. It can be used as a library to support different projects on top of it. It trains very faster. The Models can be exported to torchscript format or caffe2 format for deployment. How to implement?The implemented CODE contains THREE parts: Create custom COCO dataset You can run the voc2coco.py script to generate a COCO data formatted JSON file. 1python voc2coco.py ./dataset/annotations ./dataset/coco/output.json Then you can run the following Jupyter notebook to visualize the coco annotations. COCO_Image_Viewer.ipynb Training 1python table_detect_train.py Evaluation 1python table_detect_test.py","categories":[{"name":"Computer-Vision","slug":"Computer-Vision","permalink":"https://steven-cheng-com.github.io/categories/Computer-Vision/"}],"tags":[{"name":"Object-Detection","slug":"Object-Detection","permalink":"https://steven-cheng-com.github.io/tags/Object-Detection/"},{"name":"Table-Detection","slug":"Table-Detection","permalink":"https://steven-cheng-com.github.io/tags/Table-Detection/"}],"author":"Steven Cheng"},{"title":"Grammatical-Error-Correction Sequence Tagging System with an Pretrained BERT-like Transformer Encoder","slug":"Grammatical-Error-Correction-with-The-Pretrained-BERT-Transformer-Encoder","date":"2020-10-18T18:19:41.000Z","updated":"2021-12-12T07:52:54.037Z","comments":true,"path":"2020/10/19/Grammatical-Error-Correction-with-The-Pretrained-BERT-Transformer-Encoder/","link":"","permalink":"https://steven-cheng-com.github.io/2020/10/19/Grammatical-Error-Correction-with-The-Pretrained-BERT-Transformer-Encoder/","excerpt":"","text":"&nbsp; Steven Cheng Intro Here a simple and efficient GEC (Grammatical Error Correction) sequence tagger using a Transformer encoder is introduced. It is pre-trained on synthetic data and then fine-tuned in two stages: first on errorful corpora, and second on a combination of errorful and error-free parallel corpora. In addition, a custom token-level transformation to map input tokens to target corrections is designed. The original paper can be found here. Thus the GEC sequence tagging system here consists of three training stages: pretraining on synthetic data, fine-tuning on an errorful parallel corpus, and finally, fine-tuning on a combination of errorful and error-free parallel corpora. The GEC sequence tagging system incorporates a pre-trained Transformer encoder, those encoders from XLNet and RoBERTaoutperform three other cutting-edge Transformer encoders (ALBERT, BERT, and GPT-2). Use CaseThe original code of the GEC sequence tagging model is here, more details of running code are also included.","categories":[{"name":"Natural-Language-Processing","slug":"Natural-Language-Processing","permalink":"https://steven-cheng-com.github.io/categories/Natural-Language-Processing/"}],"tags":[{"name":"Grammatical-Error-Correction","slug":"Grammatical-Error-Correction","permalink":"https://steven-cheng-com.github.io/tags/Grammatical-Error-Correction/"},{"name":"BERT","slug":"BERT","permalink":"https://steven-cheng-com.github.io/tags/BERT/"},{"name":"Transformers","slug":"Transformers","permalink":"https://steven-cheng-com.github.io/tags/Transformers/"}],"author":"Steven Cheng"},{"title":"Spelling Correction with Soft-Masked BERT","slug":"spelling-correction-with-soft-masked-bert","date":"2020-09-06T04:47:42.000Z","updated":"2021-12-12T07:56:08.201Z","comments":true,"path":"2020/09/06/spelling-correction-with-soft-masked-bert/","link":"","permalink":"https://steven-cheng-com.github.io/2020/09/06/spelling-correction-with-soft-masked-bert/","excerpt":"","text":"&nbsp; Steven Cheng Intro Sotf-Masked BERT is a novel neural architecture to address the aforementioned issue, which consists of a network for error detection and a network for error correction based on BERT, with the former being connected to the latter with what we call soft-masking technique. The method uses ‘Soft-Masked BERT’ is general, and it may be employed in other language detection-correction problems not just focusing on CSC (Chinese Spelling error Correction) domain as it’s proposed in the original paper. The Architecture of Soft-Masked BERTSoft-Masked BERT is composed of a detection network based on Bi-GRU and a correction network based on BERT. The detection network predicts the probabilities of errors and the correction network predicts the probabilities of error corrections, while the former passes its prediction results to the latter using soft masking. The Model first creates an embedding for each character in the input sentence, referred to as input embedding. Next, it takes the sequence of embeddings as input and outputs the probabilities of errors for the sequence of characters (embeddings) using the detection network. After that it calculates the weighted sum of the input embeddings and [MASK] embeddings weighted by the error probabilities. The calculated embeddings mask the likely errors in the sequence in a soft way. Then it takes the sequence of soft-masked embeddings as input and outputs the probabilities of error corrections using the correction network, which is a BERT model whose final layer consists of a softmax function for all characters. There is also a residual connection between the input embeddings and the embeddings at the final layer. DemoDifferent with the original Sort-Masked BERT paper running models on Chinese dataset, here we modify a bit of code and use it in the English dataset. Dataset The data that we will use for this project will be 20 popular books from Project Gutenberg. Prerequired packages 1pip install -r requirements.txt Parameters The length of each sentence is between 4 and 200. So, 12max_len = 32min_len = 2 code You can find the code on Github How to run? Prepare Data: 1python data_prepare.py Process Data: 1python data_process.py Train Models: 1python train.py Test Models: 1python test.py","categories":[{"name":"Natural-Language-Processing","slug":"Natural-Language-Processing","permalink":"https://steven-cheng-com.github.io/categories/Natural-Language-Processing/"}],"tags":[{"name":"BERT","slug":"BERT","permalink":"https://steven-cheng-com.github.io/tags/BERT/"},{"name":"Spelling-Correction","slug":"Spelling-Correction","permalink":"https://steven-cheng-com.github.io/tags/Spelling-Correction/"}],"author":"Steven Cheng"},{"title":"Spelling Correction with Python Spellchecker","slug":"spelling-correction-with-python-spellchecker","date":"2020-08-16T06:52:42.000Z","updated":"2021-12-12T07:55:50.168Z","comments":true,"path":"2020/08/16/spelling-correction-with-python-spellchecker/","link":"","permalink":"https://steven-cheng-com.github.io/2020/08/16/spelling-correction-with-python-spellchecker/","excerpt":"","text":"&nbsp; Steven Cheng Intro Spelling checking or spelling correction is a basic requirement in any text processing or analysis. The python package pyspellchecker provides us this feature to find the words that may have been mis-spelled and also suggest the possible corrections. pyspellchecker supports multiple languages including English, Spanish, German, French, and Portuguese. And it supports Python 3 and Python 2.7. pyspellchecker allows for the setting of the Levenshtein Distance to check. For longer words, it is highly recommended to use a distance of 1 and not the default 2. How to install?1pip install pyspellchecker How to use? With the default Word Frequency list 12345678910111213from spellchecker import SpellCheckerspell = SpellChecker()# find those words that may be misspelledmisspelled = spell.unknown(['let', 'us', 'wlak','on','the','groun'])for word in misspelled: # Get the one `most likely` answer print(spell.correction(word)) # Get a list of `likely` options print(spell.candidates(word)) Output: 1234group&#123;'group', 'ground', 'groan', 'grout', 'grown', 'groin'&#125;walk&#123;'flak', 'weak', 'walk'&#125; With the customized Word Frequency list You can add additional text to generate a more appropriate list for your use case. 12345678from spellchecker import SpellCheckerspell = SpellChecker() # loads default word frequency listspell.word_frequency.load_text_file('./word_frequency.txt')# if you just want to make sure some words are not flagged as misspelledspell.word_frequency.load_words(['microsoft', 'apple', 'google'])spell.known(['microsoft', 'google']) # will return both now! Set the distance parameter If the words that you wish to check are long, it is recommended to reduce the distance to 1. This can be accomplished either when initializing the spell check class or after the fact. 1234567from spellchecker import SpellCheckerspell = SpellChecker(distance=1) # set at initialization# do some work on longer wordsspell.distance = 2 # set the distance parameter back to the default Additional Methodscandidates(word): Returns a set of possible candidates for the misspelled word word_probability(word): The frequency of the given word out of all words in the frequency list correction(word): Returns the most probable result for the misspelled word known([words]): Returns those words that are in the word frequency list unknown([words]): Returns those words that are not in the frequency list","categories":[{"name":"Natural-Language-Processing","slug":"Natural-Language-Processing","permalink":"https://steven-cheng-com.github.io/categories/Natural-Language-Processing/"}],"tags":[{"name":"Spelling-Correction","slug":"Spelling-Correction","permalink":"https://steven-cheng-com.github.io/tags/Spelling-Correction/"},{"name":"Natural-Language-Processing","slug":"Natural-Language-Processing","permalink":"https://steven-cheng-com.github.io/tags/Natural-Language-Processing/"}],"author":"Steven Cheng"},{"title":"Spelling Correction with The Pretrained BERT Model","slug":"spelling-correction-with-pretrained-bert","date":"2020-07-18T02:47:52.000Z","updated":"2021-12-12T07:54:51.766Z","comments":true,"path":"2020/07/18/spelling-correction-with-pretrained-bert/","link":"","permalink":"https://steven-cheng-com.github.io/2020/07/18/spelling-correction-with-pretrained-bert/","excerpt":"","text":"&nbsp; Steven Cheng Intro BERT (Bidirectional Encoder Representations from Transformers) is published by researchers at Google AI Language. It has caused a stir in the Machine Learning community by presenting state-of-the-art results in a wide variety of NLP tasks, including Question Answering, Natural Language Inference, and others. BERT’s key technical innovation is applying the bidirectional training of Transformer, a popular attention model, to language modelling. This is in contrast to previous efforts which looked at a text sequence either from left to right or combined left-to-right and right-to-left training. The results show that a language model which is bidirectionally trained can have a deeper sense of language context and flow than single-direction language models. BERT makes use of Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. Transformer includes two separate mechanisms — an encoder that reads the text input and a decoder that produces a prediction for the task. As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it’s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word). Demo Import necessary libraris 12345678910import reimport nltkimport numpy as npfrom PIL import Imageimport matplotlib.pyplot as pltfrom pytesseract import image_to_stringfrom enchant.checker import SpellCheckerfrom difflib import SequenceMatcherimport torchfrom pytorch_pretrained_bert import BertTokenizer, BertForMaskedLM Process images by using OCR 12345678imagename = '1.png'pil_img = Image.open(imagename)text = image_to_string(pil_img)text_original = str(text)print(text)plt.figure(figsize = (12,4))plt.imshow(np.asarray(pil_img)) Output: 123national economy gained momentum in recent weeks as con@gmer spendingStrengthened, manufacturing activity cont@™ed to rise, and producersscheduled more investment in plant and equipment. Process text and mask incorrect words 12345678910111213141516171819202122232425262728293031323334353637383940414243# text cleanuprep = &#123;'\\n': ' ', '\\\\': ' ', '\\\"': '\"', '-': ' ', '\"': ' \" ', ',':' , ', '.':' . ', '!':' ! ', '?':' ? ', \"n't\": \" not\", \"'ll\": \" will\", '*':' * ', '(': ' ( ', ')': ' ) ', \"s'\": \"s '\"&#125;rep = dict((re.escape(k), v) for k, v in rep.items())pattern = re.compile(\"|\".join(rep.keys()))text = pattern.sub(lambda m: rep[re.escape(m.group(0))], text)def get_personslist(text): personslist = [] for sent in nltk.sent_tokenize(text): for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))): if isinstance(chunk, nltk.tree.Tree) and chunk.label() == 'PERSON': personslist.insert(0, (chunk.leaves()[0][0])) return list(set(personslist))personslist = get_personslist(text)ignorewords = personslist + [\"!\", \",\", \".\", \"\\\"\", \"?\", '(', ')', '*', \"''\"]# use SpellChecker to find incorrect wordsd = SpellChecker(\"en_US\")words = text.split()incorrectwords = [w for w in words if not d.check(w) and w not in ignorewords]# use SpellChecker to get suggested replacementssuggestedwords = [d.suggest(w) for w in incorrectwords]# replace incorrect words with [MASK]for w in incorrectwords: text = text.replace(w, '[MASK]')print(text) Output: 1national economy gained momentum in recent weeks as [MASK] spending Strengthened , manufacturing activity [MASK] to rise , and producers scheduled more investment in plant and equipment . Use the pretrained BERT model to predict words12345678910111213141516171819202122232425# Tokenize texttokenizer = BertTokenizer.from_pretrained('bert-base-uncased')tokenized_text = tokenizer.tokenize(text)indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)MASKIDS = [i for i, e in enumerate(tokenized_text) if e == '[MASK]']# Create the segments tensorssegs = [i for i, e in enumerate(tokenized_text) if e == \".\"]segments_ids = []prev = -1for k, s in enumerate(segs): segments_ids = segments_ids + [k] * (s-prev) prev = ssegments_ids = segments_ids + [len(segs)] * (len(tokenized_text) - len(segments_ids))segments_tensors = torch.tensor([segments_ids])# prepare Torch inputstokens_tensors = torch.tensor([indexed_tokens])# Load pre-trained modelmodel = BertForMaskedLM.from_pretrained('bert-base-uncased')# Predict all tokenswith torch.no_grad(): predictions = model(tokens_tensors, segments_tensors) Match with proposals from SpellChecker 1234567891011121314151617def predict_word(text_original, predictions, MASKIDS): pred_words = [] for i in range(len(MASKIDS)): preds = torch.topk(predictions[0, MASKIDS[i]], k=50) indices = preds.indices.tolist() pred_list = tokenizer.convert_ids_to_tokens(indices) sugg_list = suggestedwords[i] sim_max = 0 predicted_token = '' for word1 in pred_list: for word2 in sugg_list: s = SequenceMatcher(None, word1, word2).ratio() if s is not None and s &gt; sim_max: sim_max = s predicted_token = word1 text_original = text_original.replace('[MASK]', predicted_token, 1) return text_original 12text_refined = predict_word(text, predictions, MASKIDS)print(text_refined) Output: 1national economy gained momentum in recent weeks as consumer spending Strengthened , manufacturing activity continued to rise , and producers scheduled more investment in plant and equipment .","categories":[{"name":"Natural-Language-Processing","slug":"Natural-Language-Processing","permalink":"https://steven-cheng-com.github.io/categories/Natural-Language-Processing/"}],"tags":[{"name":"BERT","slug":"BERT","permalink":"https://steven-cheng-com.github.io/tags/BERT/"},{"name":"Spelling-Correction","slug":"Spelling-Correction","permalink":"https://steven-cheng-com.github.io/tags/Spelling-Correction/"}],"author":"Steven Cheng"},{"title":"Spelling Corrector from Scratch","slug":"Spelling-Corrector-from-Scratch","date":"2020-06-19T05:25:39.000Z","updated":"2021-12-12T07:56:23.828Z","comments":true,"path":"2020/06/19/Spelling-Corrector-from-Scratch/","link":"","permalink":"https://steven-cheng-com.github.io/2020/06/19/Spelling-Corrector-from-Scratch/","excerpt":"","text":"&nbsp; Steven Cheng Intro To a entry-level NLP learner, an industrial-strength spell corrector are quite complex, but writing a toy spelling corrector from scratch that achieves 80% or 90% accuracy at a processing speed of tens of words per second in a few dozens of lines of code is possible. Here is the code: 123456789101112131415161718192021222324252627282930313233343536import refrom collections import Counterdef words(text): return re.findall(r'\\w+', text.lower())WORDS = Counter(words(open('book.txt').read()))def P(word, N=sum(WORDS.values())): \"Probability of `word`.\" return WORDS[word] / Ndef correction(word): \"Most probable spelling correction for word.\" return max(candidates(word), key=P)def candidates(word): \"Generate possible spelling corrections for word.\" return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])def known(words): \"The subset of `words` that appear in the dictionary of WORDS.\" return set(w for w in words if w in WORDS)def edits1(word): \"All edits that are one edit away from `word`.\" letters = 'abcdefghijklmnopqrstuvwxyz' splits = [(word[:i], word[i:]) for i in range(len(word) + 1)] deletes = [L + R[1:] for L, R in splits if R] transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)&gt;1] replaces = [L + c + R[1:] for L, R in splits if R for c in letters] inserts = [L + c + R for L, R in splits for c in letters] return set(deletes + transposes + replaces + inserts)def edits2(word): \"All edits that are two edits away from `word`.\" return (e2 for e1 in edits1(word) for e2 in edits1(e1)) The book.txt dataset could be any English e-book. The function correction(word) returns a likely spelling correction: 12345&gt;&gt;&gt; correction('speling')'spelling'&gt;&gt;&gt; correction('korrectud')'corrected' How does it work?The above function uses a Levenshtein Distance algorithm to find permutations within an edit distance of 2 from the original word. It then compares all permutations (insertions, deletions, replacements, and transpositions) to known words in a word frequency list. Those words that are found more often in the frequency list are more likely the correct results. The correction(A) function tries to choose the most likely spelling correction for A. There is no way to know for sure (for example, should “lates” be corrected to “late” or “latest” or “lattes” or …?), which suggests we use probabilities. We are trying to find the correction B, out of all possible candidate corrections, that maximizes the probability that B is the intended correction, given the original word A. ReferencePeter Norvig’s blog","categories":[{"name":"Natural-Language-Processing","slug":"Natural-Language-Processing","permalink":"https://steven-cheng-com.github.io/categories/Natural-Language-Processing/"}],"tags":[{"name":"Spelling-Correction","slug":"Spelling-Correction","permalink":"https://steven-cheng-com.github.io/tags/Spelling-Correction/"},{"name":"Natural-Language-Processing","slug":"Natural-Language-Processing","permalink":"https://steven-cheng-com.github.io/tags/Natural-Language-Processing/"}],"author":"Steven Cheng"},{"title":"Convert Pre-trained Model from MXNet to PyTorch or TensorFlow","slug":"Convert-Pre-trained-Model-from-MXNet-to-PyTorch-or-TensorFlow","date":"2020-04-20T05:20:22.000Z","updated":"2021-12-12T07:52:33.600Z","comments":true,"path":"2020/04/20/Convert-Pre-trained-Model-from-MXNet-to-PyTorch-or-TensorFlow/","link":"","permalink":"https://steven-cheng-com.github.io/2020/04/20/Convert-Pre-trained-Model-from-MXNet-to-PyTorch-or-TensorFlow/","excerpt":"","text":"&nbsp; Steven Cheng Intro Currently there are many available deep learning frameworks for researchers and engineers to implement their desired deep models. Sometimes, when you find a fantastic GitHub repository which share a pre-trained model on a framework which you are not familiar with. For example, you are an expert PyTorch deep learning code developer, meanwhile you find a great code with its pre-trained model on MXNet; and you want to modify this model according to your needs. Thus, deep learning model conversion tools are extremely needed. As each framework has its own structure, converting a model between two different frameworks requires a great knowledge of both of them. However, There are many fantastic model conversion tools such as ONNX, MMdnn, and etc. for converting and visualizing deep models between a wide collection of frameworks. Model Convertors ONNX ONNX is an effort to unify converters for neural networks in order to bring some sanity to the NN world. Released by Facebook and Microsoft. MMdnn MMdnn (Model Management Deep Neural Network) is supported by Microsoft, By using MMdnn, one can convert each model from the origin framework to a standard Intermediate Representation (IR), and then convert the IR format to the target framework structure. It can convert models between CaffeEmit, CNTK, CoreML, Keras, MXNet, ONNX, PyTorch and TensorFlow. PyTorch convertor PyTorch convertor can convert models to PyTorch model. TensorFlow convertor TensorFlow convertor can convert models to TensorFlow model. Keras convertor Keras convertor can convert models to Keras model. MXNet convertor MXNet convertor can convert models to MXNet model. Caffe convertor Caffe convertor can convert models to Caffe model. Caffe2 convertor Caffe2 convertor can convert models to Caffe2 model. CNTK convertor CNTK convertor can convert models to CNTK model. Theano/Lasagne convertor Theano/Lasagne convertor can convert models to Theano/Lasagne model. Darknet convertor Darknet convertor can convert models to Darknet model. Torch convertor Torch convertor can convert models to Torch model. Neon convertor Neon convertor can convert models to Neon model. CoreML convertor CoreML convertor can convert models to coreML model. Paddle convertor Paddle convertor can convert models to Paddle model. Chainer convertor Chainer convertor can convert models to Chainer model. A Demo of Model Convertion from MXNet to PyTorch Here is an appropriate example to convert the Full ImageNet pre-trained model from MXNet to PyTorch via MMdnn convertor. ImageNet is an image database organized according to the WordNet hierarchy, in which each node of the hierarchy is depicted by hundreds and thousands of images. Since 2010, the annual ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is a competition where research teams evaluate their algorithms on the given data set, and compete to achieve higher accuracy on several visual recognition tasks. A common reason to train a network on ImageNet data is to use it for transfer learning (including feature extraction or fine-tuning other models). Having a pre-trained model which is trained on such a huge training data set (i.e., full ImageNet), would be a really valuable network. It can speed up the convergence early in the training phase, and also improves the target task accuracy in some scenarios. Prerequisites: 1sudo pip3 install --upgrade mmdnn 1sudo pip3 install --upgrade torch torchvision Download pre-trained models: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import osimport errno_base_model_url = 'http://data.mxnet.io/models/'_default_model_info = &#123; 'imagenet11k-resnet-152': &#123;'symbol':_base_model_url+'imagenet-11k/resnet-152/resnet-152-symbol.json', 'params':_base_model_url+'imagenet-11k/resnet-152/resnet-152-0000.params'&#125;,&#125;def download_file(url, local_fname=None, force_write=False): # requests is not default installed import requests if local_fname is None: local_fname = url.split('/')[-1] if not force_write and os.path.exists(local_fname): return local_fname dir_name = os.path.dirname(local_fname) if dir_name != \"\": if not os.path.exists(dir_name): try: # try to create the directory if it doesn't exists os.makedirs(dir_name) except OSError as exc: if exc.errno != errno.EEXIST: raise r = requests.get(url, stream=True) assert r.status_code == 200, \"failed to open %s\" % url with open(local_fname, 'wb') as f: for chunk in r.iter_content(chunk_size=1024): if chunk: # filter out keep-alive new chunks f.write(chunk) return local_fnamedef download_model(model_name, dst_dir='./', meta_info=None): if meta_info is None: meta_info = _default_model_info meta_info = dict(meta_info) if model_name not in meta_info: return (None, 0) if not os.path.isdir(dst_dir): os.mkdir(dst_dir) meta = dict(meta_info[model_name]) assert 'symbol' in meta, \"missing symbol url\" model_name = os.path.join(dst_dir, model_name) download_file(meta['symbol'], model_name+'-symbol.json') assert 'params' in meta, \"mssing parameter file url\" download_file(meta['params'], model_name+'-0000.params') return (model_name, 0)if __name__ == \"__main__\": # ***** Download synset (i.e., Synonym Set): synset_url = 'http://data.mxnet.io.s3-website-us-west-1.amazonaws.com/models/imagenet-11k/synset.txt' download_file(synset_url, 'synset.txt') # ***** Download Model: download_model('imagenet11k-resnet-152', dst_dir='./') Converting Full ImageNet Pre-trained Model from MXNet to PyTorch: 1python3 -m mmdnn.conversion._script.convertToIR -f mxnet -n imagenet11k-resnet-152-symbol.json -w imagenet11k-resnet-152-0000.params -d resnet152 --inputShape 3,224,224 1python3 -m mmdnn.conversion._script.IRToCode -f pytorch --IRModelPath resnet152.pb --dstModelPath kit_imagenet.py --IRWeightPath resnet152.npy -dw kit_pytorch.npy 1python3 -m mmdnn.conversion.examples.pytorch.imagenet_test --dump resnet152Full.pth -n kit_imagenet.py -w kit_pytorch.npy Testing the Converted Model: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import torchimport numpy as npfrom tensorflow.contrib.keras.api.keras.preprocessing import image# ************** Parameters:num_predictions = 5 # Top-k Resultsmodel_address = 'resnet152Full.pth' # for loading modelslexicon_address = 'synset.txt'test_image_address = 'seagull.jpg'device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")# Load Converted Model:model = torch.load(model_address).to(device)model.eval()# Read Input Image and Apply Pre-process:img = image.load_img(test_image_address, target_size=(224, 224))x = image.img_to_array(img)x = x[..., ::-1] # transform image from RGB to BGRx = np.transpose(x, (2, 0, 1))x = np.expand_dims(x, 0).copy()x = torch.from_numpy(x)x = x.to(device)# Load Full-ImageNet Dictionary (i.e., lexicon):with open(lexicon_address, 'r') as f: labels = [l.rstrip() for l in f]# Make prediction (forward pass):with torch.no_grad(): output = model(x)max, argmax = output.data.squeeze().max(0)class_id = argmax.item()class_name = labels[class_id]# Print the top-5 Results:h_x = output.data.squeeze()probs, idx = h_x.sort(0, True)print('Top-5 Results: ')for i in range(0, num_predictions): print('&#123;:.2f&#125;% -&gt; &#123;&#125;'.format(probs[i] * 100.0, labels[idx[i]]))str_final_label = 'The Image is a ' + class_name[10:] + '.'print(str_final_label)","categories":[{"name":"Deep-Learning","slug":"Deep-Learning","permalink":"https://steven-cheng-com.github.io/categories/Deep-Learning/"}],"tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"https://steven-cheng-com.github.io/tags/PyTorch/"},{"name":"TenforFlow","slug":"TenforFlow","permalink":"https://steven-cheng-com.github.io/tags/TenforFlow/"},{"name":"Pre-trained-Model","slug":"Pre-trained-Model","permalink":"https://steven-cheng-com.github.io/tags/Pre-trained-Model/"}],"author":"Steven Cheng"},{"title":"Automatic Clustering with Silhouette Analysis on Agglomerative Hierarchical Clustering","slug":"automatic-clustering-with-silhouette-analysis-on-agglomerative-hierarchical-clustering","date":"2020-03-22T02:32:33.000Z","updated":"2021-12-12T07:52:28.273Z","comments":true,"path":"2020/03/22/automatic-clustering-with-silhouette-analysis-on-agglomerative-hierarchical-clustering/","link":"","permalink":"https://steven-cheng-com.github.io/2020/03/22/automatic-clustering-with-silhouette-analysis-on-agglomerative-hierarchical-clustering/","excerpt":"","text":"&nbsp; Steven Cheng Intro Automatic clustering algorithms are algorithms that can perform clustering without prior knowledge of data sets, and determine the optimal number of clusters even in the presence of noise and outlier points. Silhouette Analysis Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters. The silhouette score is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters. Thus silhouette analysis can be used to choose an optimal number for clusters automatically. Agglomerative Hierarchical Clustering Hierarchical clustering algorithms fall into 2 branches: top-down or bottom-up. Bottom-up algorithms treat each data point as a single cluster at the outset and then successively merge (or agglomerate) pairs of clusters until all clusters have been merged into a single cluster that contains all data points. Bottom-up hierarchical clustering is therefore called hierarchical agglomerative clustering or HAC. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. The Steps of Hierarchical clustering: We start by treating each data point as a single cluster i.e. if there are N data points in our dataset then we have N clusters. We then select a distance metric that measures the distance between two clusters. As an example, we will use average linkage which defines the distance between two clusters to be the average distance between data points in the first cluster and data points in the second cluster. On each iteration, two clusters are combined into one. The two clusters to be combined are selected as those with the smallest average linkage. I.e. according to our selected distance metric, these two clusters have the smallest distance between each other and therefore are the most similar and should be combined. Step 2 is repeated until we reach the root of the tree that one cluster contains all data points. In this way we can select how many clusters we want in the end simply by choosing when to stop combining the clusters i.e. when we stop building the tree! Automatic ClusteringSince hierarchical clustering does not require us to specify the number of clusters and we can even select which number of clusters looks best since we are building a tree. Here we use the silhouette score to help us determine choosing the optimal number of clusters for clustering task automatically. An example of auto-clustering with silhouette analysis on agglomerative hierarchical clustering is shown as follows. 12345678import pandas as pdimport numpy as npimport matplotlib.pyplot as pltfrom sklearn.decomposition import PCAfrom sklearn.cluster import AgglomerativeClusteringfrom sklearn.preprocessing import StandardScaler, normalizefrom sklearn.metrics import silhouette_scoreimport scipy.cluster.hierarchy as shc Load and clean the data: 1234567X = pd.read_csv('customer_info.csv')# Dropping the CUST_ID column from the dataX = X.drop('CUST_ID', axis = 1)# Handling the missing valuesX.fillna(method ='ffill', inplace = True) Preprocess the data: 12345678910# Scaling the data so that all the features become comparablescaler = StandardScaler()X_scaled = scaler.fit_transform(X)# Normalizing the data so that the data approximately # follows a Gaussian distributionX_normalized = normalize(X_scaled)# Converting the numpy array into a pandas DataFrameX_normalized = pd.DataFrame(X_normalized) Reduce the dimensionality: 1234pca = PCA(n_components = 2)X_new = pca.fit_transform(X_normalized)df_new = pd.DataFrame(X_new)df_new.columns = ['P1', 'P2'] Visualize the dendograms: 123plt.figure(figsize =(8, 8))plt.title('Visualising Clustering')Dendrogram = shc.dendrogram((shc.linkage(df_new, method ='ward'))) Evaluate the clustering models: 123456789101112131415161718k = range(2,10)ac_list = [AgglomerativeClustering(n_clusters = i) for i in k]# Appending the silhouette scoressilhouette_scores = &#123;&#125;silhouette_scores.fromkeys(k)for i,j in enumerate(k): silhouette_scores[j] = silhouette_score(df_new, ac_list[i].fit_predict(df_new))# Plottingy = list(silhouette_scores.values())plt.bar(k, y)plt.xlabel('Number of clusters', fontsize = 20)plt.ylabel('S(i)', fontsize = 20)plt.show() From the result above, we can conclude that 3 clusters obtain the highest silhouette score so the optimal number of clusters is 3 in this case. The complete code and dataset can be found here Reference[1] Wikipedia-Automatic clustering algorithms[2] https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html[3] The 5 Clustering Algorithms Data Scientists Need to Know","categories":[{"name":"Machine-Learning","slug":"Machine-Learning","permalink":"https://steven-cheng-com.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Hierarchical-Clustering","slug":"Hierarchical-Clustering","permalink":"https://steven-cheng-com.github.io/tags/Hierarchical-Clustering/"},{"name":"Silhouette","slug":"Silhouette","permalink":"https://steven-cheng-com.github.io/tags/Silhouette/"}],"author":"Steven Cheng"},{"title":"Object Detection with Luminoth Step by Step","slug":"object-detection-with-luminoth-step-by-step","date":"2020-02-14T09:19:36.000Z","updated":"2021-12-12T07:53:28.122Z","comments":true,"path":"2020/02/14/object-detection-with-luminoth-step-by-step/","link":"","permalink":"https://steven-cheng-com.github.io/2020/02/14/object-detection-with-luminoth-step-by-step/","excerpt":"","text":"&nbsp; Steven Cheng Intro Luminoth is an open source toolkit for computer vision. It supports object detection and it’s built in Python, using TensorFlow. The code is open source and available on GitHub. Detecting Objects with a Pre-trained ModelThe first thing is being familiarized with the Luminoth CLI tool, that is, the tool that you interact with using the lumi command. This is the main gate to Luminoth, allowing you to train new models, evaluate them, use them for predictions, manage the checkpoints and more. If we want Luminoth to predict the objects present in one of pictures (image.jpg). The way to do that is by running the following command: 1lumi predict image.jpg The result will be like this: 123Found 1 files to predict.Neither checkpoint not config specified, assuming `accurate`.Checkpoint not found. Check remote repository? [y/N]: Since you didn’t tell Luminoth what an “object” is for you, nor have taught it how to recognize said objects. So one way to do this is to use a pre-trained model that has been trained to detect popular types of objects. E.g., it can be a model trained with COCO dataset or Pascal VOC. What’s more, each pre-trained model might be associated with a different algorithm. The checkpoints correspond to the weights of a particular model (Faster R-CNN or SSD), trained with a particular dataset. The case of “accurate” is just a label for a particular Deep Learning model underneath, here, Faster R-CNN, trained with images from the COCO dataset. After the checkpoint download, with these commands we can output everything (a resulting image and a json file) to a preds directory: 12mkdir predslumi predict image.jpg -f preds/objects.json -d preds/ Exploring the Pre-trained CheckpointsFirst, run the lumi checkpoint refresh command, so Luminoth knows about the checkpoints that it has available for download. After refreshing the local index, you can list the available checkpoints running lumi checkpoint list: 123456================================================================================| id | name | alias | source | status |================================================================================| e1c2565b51e9 | Faster R-CNN w/COCO | accurate | remote | DOWNLOADED || aad6912e94d9 | SSD w/Pascal VOC | fast | remote | NOT_DOWNLOADED |================================================================================ Here, you can see the “accurate” checkpoint and another “fast” checkpoint that is the SSD model trained with Pascal VOC dataset. Let’s get some information about the “accurate” checkpoint by the following command: 1lumi checkpoint info e1c2565b51e9 or 1lumi checkpoint info accurate If getting predictions for an image or video using a specific checkpoint (e.g., fast) you can do so by using the –checkpoint parameter: 1lumi predict img.jpg --checkpoint fast -f preds/objects.json -d preds/ Playing Around with the Built-in InterfaceLuminoth includes a simple web frontend so you can play around with detected objects in images using different thresholds. To launch this, simply type lumi server web and then open your browser at http://localhost:5000. If you are running on an external VM, you can do lumi server web –host 0.0.0.0 –port to open in a custom port. Building Custom datasetIn order to use a custom dataset, we must first transform whatever format your data is in, to TFRecords files (one for each split — train, val, test). Luminoth reads datasets natively only in TensorFlow’s TFRecords format. This is a binary format that will let Luminoth consume the data very efficiently. Fortunately, Luminoth provides several CLI tools for transforming popular dataset format (such as Pascal VOC, ImageNet, COCO, CSV, etc.) into TFRecords. We should start by downloading the annotation files (this and this, for train) and the class description file. After we get the class-descriptions-boxable.csv file, we can go over all the classes available in the OpenImages dataset and see which ones are related to traffic dataset. The following were hand-picked after examining the full file: 12345678/m/015qff,Traffic light/m/0199g,Bicycle/m/01bjv,Bus/m/01g317,Person/m/04_sv,Motorcycle/m/07r04,Truck/m/0h2r6,Van/m/0k4j,Car Luminoth includes a dataset reader that can take OpenImages format, the dataset reader expects a particular directory layout so it knows where the files are located. In this case, files corresponding to the examples must be in a folder named like their split (train, test, …). So, you should have the following: 12345.├── class-descriptions-boxable.csv└── train ├── train-annotations-bbox.csv └── train-annotations-human-imagelabels-boxable.csv Then run the following command: 1234567lumi dataset transform \\ --type openimages \\ --data-dir . \\ --output-dir ./out \\ --split train \\ --class-examples 100 \\ --only-classes=/m/015qff,/m/0199g,/m/01bjv,/m/01g317,/m/04_sv,/m/07r04,/m/0h2r6,/m/0k4j This will generate TFRecord file for the train split: 12345678910INFO:tensorflow:Saved 360 records to \"./out/train.tfrecords\"INFO:tensorflow:Composition per class (train):INFO:tensorflow: Person (/m/01g317): 380INFO:tensorflow: Car (/m/0k4j): 255INFO:tensorflow: Bicycle (/m/0199g): 126INFO:tensorflow: Bus (/m/01bjv): 106INFO:tensorflow: Traffic light (/m/015qff): 105INFO:tensorflow: Truck (/m/07r04): 101INFO:tensorflow: Van (/m/0h2r6): 100INFO:tensorflow: Motorcycle (/m/04_sv): 100 Training the ModelTraining orchestration, including the model to be used, the dataset location and training schedule, is specified in a YAML config file. This file will be consumed by Luminoth and merged to the default configuration, to start the training session. You can see a minimal config file example in sample_config.yml. This file illustrates the entries you’ll most probably need to modify, which are: 1) train.run_name: the run name for the training session, used to identify it.2) train.job_dir: directory in which both model checkpoints and summaries (for TensorBoard consumption) will be saved. The actual files will be stored under /.3) dataset.dir: directory from which to read the TFRecord files.4) model.type: model to use for object detection (fasterrcnn, or ssd).5) network.num_classes: number of classes to predict (depends on your dataset). For looking at all the possible configuration options, mostly related to the model itself, you can check the base_config.yml file. Building the config file for the datasetProbably the most important setting for training is the learning rate. You will most likely want to tune this depending on your dataset, and you can do it via the train.learning_rate setting in the configuration. For example, this would be a good setting for training on the full COCO dataset: 1234learning_rate: decay_method: piecewise_constant boundaries: [250000, 450000, 600000] values: [0.0003, 0.0001, 0.00003, 0.00001] To get to this, you will need to run some experiments and see what works best. 12345678910111213141516171819train: # Run name for the training session. run_name: traffic job_dir: &lt;change this directory&gt; learning_rate: decay_method: piecewise_constant # Custom dataset for Luminoth Tutorial boundaries: [90000, 160000, 250000] values: [0.0003, 0.0001, 0.00003, 0.00001]dataset: type: object_detection dir: &lt;directory with your dataset&gt;model: type: fasterrcnn network: num_classes: 8 anchors: # Add one more scale to be better at detecting small objects scales: [0.125, 0.25, 0.5, 1, 2] Running the trainingAssuming you already have both your dataset (TFRecords) and the config file ready, you can start your training session by running the command as follows: 1lumi train -c config.yml You can use the -o option to override any configuration option using dot notation (e.g. -o model.rpn.proposals.nms_threshold=0.8). If you are using a CUDA-based GPU, you can select the GPU to use by setting the CUDA_VISIBLE_DEVICES environment variable. Storing checkpoints (partial weights)As the training progresses, Luminoth will periodically save a checkpoint with the current weights of the model. The files will be output in your / folder. By default, they will be saved every 600 seconds of training, but you can configure this with the train.save_checkpoint_secs setting in your config file. The default is to only store the latest checkpoint (that is, when a checkpoint is generated, the previous checkpoint gets deleted) in order to conserve storage. Evaluating ModelsGenerally, datasets (like OpenImages, which we just used) provide “splits”. The “train” split is the largest, and the one from which the model actually does the learning. Then, you have the “validation” (or “val”) split, which consists of different images, in which you can draw metrics of your model’s performance, in order to better tune your hyperparameters. Finally, a “test” split is provided in order to conduct the final evaluation of how your model would perform in the real world once it is trained. Building a validation datasetLet’s start by building TFRecords from the validation split of OpenImages. For this, we can download the files with the annotations and use the same lumi dataset transform that we used to build our training data. In your dataset folder (where the class-descriptions-boxable.csv is located), run the following commands: 123mkdir validationwget -P validation https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-bbox.csvwget -P validation https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-human-imagelabels-boxable.csv After the downloads finish, we can build the TFRecords with the following: 1234567lumi dataset transform \\ --type openimages \\ --data-dir . \\ --output-dir ./out \\ --split validation \\ --class-examples 100 \\ --only-classes=/m/015qff,/m/0199g,/m/01bjv,/m/01g317,/m/04_sv,/m/07r04,/m/0h2r6,/m/0k4j The lumi eval commandIn Luminoth, lumi eval will make a run through your chosen dataset split (ie. validation or test), and run the model through every image, and then compute metrics like loss and mAP. If you are lucky and happen to have more than one GPU in your machine, it is advisable to run both train and eval at the same time. Start by running the evaluation: 1lumi eval --split validation -c custom.yml The mAP metricsMean Average Precision (mAP) is the metric commonly used to evaluate object detection task. It computes how well your classifier works across all classes, mAP will be a number between 0 and 1, and the higher the better. Moreover, it can be calculated across different IoU (Intersection over Union) thresholds. For example, Pascal VOC challenge metric uses 0.5 as threshold (notation mAP@0.5), and COCO dataset uses mAP at different thresholds and averages them all out (notation mAP@[0.5:0.95]). Luminoth will print out several of these metrics, specifying the thresholds that were used under this notation. Using TensorBoard for VisualizingTensorBoard is a very good tool for this, allowing you to see plenty of plots with the training related metrics. By default, Luminoth writes TensorBoard summaries during training, so you can leverage this tool without any effort: 1tensorboard --logdir &lt;job_dir&gt;/&lt;run_name&gt; If you are running from an external VM, make sure to use --host 0.0.0.0 and --port if you need other one than the default 6006. What to look forFirst, go to the “Scalars” tab. You are going to see several tags. validation_lossesHere, you will get the same loss values that Luminoth computes for the train, but for the chosen dataset split (validation, in this case). As in the case of train, you should mostly look at validation_losses/no_reg_loss. These will be the mAP metrics that will help you judge how well your model perform: The mAP values refer to the entire dataset split, so it will not jump around as much as other metrics. Manually inspecting with lumi server webYou can also use lumi server web command that we have seen before and try your partially trained model in a bunch of novel images. For this, you can launch it with a config file like: 1lumi server web -c config.yml Here you can also use –host and –port options. Creating and Sharing Your Own CheckpointsCreating a checkpointWe can create checkpoints and set some metadata like name, alias, etc. This time, we are going to create the checkpoint for our traffic model: 1234lumi checkpoint create \\ config.yml \\ -e name=\"OpenImages Traffic\" \\ -e alias=traffic You can verify that you do indeed have the checkpoint when running lumi checkpoint list, which should get you an output similar to this: 1234567================================================================================| id | name | alias | source | status |================================================================================| e1c2565b51e9 | Faster R-CNN w/COCO | accurate | remote | DOWNLOADED || aad6912e94d9 | SSD w/Pascal VOC | fast | remote | DOWNLOADED || cb0e5d92a854 | OpenImages Traffic | traffic | local | LOCAL |================================================================================ Moreover, if you inspect the ~/.luminoth/checkpoints/ folder, you will see that now you have a folder that corresponds to your newly created checkpoint. Inside this folder are the actual weights of the model, plus some metadata and the configuration file that was used during training. Sharing checkpointsSimply run lumi checkpoint export cb0e5d92a854. You will get a file named cb0e5d92a854.tar in your current directory, which you can easily share to somebody else. By running lumi checkpoint import cb0e5d92a854.tar, the checkpoint will be listed locally. Using Luminoth with PythonCalling Luminoth from your Python app is very straightforward. 123456789101112131415from luminoth import Detector, read_image, vis_objectsimage = read_image('traffic-image.png')# If no checkpoint specified, will assume `accurate` by default. In this case,# we want to use our traffic checkpoint. The Detector can also take a config# object.detector = Detector(checkpoint='traffic')# Returns a dictionary with the detections.objects = detector.predict(image)print(objects)vis_objects(image, objects).save('traffic-out.png') The EndHope you enjoyed the simple tutorial! :)","categories":[{"name":"Computer-Vision","slug":"Computer-Vision","permalink":"https://steven-cheng-com.github.io/categories/Computer-Vision/"}],"tags":[{"name":"Computer-Vision","slug":"Computer-Vision","permalink":"https://steven-cheng-com.github.io/tags/Computer-Vision/"},{"name":"Object-Detection","slug":"Object-Detection","permalink":"https://steven-cheng-com.github.io/tags/Object-Detection/"}],"author":"Steven Cheng"},{"title":"Usage of PyTessBaseAPI in Tesserocr","slug":"Usage-of-PyTessBaseAPI-in-Tesserocr","date":"2020-01-18T07:16:21.000Z","updated":"2021-12-12T07:56:42.404Z","comments":true,"path":"2020/01/18/Usage-of-PyTessBaseAPI-in-Tesserocr/","link":"","permalink":"https://steven-cheng-com.github.io/2020/01/18/Usage-of-PyTessBaseAPI-in-Tesserocr/","excerpt":"","text":"&nbsp; Steven Cheng Intro Tesseroct is a simple, Pillow-friendly, wrapper around the tesseract-ocr API for Optical Character Recognition (OCR), it integrates directly with Tesseract’s C++ API using Cython which allows for a simple Pythonic and easy-to-read source code. It enables real concurrent execution when used with Python’s threading module by releasing the GIL while processing an image in tesseract. Installation Linux and BSD/MacOS 1$ pip install tesserocr Windows The proposed downloads consist of stand-alone packages containing all the Windows libraries needed for execution. The recommended method of installation is via Conda as described below. 1) Conda 1&gt; conda install -c conda-forge tesserocr 2) pipDownload the wheel file corresponding to your Windows platform and Python installation from tesserocr-windows_build and install them via: 1&gt; pip install &lt;package_name&gt;.whl Usage Initialize and re-use the tesseract API instance to score multiple images: 1234567891011from tesserocr import PyTessBaseAPIimages = ['sample.jpg', 'sample2.jpg', 'sample3.jpg']with PyTessBaseAPI() as api: for img in images: api.SetImageFile(img) print(api.GetUTF8Text()) print(api.AllWordConfidences())# api is automatically finalized when used in a with-statement (context manager).# otherwise api.End() should be explicitly called when it's no longer needed. Advanced API Examples 1) GetComponentImages example: 12345678910111213141516from PIL import Imagefrom tesserocr import PyTessBaseAPI, RILimage = Image.open('/usr/src/tesseract/testing/phototest.tif')with PyTessBaseAPI() as api: api.SetImage(image) boxes = api.GetComponentImages(RIL.TEXTLINE, True) print('Found &#123;&#125; textline image components.'.format(len(boxes))) for i, (im, box, _, _) in enumerate(boxes): # im is a PIL image object # box is a dict with x, y, w and h keys api.SetRectangle(box['x'], box['y'], box['w'], box['h']) ocrResult = api.GetUTF8Text() conf = api.MeanTextConf() print(u\"Box[&#123;0&#125;]: x=&#123;x&#125;, y=&#123;y&#125;, w=&#123;w&#125;, h=&#123;h&#125;, \" \"confidence: &#123;1&#125;, text: &#123;2&#125;\".format(i, conf, ocrResult, **box)) 2) Orientation and script detection (OSD): 1234567891011121314from PIL import Imagefrom tesserocr import PyTessBaseAPI, PSMwith PyTessBaseAPI(psm=PSM.AUTO_OSD) as api: image = Image.open(\"/usr/src/tesseract/testing/eurotext.tif\") api.SetImage(image) api.Recognize() it = api.AnalyseLayout() orientation, direction, order, deskew_angle = it.Orientation() print(\"Orientation: &#123;:d&#125;\".format(orientation)) print(\"WritingDirection: &#123;:d&#125;\".format(direction)) print(\"TextlineOrder: &#123;:d&#125;\".format(order)) print(\"Deskew angle: &#123;:.4f&#125;\".format(deskew_angle)) or simply with OSD_ONLY page segmentation mode: 12345678from tesserocr import PyTessBaseAPI, PSMwith PyTessBaseAPI(psm=PSM.OSD_ONLY) as api: api.SetImageFile(\"/usr/src/tesseract/testing/eurotext.tif\") os = api.DetectOS() print(\"Orientation: &#123;orientation&#125;\\nOrientation confidence: &#123;oconfidence&#125;\\n\" \"Script: &#123;script&#125;\\nScript confidence: &#123;sconfidence&#125;\".format(**os)) more human-readable info with tesseract 4+ (with LSTM engine): 12345678from tesserocr import PyTessBaseAPI, PSM, OEMwith PyTessBaseAPI(psm=PSM.OSD_ONLY, oem=OEM.LSTM_ONLY) as api: api.SetImageFile(\"/usr/src/tesseract/testing/eurotext.tif\") os = api.DetectOrientationScript() print(\"Orientation: &#123;orient_deg&#125;\\nOrientation confidence: &#123;orient_conf&#125;\\n\" \"Script: &#123;script_name&#125;\\nScript confidence: &#123;script_conf&#125;\".format(**os)) 3） Iterator over the classifier choices for a single symbol: 123456789101112131415161718192021222324252627from __future__ import print_functionfrom tesserocr import PyTessBaseAPI, RIL, iterate_levelwith PyTessBaseAPI() as api: api.SetImageFile('/usr/src/tesseract/testing/phototest.tif') api.SetVariable(\"save_blob_choices\", \"T\") api.SetRectangle(37, 228, 548, 31) api.Recognize() ri = api.GetIterator() level = RIL.SYMBOL for r in iterate_level(ri, level): symbol = r.GetUTF8Text(level) # r == ri conf = r.Confidence(level) if symbol: print(u'symbol &#123;&#125;, conf: &#123;&#125;'.format(symbol, conf), end='') indent = False ci = r.GetChoiceIterator() for c in ci: if indent: print('\\t\\t ', end='') print('\\t- ', end='') choice = c.GetUTF8Text() # c == ci print(u'&#123;&#125; conf: &#123;&#125;'.format(choice, c.Confidence())) indent = True print('----------')","categories":[{"name":"Computer-Vision","slug":"Computer-Vision","permalink":"https://steven-cheng-com.github.io/categories/Computer-Vision/"}],"tags":[{"name":"OCR","slug":"OCR","permalink":"https://steven-cheng-com.github.io/tags/OCR/"},{"name":"Computer-Vision","slug":"Computer-Vision","permalink":"https://steven-cheng-com.github.io/tags/Computer-Vision/"}],"author":"Stephen Cheng"},{"title":"PyCharm Keyboard Shortcuts","slug":"PyCharm-keyboard-shortcuts","date":"2019-12-19T14:17:38.000Z","updated":"2020-06-18T06:29:20.580Z","comments":true,"path":"2019/12/19/PyCharm-keyboard-shortcuts/","link":"","permalink":"https://steven-cheng-com.github.io/2019/12/19/PyCharm-keyboard-shortcuts/","excerpt":"","text":"&nbsp; Steven Cheng Intro PyCharm is an integrated development environment for computer programming, specifically for the Python language. It is developed by the Czech company JetBrains on February 3, 2010. Part one Part two Part three","categories":[{"name":"Programming","slug":"Programming","permalink":"https://steven-cheng-com.github.io/categories/Programming/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://steven-cheng-com.github.io/tags/Python/"},{"name":"PyCharm","slug":"PyCharm","permalink":"https://steven-cheng-com.github.io/tags/PyCharm/"},{"name":"Shortcuts","slug":"Shortcuts","permalink":"https://steven-cheng-com.github.io/tags/Shortcuts/"}],"author":"Stephen Cheng"},{"title":"Keras for Data Science","slug":"Keras-for-Data-Science","date":"2019-09-21T13:36:52.000Z","updated":"2021-12-12T07:53:13.171Z","comments":true,"path":"2019/09/21/Keras-for-Data-Science/","link":"","permalink":"https://steven-cheng-com.github.io/2019/09/21/Keras-for-Data-Science/","excerpt":"","text":"&nbsp; Steven Cheng Intro Keras is a powerful and easy-to-use deep learning library for Theano and TensorFlow that provides a high-level neural networks API to develop and evaluate deep learning models. A Basic Example:123456789101112131415&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; from keras.models import Sequential&gt;&gt;&gt; from keras.layers import Dense&gt;&gt;&gt; data = np.random.random((1000,100))&gt;&gt;&gt; labels = np.random.randint(2,size=(1000,1))&gt;&gt;&gt; model = Sequential()&gt;&gt;&gt; model.add(Dense(32, activation='relu', input_dim=100))&gt;&gt;&gt; model.add(Dense(1, activation='sigmoid'))&gt;&gt;&gt; model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])&gt;&gt;&gt; model.fit(data,labels,epochs=10,batch_size=32)&gt;&gt;&gt; predictions = model.predict(data) DataYour data needs to be stored as NumPy arrays or as a list of NumPy arrays. Ideally, you split the data in training and test sets, for which you can also resort to the train_test_split module of sklearn.cross_validation. Keras Data Sets:123456789&gt;&gt;&gt; from keras.datasets import boston_housing, mnist, cifar10, imdb&gt;&gt;&gt; (x_train,y_train),(x_test,y_test) = mnist.load_data()&gt;&gt;&gt; (x_train2,y_train2),(x_test2,y_test2) = boston_housing.load_data()&gt;&gt;&gt; (x_train3,y_train3),(x_test3,y_test3) = cifar10.load_data()&gt;&gt;&gt; (x_train4,y_train4),(x_test4,y_test4) = imdb.load_data(num_words=20000)&gt;&gt;&gt; num_classes = 10 Other:1234&gt;&gt;&gt; from urllib.request import urlopen&gt;&gt;&gt; data = np.loadtxt(urlopen(\"http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"),delimiter=\",\")&gt;&gt;&gt; X = data[:,0:8]&gt;&gt;&gt; y = data [:,8] PreprocessingSequence Padding:123&gt;&gt;&gt; from keras.preprocessing import sequence&gt;&gt;&gt; x_train4 = sequence.pad_sequences(x_train4,maxlen=80)&gt;&gt;&gt; x_test4 = sequence.pad_sequences(x_test4,maxlen=80) One-Hot Encoding:12345&gt;&gt;&gt; from keras.utils import to_categorical&gt;&gt;&gt; Y_train = to_categorical(y_train, num_classes)&gt;&gt;&gt; Y_test = to_categorical(y_test, num_classes)&gt;&gt;&gt; Y_train3 = to_categorical(y_train3, num_classes)&gt;&gt;&gt; Y_test3 = to_categorical(y_test3, num_classes) Train and Test Sets:12&gt;&gt;&gt; from sklearn.model_selection import train_test_split&gt;&gt;&gt; X_train5,X_test5,y_train5,y_test5 = train_test_split(X, y, test_size=0.33, random_state=42) Standardization/Normalization:1234&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler&gt;&gt;&gt; scaler = StandardScaler().fit(x_train2)&gt;&gt;&gt; standardized_X = scaler.transform(x_train2)&gt;&gt;&gt; standardized_X_test = scaler.transform(x_test2) Model ArchitectureSequential Model:1234&gt;&gt;&gt; from keras.models import Sequential&gt;&gt;&gt; model = Sequential()&gt;&gt;&gt; model2 = Sequential()&gt;&gt;&gt; model3 = Sequential() Multilayer Perceptron (MLP):Binary Classification 1234567&gt;&gt;&gt; from keras.layers import Dense&gt;&gt;&gt; model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))&gt;&gt;&gt; model.add(Dense(8,kernel_initializer='uniform',activation='relu'))&gt;&gt;&gt; model.add(Dense(1,kernel_initializer='uniform',activation='sigmoid')) Multi-Class Classification 123456&gt;&gt;&gt; from keras.layers import Dropout&gt;&gt;&gt; model.add(Dense(512,activation='relu',input_shape=(784,)))&gt;&gt;&gt; model.add(Dropout(0.2))&gt;&gt;&gt; model.add(Dense(512,activation='relu'))&gt;&gt;&gt; model.add(Dropout(0.2))&gt;&gt;&gt; model.add(Dense(10,activation='softmax')) Regression 12&gt;&gt;&gt; model.add(Dense(64,activation='relu',input_dim=train_data.shape[1]))&gt;&gt;&gt; model.add(Dense(1)) Convolutional Neural Network (CNN):12345678910111213141516171819&gt;&gt;&gt; from keras.layers import Activation,Conv2D,MaxPooling2D,Flatten&gt;&gt;&gt; model2.add(Conv2D(32,(3,3),padding='same',input_shape=x_train.shape[1:]))&gt;&gt;&gt; model2.add(Activation('relu'))&gt;&gt;&gt; model2.add(Conv2D(32,(3,3)))&gt;&gt;&gt; model2.add(Activation('relu'))&gt;&gt;&gt; model2.add(MaxPooling2D(pool_size=(2,2)))&gt;&gt;&gt; model2.add(Dropout(0.25))&gt;&gt;&gt; model2.add(Conv2D(64,(3,3), padding='same'))&gt;&gt;&gt; model2.add(Activation('relu'))&gt;&gt;&gt; model2.add(Conv2D(64,(3, 3)))&gt;&gt;&gt; model2.add(Activation('relu'))&gt;&gt;&gt; model2.add(MaxPooling2D(pool_size=(2,2)))&gt;&gt;&gt; model2.add(Dropout(0.25))&gt;&gt;&gt; model2.add(Flatten())&gt;&gt;&gt; model2.add(Dense(512))&gt;&gt;&gt; model2.add(Activation('relu'))&gt;&gt;&gt; model2.add(Dropout(0.5))&gt;&gt;&gt; model2.add(Dense(num_classes))&gt;&gt;&gt; model2.add(Activation('softmax')) Recurrent Neural Network (RNN):1234&gt;&gt;&gt; from keras.klayers import Embedding,LSTM&gt;&gt;&gt; model3.add(Embedding(20000,128))&gt;&gt;&gt; model3.add(LSTM(128,dropout=0.2,recurrent_dropout=0.2))&gt;&gt;&gt; model3.add(Dense(1,activation='sigmoid')) Inspect Model12345678# Model output shape&gt;&gt;&gt; model.output_shape# Model summary representation&gt;&gt;&gt; model.summary()# Model configuration&gt;&gt;&gt; model.get_config()# List all weight tensors in the model&gt;&gt;&gt; model.get_weights() Compile ModelMLP: Binary Classification 123&gt;&gt;&gt; model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) MLP: Multi-Class Classification 123&gt;&gt;&gt; model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) MLP: Regression 1&gt;&gt;&gt; model.compile(optimizer='rmsprop', loss='mse', metrics=['mae']) Recurrent Neural Network 123&gt;&gt;&gt; model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) Model Training123456&gt;&gt;&gt; model3.fit(x_train4, y_train4, batch_size=32, epochs=15, verbose=1, validation_data=(x_test4,y_test4)) Evaluate Your Model’s Performance123&gt;&gt;&gt; score = model3.evaluate(x_test, y_test, batch_size=32) Prediction12&gt;&gt;&gt; model3.predict(x_test4, batch_size=32)&gt;&gt;&gt; model3.predict_classes(x_test4,batch_size=32) Save/ Reload Models123&gt;&gt;&gt; from keras.models import load_model&gt;&gt;&gt; model3.save('model_file.h5')&gt;&gt;&gt; my_model = load_model('my_model.h5') Model Fine-tuningOptimization Parameters:1234&gt;&gt;&gt; from keras.optimizers import RMSprop&gt;&gt;&gt; opt = RMSprop(lr=0.0001, decay=1e-6)&gt;&gt;&gt; model2.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) Early Stopping:12345678&gt;&gt;&gt; from keras.callbacks import EarlyStopping&gt;&gt;&gt; early_stopping_monitor = EarlyStopping(patience=2)&gt;&gt;&gt; model3.fit(x_train4, y_train4, batch_size=32, epochs=15, validation_data=(x_test4,y_test4), callbacks=[early_stopping_monitor])","categories":[{"name":"Deep-Learning","slug":"Deep-Learning","permalink":"https://steven-cheng-com.github.io/categories/Deep-Learning/"}],"tags":[{"name":"Keras","slug":"Keras","permalink":"https://steven-cheng-com.github.io/tags/Keras/"},{"name":"Data-Science","slug":"Data-Science","permalink":"https://steven-cheng-com.github.io/tags/Data-Science/"},{"name":"Deep-Learning","slug":"Deep-Learning","permalink":"https://steven-cheng-com.github.io/tags/Deep-Learning/"}],"author":"Stephen Cheng"},{"title":"Python for Data Science Cheat Sheet with Scikit-Learn","slug":"Python-for-Data-Science-Cheat-Sheet-with-Scikit-Learn","date":"2019-06-18T08:33:19.000Z","updated":"2021-12-12T07:54:27.948Z","comments":true,"path":"2019/06/18/Python-for-Data-Science-Cheat-Sheet-with-Scikit-Learn/","link":"","permalink":"https://steven-cheng-com.github.io/2019/06/18/Python-for-Data-Science-Cheat-Sheet-with-Scikit-Learn/","excerpt":"","text":"&nbsp; Steven Cheng Intro of Scikit-LearnScikit-learn is an open source Python library that implements a range of machine learning, data preprocessing, cross-validation and visualization algorithms using a unified interface. The whole workflow of data science includes: Loading the data Training and test data Preprocessing ehe data Create your model Model fitting Prediction Evaluate your model’a performance Tune your model Here I give you a basic example for reference. 12345678910111213&gt;&gt;&gt; from sklearn import neighbors, datasets, preprocessing&gt;&gt;&gt; from sklearn.model_selection import train_test_split&gt;&gt;&gt; from sklearn.metrics import accuracy_score&gt;&gt;&gt; iris = datasets.load_iris()&gt;&gt;&gt; X, y = iris.data[:, :2], iris.target&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=33)&gt;&gt;&gt; scaler = preprocessing.StandardScaler().fit(X_train)&gt;&gt;&gt; X_train = scaler.transform(X_train)&gt;&gt;&gt; X_test = scaler.transform(X_test)&gt;&gt;&gt; knn = neighbors.KNeighborsClassifier(n_neighbors=5)&gt;&gt;&gt; knn.fit(X_train, y_train)&gt;&gt;&gt; y_pred = knn.predict(X_test)&gt;&gt;&gt; accuracy_score(y_test, y_pred) Loading The DataYour data needs to be numeric and stored as NumPy arrays or SciPy sparse matrices. Other types that are convertible to numeric arrays, such as Pandas DataFrame, are also acceptable. 1234&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; X = np.random.random((10,5))&gt;&gt;&gt; y = np.array(['M','M','F','F','M','F','M','M','F','F','F'])&gt;&gt;&gt; X[X &lt; 0.7] = 0 Training And Test Data12&gt;&gt;&gt; from sklearn.model_selection import train_test_split&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) Preprocessing The DataImputing Missing Values123&gt;&gt;&gt; from sklearn.preprocessing import Imputer&gt;&gt;&gt; imp = Imputer(missing_values=0, strategy='mean', axis=0)&gt;&gt;&gt; imp.fit_transform(X_train) Standardization1234&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler&gt;&gt;&gt; scaler = StandardScaler().fit(X_train)&gt;&gt;&gt; standardized_X = scaler.transform(X_train)&gt;&gt;&gt; standardized_X_test = scaler.transform(X_test) Normalization1234&gt;&gt;&gt; from sklearn.preprocessing import Normalizer&gt;&gt;&gt; scaler = Normalizer().fit(X_train)&gt;&gt;&gt; normalized_X = scaler.transform(X_train)&gt;&gt;&gt; normalized_X_test = scaler.transform(X_test) Binarization123&gt;&gt;&gt; from sklearn.preprocessing import Binarizer&gt;&gt;&gt; binarizer = Binarizer(threshold=0.0).fit(X)&gt;&gt;&gt; binary_X = binarizer.transform(X) Encoding Categorical Features123&gt;&gt;&gt; from sklearn.preprocessing import LabelEncoder&gt;&gt;&gt; enc = LabelEncoder()&gt;&gt;&gt; y = enc.fit_transform(y) Generating Polynomial Features123&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures&gt;&gt;&gt; poly = PolynomialFeatures(5)&gt;&gt;&gt; poly.fit_transform(X) Create Your ModelSupervised Learning EstimatorsLinear Regression 12&gt;&gt;&gt; from sklearn.linear_model import LinearRegression&gt;&gt;&gt; lr = LinearRegression(normalize=True) Support Vector Machines (SVM) 12&gt;&gt;&gt; from sklearn.svm import SVC&gt;&gt;&gt; svc = SVC(kernel='linear') Naive Bayes 12&gt;&gt;&gt; from sklearn.naive_bayes import GaussianNB&gt;&gt;&gt; gnb = GaussianNB() KNN 12&gt;&gt;&gt; from sklearn import neighbors&gt;&gt;&gt; knn = neighbors.KNeighborsClassifier(n_neighbors=5) Unsupervised Learning EstimatorsPrincipal Component Analysis (PCA) 12&gt;&gt;&gt; from sklearn.decomposition import PCA&gt;&gt;&gt; pca = PCA(n_components=0.95) K Means 12&gt;&gt;&gt; from sklearn.cluster import KMeans&gt;&gt;&gt; k_means = KMeans(n_clusters=3, random_state=0) Model FittingSupervised learningFit the model to the data. 123&gt;&gt;&gt; lr.fit(X, y)&gt;&gt;&gt; knn.fit(X_train, y_train)&gt;&gt;&gt; svc.fit(X_train, y_train) Unsupervised LearningFit the model to the data.Fit to data, then transform it. 12&gt;&gt;&gt; k_means.fit(X_train)&gt;&gt;&gt; pca_model = pca.fit_transform(X_train) PredictionSupervised EstimatorsPredict labels. Predict labels. Estimate probability of a label. 123&gt;&gt;&gt; y_pred = svc.predict(np.random.random((2,5)))&gt;&gt;&gt; y_pred = lr.predict(X_test)&gt;&gt;&gt; y_pred = knn.predict_proba(X_test) Unsupervised EstimatorsPredict labels in clustering algos. 1&gt;&gt;&gt; y_pred = k_means.predict(X_test) Evaluate Your Model’s PerformanceClassification MetricsAccuracy Score Estimator score method. Metric scoring functions. 123&gt;&gt;&gt; knn.score(X_test, y_test)&gt;&gt;&gt; from sklearn.metrics import accuracy_score&gt;&gt;&gt; accuracy_score(y_test, y_pred) Classification Report Precision, recall, f1-score and support. 12&gt;&gt;&gt; from sklearn.metrics import classification_report&gt;&gt;&gt; print(classification_report(y_test, y_pred)) Confusion Matrix 12&gt;&gt;&gt; from sklearn.metrics import confusion_matrix&gt;&gt;&gt; print(confusion_matrix(y_test, y_pred)) Regression MetricsMean Absolute Error 123&gt;&gt;&gt; from sklearn.metrics import mean_absolute_error&gt;&gt;&gt; y_true = [3, -0.5, 2]&gt;&gt;&gt; mean_absolute_error(y_true, y_pred) Mean Squared Error 12&gt;&gt;&gt; from sklearn.metrics import mean_squared_error&gt;&gt;&gt; mean_squared_error(y_test, y_pred) R² Score 12&gt;&gt;&gt; from sklearn.metrics import r2_score&gt;&gt;&gt; r2_score(y_true, y_pred) Clustering MetricsAdjusted Rand Index 12&gt;&gt;&gt; from sklearn.metrics import adjusted_rand_score&gt;&gt;&gt; adjusted_rand_score(y_true, y_pred) Homogeneity 12&gt;&gt;&gt; from sklearn.metrics import homogeneity_score&gt;&gt;&gt; homogeneity_score(y_true, y_pred) V-measure 12&gt;&gt;&gt; from sklearn.metrics import v_measure_score&gt;&gt;&gt; metrics.v_measure_score(y_true, y_pred) Cross-Validation123&gt;&gt;&gt; from sklearn.cross_validation import cross_val_score&gt;&gt;&gt; print(cross_val_score(knn, X_train, y_train, cv=4))&gt;&gt;&gt; print(cross_val_score(lr, X, y, cv=2)) Tune Your ModelGrid Search123456&gt;&gt;&gt; from sklearn.grid_search import GridSearchCV&gt;&gt;&gt; params = &#123;\"n_neighbors\": np.arange(1,3), \"metric\": [\"euclidean\", \"cityblock\"]&#125;&gt;&gt;&gt; grid = GridSearchCV(estimator=knn, param_grid=params)&gt;&gt;&gt; grid.fit(X_train, y_train)&gt;&gt;&gt; print(grid.best_score_)&gt;&gt;&gt; print(grid.best_estimator_.n_neighbors) Randomized Parameter Optimization1234567&gt;&gt;&gt; from sklearn.grid_search import RandomizedSearchCV&gt;&gt;&gt; params = &#123;\"n_neighbors\": range(1,5), \"weights\": [\"uniform\", \"distance\"]&#125;&gt;&gt;&gt; rsearch = RandomizedSearchCV(estimator=knn, param_distributions=params, cv=4, n_iter=8,random_state=5)&gt;&gt;&gt; rsearch.fit(X_train, y_train)&gt;&gt;&gt; print(rsearch.best_score_)","categories":[{"name":"Machine-Learning","slug":"Machine-Learning","permalink":"https://steven-cheng-com.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Machine-Learning","slug":"Machine-Learning","permalink":"https://steven-cheng-com.github.io/tags/Machine-Learning/"},{"name":"Python","slug":"Python","permalink":"https://steven-cheng-com.github.io/tags/Python/"},{"name":"Data-Science","slug":"Data-Science","permalink":"https://steven-cheng-com.github.io/tags/Data-Science/"},{"name":"Scikit-Learn","slug":"Scikit-Learn","permalink":"https://steven-cheng-com.github.io/tags/Scikit-Learn/"},{"name":"Sklearn","slug":"Sklearn","permalink":"https://steven-cheng-com.github.io/tags/Sklearn/"}],"author":"Steven Cheng"},{"title":"How to Install and Run MongoDB on Mac OS","slug":"Install-MongoDB-on-Mac","date":"2019-03-11T08:33:19.000Z","updated":"2020-06-18T06:29:00.300Z","comments":true,"path":"2019/03/11/Install-MongoDB-on-Mac/","link":"","permalink":"https://steven-cheng-com.github.io/2019/03/11/Install-MongoDB-on-Mac/","excerpt":"","text":"&nbsp; Steven Cheng Intro MongoDB is a document database which belongs to a family of databases called NoSQL - not only SQL. In MongoDB, records are documents which behave a lot like JSON objects in JavaScript. Values in documents can be looked up by their field’s key. Documents can have some fields/keys and not others, which makes Mongo extremely flexible. This is much different than SQL databases like MySQL, where fields correspond to columns in a table and individual records correspond to rows. Install and Run MongoDB with Homebrew Make sure Homebrew already installed on Mac (Homebrew is a package manager for the Mac – it makes installing most open source software). Open the Mac terminal application and type brew update, then type brew install mongodb After MongoDB is downloaded, create a directory to store MongoDB data files, type mkdir -p /data/db Make sure the directory has the right permissions: 12&gt; sudo chown -R `id -un` /data/db&gt; # Enter your psd Run the Mongo daemon by typing mongod in your terminal window. Run the Mongo shell by typing mongo in another terminal window. To exit the Mongo shell run quit(), to stop the Mongo daemon hit ctrl+c Install and Run MongoDB by Downloading it Manually 1) Go to the MongoDB website and download the correct version of MongoDB. 2) After downloading MongoDB, move the gzipped tar file to the folder where you want MongoDB installed by typing commands like these in your terminal: 12&gt; cd Downloads&gt; mv mongodb-macos-x86_64-4.2.2.tgz ~/ 3) Extract MongoDB from the downloaded archive and change the name of the directory to something more concise by following commands: 12&gt; cd ~/ &gt; tar -zxvf mongodb-macos-x86_64-4.2.2.tgz&gt; mv mongodb-macos-x86_64-4.2.2 mongodb 4) Create a directory to store MongoDB data files, type mkdir -p /data/db 5) Make sure the directory has the right permissions: 12&gt; sudo chown -R `id -un` /data/db&gt; # Enter your psd 6) If you can’t create /data/db directory, you can also use other directory to replace it by running command like this: 1~/mongodb/bin/mongod --dbpath ~/mongodb/data then jump to step 8. 7) Run the Mongo daemon by typing ~/mongodb/bin/mongod in your terminal window. 8) Run the Mongo shell by typing ~/mongodb/bin/mongo in another terminal window. 9) To exit the Mongo shell run quit(), to stop the Mongo daemon hit ctrl+c","categories":[{"name":"System","slug":"System","permalink":"https://steven-cheng-com.github.io/categories/System/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://steven-cheng-com.github.io/tags/MongoDB/"}],"author":"Stephen Cheng"},{"title":"How to Transfer Files & Connect to Jupyter Notebook Between Azure Virtual Machines and Local Systems","slug":"HowtoTransferFiles","date":"2019-01-28T06:02:39.000Z","updated":"2020-06-18T06:28:56.626Z","comments":true,"path":"2019/01/28/HowtoTransferFiles/","link":"","permalink":"https://steven-cheng-com.github.io/2019/01/28/HowtoTransferFiles/","excerpt":"","text":"&nbsp; Steven Cheng IntroRecently, cloud platforms are popular among various companies and conferences. Virtual Machines are one of the main applications provided by cloud platforms. For instance, Microsoft Azure offers Data Science Virtual Machines. Even though we have cloud platforms do us a favour to process hundreds and thousands of data remotely, I do need to upload files or download results from remote server. So how to interact between remote cloud servers and local systems? We’ll take Azure Virtual Machines as a case study. Transfer Files Copy a Local File to a Azure Virtual Machine with the scp Command: 1scp /local/path/file.txt azure_username@azure_host_ip:/remote/directory If SSH on the remote host is listening on a port other than the default 22, then you can specify a port using the -P argument: 1scp -P 1234 /local/path/file.txt azure_username@azure_host_ip:/remote/directory To copy a directory from a local to remote system, we can use the -r option: 1scp -r /local/directory azure_username@azure_host_ip:/remote/directory Copy a Remote File to a Local System using the scp command: 1scp azure_username@azure_host_ip:/remote/file.txt /local/directory Copy a File Between Two Azure Virtual Machines using the scp Command: 1scp azure_username1@azure_host_ip1:/directory/file.txt azure_username2@azure_host_ip2:/directory Connect to Azure Virtual MachineHere, we can use SSH command to connect to the Azure Virtual Machine from local path. 1ssh azure_username@azure_host_ip Connect to Jupyter Notebook applicationHere, we use SSH command to connect to the Jupyter Notebook Application. First, install and start the Jupyter Notebook Application in the Azure Virtual Machines, then open the local terminal and input the following command: 1ssh -L 8000:localhost:8888 azure_username@azure_host_ip Next, entering to the Jupyter Notebook Application using local browser with the url ‘localhost:8000’, and input the Jupyter notebook token from Azure Virtual Machines.","categories":[{"name":"System","slug":"System","permalink":"https://steven-cheng-com.github.io/categories/System/"}],"tags":[{"name":"VM","slug":"VM","permalink":"https://steven-cheng-com.github.io/tags/VM/"},{"name":"Jupyter","slug":"Jupyter","permalink":"https://steven-cheng-com.github.io/tags/Jupyter/"},{"name":"Azure","slug":"Azure","permalink":"https://steven-cheng-com.github.io/tags/Azure/"}],"author":"Stephen Cheng"},{"title":"Awesome Programming Skills for Python User","slug":"AwesomeProgramminSkillsforPythonUser","date":"2018-12-24T10:22:38.000Z","updated":"2021-12-12T07:52:31.901Z","comments":true,"path":"2018/12/24/AwesomeProgramminSkillsforPythonUser/","link":"","permalink":"https://steven-cheng-com.github.io/2018/12/24/AwesomeProgramminSkillsforPythonUser/","excerpt":"","text":"&nbsp; Steven Cheng IntroHere are some awesome programming skills shared for Python users. Hopefully it’s useful to you. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126# most frequent element in a lista = [1,2,3,1,2,3,2,2,2,4,5,1]print(max(set(a), key = a.count))# use Counterfrom collections import Countercnt = Counter(a)print(cnt.most_common(3))# check two strings have same charactersfrom collections import Counterstr1 = \"I see him\"str2 = \"him I see\"print(Counter(str1) == Counter(str2))# reverse stringb = 'abcdefghijklmnopqrstuvwxyz'print(b[::-1])for char in reversed(b): print(char)num = 123456789print(int(str(num)[::-1]))# transpose 2d arrayoriginal = [['a','b'], ['1','2'],['A','B']]transposed = zip(*original)print(list(transposed))# call different functions with same argumentsdef product(a,b): return a*bdef add(a,b): return a+bx = Trueprint((product if x else add)(5,7))# a shallow copye = [1,2,3,4,5]d = ed[0] = 10print(d,e)d2 = e[:]d2[0] = 11print(d2,e)# deep copyfrom copy import deepcopyf = [[1,2],[3,4]]f2 = deepcopy(f)f[0] = ['x','y']print(f,f2)# sort a dict by its value with built-in sorted funcg = &#123;'apple':50, 'banana':25, 'orange': 20, 'watermelon':10&#125;print(sorted(g.items(), key = lambda x:x[1]))# use itemgetter instead of a lambdafrom operator import itemgetterprint(sorted(g.items(), key=itemgetter(1)))# sort dict by valueprint(sorted(g, key = g.get))# merge dictd1 = &#123;'x':1&#125;d2 = &#123;'y':2&#125;print(&#123;**d1, **d2&#125;) #python 3.5print(dict(d1.items() | d2.items())) #python 3.5d1.update(d2) #python 3.5 print(d1)# convert list to comma separated stringdata = [1,'re', 3, 'fa', 5]print(','.join(map(str, data)))# find index of min/max elementh = [40, 30, 20, 10, 50]def minIndex(lst): return min(range(len(lst)), key = lst.__getitem__)def maxIndex(lst): return max(range(len(lst)), key = lst.__getitem__)print(minIndex(h))print(maxIndex(h))# remove duplicate items from listi = [6,2,2,3,4,4,4,5]print(list(set(i)))from collections import OrderedDictprint(list(OrderedDict.fromkeys(i).keys()))","categories":[{"name":"Programming","slug":"Programming","permalink":"https://steven-cheng-com.github.io/categories/Programming/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://steven-cheng-com.github.io/tags/Python/"},{"name":"Code","slug":"Code","permalink":"https://steven-cheng-com.github.io/tags/Code/"},{"name":"Programming","slug":"Programming","permalink":"https://steven-cheng-com.github.io/tags/Programming/"}],"author":"Stephen Cheng"},{"title":"What Is Bagging and How Does It Work?","slug":"WhatIsBaggingandHowDoesItWork","date":"2018-11-21T05:42:41.000Z","updated":"2021-12-12T07:57:22.335Z","comments":true,"path":"2018/11/21/WhatIsBaggingandHowDoesItWork/","link":"","permalink":"https://steven-cheng-com.github.io/2018/11/21/WhatIsBaggingandHowDoesItWork/","excerpt":"","text":"&nbsp; Steven Cheng IntroBagging is a technique used to reduce the variance of our predictions by combining the result of multiple classifiers modeled on different sub-samples of the same dataset. The following figure will make it clearer. StepsThe steps followed in bagging are: 1) Create Multiple DataSets Sampling is done with replacement on the original data and new datasets are formed. The new data sets can have a fraction of the columns as well as rows, which are generally hyper-parameters in a bagging model. Taking row and column fractions less than 1 helps in making robust models, less prone to overfitting. 2) Build Multiple Classifiers Classifiers are built on each data set. Generally the same classifier is modeled on each dataset and predictions are made. 3) Combine Classifiers The predictions of all the classifiers are combined using a mean, median or mode value depending on the problem at hand. The combined values are generally more robust than a single model. Note that, here the number of models built is not a hyper-parameters. Higher number of models are always better or may give similar performance than lower numbers. It can be theoretically shown that the variance of the combined predictions are reduced to 1/n (n: number of classifiers) of the original variance, under some assumptions.","categories":[{"name":"Machine-Learning","slug":"Machine-Learning","permalink":"https://steven-cheng-com.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Bagging","slug":"Bagging","permalink":"https://steven-cheng-com.github.io/tags/Bagging/"},{"name":"Classifier","slug":"Classifier","permalink":"https://steven-cheng-com.github.io/tags/Classifier/"}],"author":"Stephen Cheng"},{"title":"What Are Ensemble Methods in Tree Based Modelling?","slug":"WhatAreEnsembleMethodsinTreeBasedModelling","date":"2018-10-10T07:33:00.000Z","updated":"2021-12-12T07:56:50.365Z","comments":true,"path":"2018/10/10/WhatAreEnsembleMethodsinTreeBasedModelling/","link":"","permalink":"https://steven-cheng-com.github.io/2018/10/10/WhatAreEnsembleMethodsinTreeBasedModelling/","excerpt":"","text":"&nbsp; Steven Cheng IntroThe literary meaning of word ‘ensemble’ is group. Ensemble methods involve group of predictive models to achieve a better accuracy and model stability. Ensemble methods are known to impart supreme boost to tree based models. Bias &amp; VarianceLike every other model, a tree based model also suffers from the plague of bias and variance. Bias means, ‘how much on an average are the predicted values different from the actual value.’ Variance means, ‘how different will the predictions of the model be at the same point if different samples are taken from the same population’. You build a small tree and you will get a model with low variance and high bias. How do you manage to balance the trade off between bias and variance ? Normally, as you increase the complexity of your model, you will see a reduction in prediction error due to lower bias in the model. As you continue to make your model more complex, you end up over-fitting your model and your model will start suffering from high variance. A champion model should maintain a balance between these two types of errors. This is known as the trade-off management of bias-variance errors. Ensemble learning is one way to execute this trade off analysis. Some of the commonly used ensemble methods include: Bagging, Boosting and Stacking.","categories":[{"name":"Machine-Learning","slug":"Machine-Learning","permalink":"https://steven-cheng-com.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Ensemble","slug":"Ensemble","permalink":"https://steven-cheng-com.github.io/tags/Ensemble/"},{"name":"Bias","slug":"Bias","permalink":"https://steven-cheng-com.github.io/tags/Bias/"},{"name":"Variance","slug":"Variance","permalink":"https://steven-cheng-com.github.io/tags/Variance/"}],"author":"Stephen Cheng"},{"title":"Decision Trees Implementation in R and Python","slug":"DecisionTreesImplementationinRandPython","date":"2018-08-06T18:58:03.000Z","updated":"2021-12-12T07:52:39.811Z","comments":true,"path":"2018/08/07/DecisionTreesImplementationinRandPython/","link":"","permalink":"https://steven-cheng-com.github.io/2018/08/07/DecisionTreesImplementationinRandPython/","excerpt":"","text":"&nbsp; Steven Cheng IntroFor R users and Python users, decision tree is quite easy to implement. Let’s quickly look at the set of codes which can get you started with this algorithm. For ease of use, I’ve shared standard codes where you’ll need to replace your dataset name and variables to get started. RFor R users, there are multiple packages available to implement decision tree such as ctree, rpart, tree etc. 1234567891011121314&gt; library(rpart)&gt; x &lt;- cbind(x_train,y_train)# grow tree&gt; fit &lt;- rpart(y_train ~ ., data = x,method=\"class\")&gt; summary(fit)#Predict Output&gt; predicted= predict(fit,x_test)&gt; library(rpart)&gt; x &lt;- cbind(x_train,y_train)# grow tree&gt; fit &lt;- rpart(y_train ~ ., data = x,method=\"class\")&gt; summary(fit)#Predict Output&gt; predicted= predict(fit,x_test) In the code above: y_train – represents dependent variable. x_train – represents independent variable x – represents training data. PythonFor Python users, below is the code: 123456789101112#Import necessary libraries like pandas, numpy...from sklearn import tree#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset# Create tree object# for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini model = tree.DecisionTreeClassifier(criterion='gini')# model = tree.DecisionTreeRegressor() for regression# Train the model using the training sets and check scoremodel.fit(X, y)model.score(X, y)#Predict Outputpredicted= model.predict(x_test)","categories":[{"name":"Machine-Learning","slug":"Machine-Learning","permalink":"https://steven-cheng-com.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Tree","slug":"Tree","permalink":"https://steven-cheng-com.github.io/tags/Tree/"},{"name":"Python","slug":"Python","permalink":"https://steven-cheng-com.github.io/tags/Python/"},{"name":"R","slug":"R","permalink":"https://steven-cheng-com.github.io/tags/R/"}],"author":"Stephen Cheng"},{"title":"Are Tree Based Models Better than Linear Models?","slug":"AreTreeBasedModelsBetterThanLinearModels","date":"2018-07-27T04:44:50.000Z","updated":"2021-12-12T07:46:20.851Z","comments":true,"path":"2018/07/27/AreTreeBasedModelsBetterThanLinearModels/","link":"","permalink":"https://steven-cheng-com.github.io/2018/07/27/AreTreeBasedModelsBetterThanLinearModels/","excerpt":"","text":"&nbsp; Steven Cheng Intro“If I can use logistic regression for classification problems and linear regression for regression problems, why is there a need to use trees”? Many of us have this question. And, this is a valid one too. How to choose a proper algorithms?Actually, you can use any algorithm. It is dependent on the type of problem you are solving. Let’s look at some key factors which will help you to decide which algorithm to use: 1) If the relationship between dependent &amp; independent variable is well approximated by a linear model, linear regression will outperform tree based model. 2) If there is a high non-linearity &amp; complex relationship between dependent &amp; independent variables, a tree model will outperform a classical regression method. 3) If you need to build a model which is easy to explain to people, a decision tree model will always do better than a linear model. Decision tree models are even simpler to interpret than linear regression!","categories":[{"name":"Machine-Learning","slug":"Machine-Learning","permalink":"https://steven-cheng-com.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Tree","slug":"Tree","permalink":"https://steven-cheng-com.github.io/tags/Tree/"},{"name":"Linear","slug":"Linear","permalink":"https://steven-cheng-com.github.io/tags/Linear/"},{"name":"Machine-Learning","slug":"Machine-Learning","permalink":"https://steven-cheng-com.github.io/tags/Machine-Learning/"}],"author":"Stephen Cheng"},{"title":"What Are The Key Parameters of Tree Modelling and How to Avoid Over-fitting in Decision Trees?","slug":"WhatAreTheKeyParametersofTreeModellingandHowtoAvoidOver-fittinginDecisionTrees","date":"2018-06-06T02:37:33.000Z","updated":"2021-12-12T07:57:10.736Z","comments":true,"path":"2018/06/06/WhatAreTheKeyParametersofTreeModellingandHowtoAvoidOver-fittinginDecisionTrees/","link":"","permalink":"https://steven-cheng-com.github.io/2018/06/06/WhatAreTheKeyParametersofTreeModellingandHowtoAvoidOver-fittinginDecisionTrees/","excerpt":"","text":"&nbsp; Steven Cheng IntroOverfitting is one of the key challenges faced while modeling decision trees. If there is no limit set of a decision tree, it will give you 100% accuracy on training set because in the worse case it will end up making 1 leaf for each observation. Thus, preventing overfitting is pivotal while modeling a decision tree and it can be done in 2 ways: 1) Setting constraints on tree size.2) Tree pruning. Let’s discuss both of these briefly. Setting Constraints on Tree SizeThis can be done by using various parameters which are used to define a tree. First, let‘s look at the general structure of a decision tree. The parameters used for defining a tree are further explained below. The parameters described below are irrespective of tool. It is important to understand the role of parameters used in tree modeling. These parameters are available in R &amp; Python. Minimum samples for a node split 1) Defines the minimum number of samples (or observations) which are required in a node to be considered for splitting.2) Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.3) Too high values can lead to under-fitting hence, it should be tuned using CV. Minimum samples for a terminal node (leaf) 1) Defines the minimum samples (or observations) required in a terminal node or leaf.2) Used to control over-fitting similar to min_samples_split.3) Generally lower values should be chosen for imbalanced class problems because the regions in which the minority class will be in majority will be very small. Maximum depth of tree (vertical depth) 1) The maximum depth of a tree.2) Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.3) Should be tuned using CV. Maximum number of terminal nodes 1) The maximum number of terminal nodes or leaves in a tree.2) Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves. Maximum features to consider for split 1) The number of features to consider while searching for a best split. These will be randomly selected.2) As a thumb-rule, square root of the total number of features works great but we should check up to 30-40% of the total number of features.3) Higher values can lead to over-fitting but depends on case to case. Tree PruningAs discussed earlier, the technique of setting constraint is a greedy-approach. In other words, it will check for the best split instantaneously and move forward until one of the specified stopping condition is reached. Let’s consider the following case when you’re driving. There are 2 lanes: 1) A lane with cars moving at 80km/h.2) A lane with trucks moving at 30km/h. At this instant, you are the yellow car and you have 2 choices: 1) Take a left and overtake the other 2 cars quickly.2) Keep moving in the present lane. Let’s analyze these choice. In the former choice, you’ll immediately overtake the car ahead and reach behind the truck and start moving at 30 km/h, looking for an opportunity to move back right. All cars originally behind you move ahead in the meanwhile. This would be the optimum choice if your objective is to maximize the distance covered in next say 10 seconds. In the later choice, you sale through at same speed, cross trucks and then overtake maybe depending on situation ahead. Greedy you! This is exactly the difference between normal decision tree &amp; pruning. A decision tree with constraints won’t see the truck ahead and adopt a greedy approach by taking a left. On the other hand if we use pruning, we in effect look at a few steps ahead and make a choice. How to prune? So we know pruning is better. But how to implement it in decision tree? The idea is simple. 1) We first make the decision tree to a large depth.2) Then we start at the bottom and start removing leaves which are giving us negative returns when compared from the top.3) Suppose a split is giving us a gain of say -10 (loss of 10) and then the next split on that gives us a gain of 20. A simple decision tree will stop at step 1 but in pruning, we will see that the overall gain is +10 and keep both leaves. Note Note that sklearn’s decision tree classifier does not currently support pruning. Advanced packages like xgboost have adopted tree pruning in their implementation. But the library rpart in R, provides a function to prune. Good for R users!","categories":[{"name":"Machine-Learning","slug":"Machine-Learning","permalink":"https://steven-cheng-com.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Tree","slug":"Tree","permalink":"https://steven-cheng-com.github.io/tags/Tree/"},{"name":"Overfitting","slug":"Overfitting","permalink":"https://steven-cheng-com.github.io/tags/Overfitting/"}],"author":"Stephen Cheng"},{"title":"Merge Sort","slug":"MergeSort","date":"2018-05-18T16:23:47.000Z","updated":"2021-12-12T07:53:18.997Z","comments":true,"path":"2018/05/19/MergeSort/","link":"","permalink":"https://steven-cheng-com.github.io/2018/05/19/MergeSort/","excerpt":"","text":"&nbsp; Steven Cheng IntroIn computer science, merge sort (also commonly spelled mergesort) is an efficient, general-purpose, comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the implementation preserves the input order of equal elements in the sorted output. Mergesort is a divide and conquer algorithm that was invented by John von Neumann in 1945. DefinitionConceptually, a merge sort works as follows: 1) Divide the unsorted list into n sublists, each containing 1 element (a list of 1 element is considered sorted). 2) Repeatedly merge sublists to produce new sorted sublists until there is only 1 sublist remaining. This will be the sorted list. DetailsClass: Sorting algorithmData structure: ArrayWorst-case performance: O(n log n)Best-case performance: O(n log n) typical, O(n) natural variantAverage performance: O(n log n)Worst-case space complexity:&lt;/1b&gt; О(n) total, O(n) auxiliary Figure above, first divide the list into the smallest unit (1 element), then compare each element with the adjacent list to sort and merge the two adjacent lists. Finally all the elements are sorted and merged. CodePython 123456789101112131415from collections import dequedef merge_sort(lst): if len(lst) &lt;= 1: return lst def merge(left, right): merged,left,right = deque(),deque(left),deque(right) while left and right: # deque popleft is also O(1) merged.append(left.popleft() if left[0] &lt;= right[0] else right.popleft()) merged.extend(right if right else left) return list(merged) middle = int(len(lst) // 2) left = merge_sort(lst[:middle]) right = merge_sort(lst[middle:]) return merge(left, right)","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://steven-cheng-com.github.io/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://steven-cheng-com.github.io/tags/Algorithm/"},{"name":"Merge","slug":"Merge","permalink":"https://steven-cheng-com.github.io/tags/Merge/"},{"name":"Sort","slug":"Sort","permalink":"https://steven-cheng-com.github.io/tags/Sort/"}],"author":"Steven Cheng"},{"title":"Regression Trees vs. Classification Trees","slug":"RegressionTreesvs-ClassificationTrees","date":"2018-05-06T01:52:39.000Z","updated":"2021-12-12T07:54:39.420Z","comments":true,"path":"2018/05/06/RegressionTreesvs-ClassificationTrees/","link":"","permalink":"https://steven-cheng-com.github.io/2018/05/06/RegressionTreesvs-ClassificationTrees/","excerpt":"","text":"&nbsp; Steven Cheng IntroWe all know that the terminal nodes (or leaves) lies at the bottom of the decision tree. This means that decision trees are typically drawn upside down such that leaves are the bottom &amp; roots are the tops (shown below). Both the trees work almost similar to each other, let’s look at the primary differences &amp; similarity between classification and regression trees: 1) Regression trees are used when dependent variable is continuous. Classification trees are used when dependent variable is categorical. 2) In case of regression tree, the value obtained by terminal nodes in the training data is the mean response of observation falling in that region. Thus, if an unseen data observation falls in that region, we’ll make its prediction with mean value. 3) In case of classification tree, the value (class) obtained by terminal node in the training data is the mode of observations falling in that region. Thus, if an unseen data observation falls in that region, we’ll make its prediction with mode value. 4) Both the trees divide the predictor space (independent variables) into distinct and non-overlapping regions. For the sake of simplicity, you can think of these regions as high dimensional boxes or boxes. 5) Both the trees follow a top-down greedy approach known as recursive binary splitting. We call it as ‘top-down’ because it begins from the top of tree when all the observations are available in a single region and successively splits the predictor space into two new branches down the tree. It is known as ‘greedy’ because, the algorithm cares (looks for best variable available) about only the current split, and not about future splits which will lead to a better tree. 6) This splitting process is continued until a user defined stopping criteria is reached. For example: we can tell the algorithm to stop once the number of observations per node becomes less than 50. 7) In both the cases, the splitting process results in fully grown trees until the stopping criteria is reached. But, the fully grown tree is likely to overfit data, leading to poor accuracy on unseen data. This bring ‘pruning’. Pruning is one of the technique used tackle overfitting.","categories":[{"name":"Machine-Learning","slug":"Machine-Learning","permalink":"https://steven-cheng-com.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Machine-Learning","slug":"Machine-Learning","permalink":"https://steven-cheng-com.github.io/tags/Machine-Learning/"},{"name":"Regression","slug":"Regression","permalink":"https://steven-cheng-com.github.io/tags/Regression/"},{"name":"Classification","slug":"Classification","permalink":"https://steven-cheng-com.github.io/tags/Classification/"}],"author":"Steven Cheng"},{"title":"Levenshtein Distance","slug":"LevenshteinDistance","date":"2018-04-18T10:09:03.000Z","updated":"2020-06-18T06:29:09.784Z","comments":true,"path":"2018/04/18/LevenshteinDistance/","link":"","permalink":"https://steven-cheng-com.github.io/2018/04/18/LevenshteinDistance/","excerpt":"","text":"&nbsp; Steven Cheng IntroLevenshtein distance is a string metric for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other. It is named after Vladimir Levenshtein, who considered this distance in 1965. Levenshtein distance may also be referred to as edit distance, although that term may also denote a larger family of distance metrics. DefinitionMathematically, the Levenshtein distance between two strings a,b (of length |a| and |b| respectively) is given by lev_a,b(|a|,|b|) where ExampleThe Levenshtein distance between “kitten” and “sitting” is 3, since the following three edits change one into the other, and there is no way to do it with fewer than three edits: 1) kitten → sitten (substitution of “s” for “k”)2) sitten → sittin (substitution of “i” for “e”)3) sittin → sitting (insertion of “g” at the end) CodePython 12345678910111213141516171819202122232425262728#-*- coding: utf-8 -*-\"\"\"Levenshtein distance for measuring string differenceCreated on Apr. 6th, 2017@author: Stephen\"\"\"import numpy as npclass levenshtein_distance: def le_dis(self, input_x, input_y): xlen = len(input_x) + 1 ylen = len(input_y) + 1 dp = np.zeros(shape=(xlen, ylen), dtype=int) for i in range(0, xlen): dp[i][0] = i for j in range(0, ylen): dp[0][j] = j for i in range(1, xlen): for j in range(1, ylen): if input_x[i - 1] == input_y[j - 1]: dp[i][j] = dp[i - 1][j - 1] else: dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1]) return dp[xlen - 1][ylen - 1]if __name__ == '__main__': ld = levenshtein_distance() print(ld.le_dis('abcd', 'abd')) # print out 1 print(ld.le_dis('ace', 'abcd')) # print out 2 print(ld.le_dis('hello world', 'hey word')) # print out 4","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://steven-cheng-com.github.io/categories/Algorithm/"}],"tags":[{"name":"Levenshtein","slug":"Levenshtein","permalink":"https://steven-cheng-com.github.io/tags/Levenshtein/"},{"name":"Distance","slug":"Distance","permalink":"https://steven-cheng-com.github.io/tags/Distance/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://steven-cheng-com.github.io/tags/Algorithm/"}],"author":"Stephen Cheng"},{"title":"What Is A Decision Tree ? How Does It Work ?","slug":"WhatIsADecisionTreeHowDoesItWork","date":"2018-04-15T10:09:03.000Z","updated":"2021-12-12T07:57:17.216Z","comments":true,"path":"2018/04/15/WhatIsADecisionTreeHowDoesItWork/","link":"","permalink":"https://steven-cheng-com.github.io/2018/04/15/WhatIsADecisionTreeHowDoesItWork/","excerpt":"","text":"&nbsp; Steven Cheng IntroDecision tree is a type of supervised learning algorithm (having a pre-defined target variable) that is mostly used in classification problems. It works for both categorical and continuous input and output variables. In this technique, the population or sample are split into two or more homogeneous sets (or sub-populations) based on most significant splitter differentiator in input variables. ExampleSay we have a sample of 30 students with three variables: Gender (Boy/Girl), Class(IX/X) and Height (5 to 6 ft). 15 out of these 30 play cricket in leisure time. Now, we want to create a model to predict who will play cricket during leisure period? In this problem, we need to segregate students who play cricket in their leisure time based on highly significant input variable among all three. This is where decision tree helps, it will segregate the students based on all values of three variables and identify the variable, which creates the best homogeneous sets of students (which are heterogeneous to each other). In the snapshot below, you can see that variable Gender is able to identify best homogeneous sets compared to the other two variables. As mentioned above, decision tree identifies the most significant variable and its value that gives best homogeneous sets of population. Now the question which arises is, how does it identify the variable and the split? To do this, decision tree uses various algorithms, which we will shall discuss in the following section. Types of Decision TreesTypes of decision tree are based on the type of target variable we have. It can be of two types: 1) Categorical Variable Decision Tree This type of Decision Tree which has categorical target variable then it called as categorical variable decision tree. E.g., in above scenario of student problem, where the target variable was “Student will play cricket or not” i.e. YES or NO. 2) Continuous Variable Decision Tree This type of Decision Tree has continuous target variable then it is called as Continuous Variable Decision Tree. ExampleLet’s say we have a problem to predict whether a customer will pay his renewal premium with an insurance company (yes/no). Here we know that income of a customer is a significant variable but insurance company does not have income details for all customers. Now, as we know this is an important variable, then we can build a decision tree to predict customer income based on occupation, age and various other variables. In this case, we are predicting values for continuous variable. Important TerminologyLet’s look at the basic terminology used with Decision trees: 1) Root Node It represents entire population or sample and this further gets divided into two or more homogeneous sets. 2) Splitting It is a process of dividing a node into two or more sub-nodes. 3) Decision Node When a sub-node splits into further sub-nodes, then it is called decision node. 4) Leaf/Terminal Node Nodes do not split is called Leaf or Terminal node. 5) PruningWhen we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting. 6) Branch/Sub-Tree A sub section of entire tree is called branch or sub-tree. 7) Parent and Child Node A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node. These above are the terms commonly used for decision trees. As we know that every algorithm has advantages and disadvantages, below are the important factors which one should know. Advantages1) Easy to Understand Decision tree output is very easy to understand even for people from non-analytical background. It does not require any statistical knowledge to read and interpret them. Its graphical representation is very intuitive and users can easily relate their hypothesis. 2) Useful in Data Exploration Decision tree is one of the fastest way to identify most significant variables and relation between two or more variables. With the help of decision trees, we can create new variables/features that has better power to predict target variable. For example, we are working on a problem where we have information available in hundreds of variables, there decision tree will help to identify most significant variable. 3) Less Data Cleaning Required It requires less data cleaning compared to some other modeling techniques. It is not influenced by outliers and missing values to a fair degree. 4) Data Type Is Not a Constraint It can handle both numerical and categorical variables. 5) Non Parametric Method Decision tree is considered to be a non-parametric method. This means that decision trees have no assumptions about the space distribution and the classifier structure. Disadvantages1) Over Fitting Over fitting is one of the most practical difficulty for decision tree models. This problem gets solved by setting constraints on model parameters and pruning (discussed in detailed below). 2) Not Fit for Continuous Variables While working with continuous numerical variables, decision tree looses information when it categorizes variables in different categories.","categories":[{"name":"Machine-Learning","slug":"Machine-Learning","permalink":"https://steven-cheng-com.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Tree","slug":"Tree","permalink":"https://steven-cheng-com.github.io/tags/Tree/"},{"name":"Decision-Tree","slug":"Decision-Tree","permalink":"https://steven-cheng-com.github.io/tags/Decision-Tree/"}],"author":"Stephen Cheng"}]}