<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Spelling Correction with The Pretrained BERT Model | 双子塔科技</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
    <meta name="keywords" content="人工智能, AI, Tech, 教育" />
  
  <meta name="description" content="&amp;nbsp; Steven Cheng   Intro BERT (Bidirectional Encoder Representations from Transformers) is published by researchers at Google AI Language. It has caused a stir in the Machine Learning community by">
<meta name="keywords" content="BERT,Spelling-Correction">
<meta property="og:type" content="article">
<meta property="og:title" content="Spelling Correction with The Pretrained BERT Model">
<meta property="og:url" content="https:&#x2F;&#x2F;steven-cheng-com.github.io&#x2F;2020&#x2F;07&#x2F;18&#x2F;spelling-correction-with-pretrained-bert&#x2F;index.html">
<meta property="og:site_name" content="双子塔科技">
<meta property="og:description" content="&amp;nbsp; Steven Cheng   Intro BERT (Bidirectional Encoder Representations from Transformers) is published by researchers at Google AI Language. It has caused a stir in the Machine Learning community by">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;steven-cheng-com&#x2F;images&#x2F;master&#x2F;blog&#x2F;2020&#x2F;202007&#x2F;20200718&#x2F;0.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;steven-cheng-com&#x2F;images&#x2F;master&#x2F;blog&#x2F;2020&#x2F;202007&#x2F;20200718&#x2F;1.png">
<meta property="og:updated_time" content="2021-12-12T07:54:51.766Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;steven-cheng-com&#x2F;images&#x2F;master&#x2F;blog&#x2F;2020&#x2F;202007&#x2F;20200718&#x2F;0.png">
  
  
    <link rel="icon" href="images/favicon.ico">
  
  <link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  <script src="/js/pace.min.js"></script>
  

  
  

</head>

<body>
  <div id="container">
      <header id="header">
    <div id="banner"></div>
    <div id="header-outer">
        <div id="header-menu" class="header-menu-pos animated">
            <div class="header-menu-container">
                <a href="/" class="left">
                    <span class="site-title">双子塔科技</span>
                </a>
                <nav id="header-menu-nav" class="right">
                    
                    <a  href="/">
                        <i class="fa fa-home"></i>
                        <span>首页</span>
                    </a>
                    
                    <a  href="/archives">
                        <i class="fa fa-archives"></i>
                        <span>归档</span>
                    </a>
                    
                    <a  href="/about">
                        <i class="fa fa-about"></i>
                        <span>关于</span>
                    </a>
                    
                </nav>
                <a class="mobile-header-menu-button">
                    <i class="fa fa-bars"></i>
                </a>
            </div>
        </div>
        <div id="header-row">
            <div id="logo">
                <a href="/">
                    <img src="/images/logo.png" alt="logo">
                </a>
            </div>
            <div class="header-info">
                <div id="header-title">
                    
                    <h2>
                        双子塔科技
                    </h2>
                    
                </div>
                <div id="header-description">
                    
                </div>
            </div>
            <nav class="header-nav">
                <div class="social">
                    
                </div>
            </nav>
        </div>
    </div>
</header>
      <div class="outer">
        <section id="main" class="body-wrap"><article id="post-spelling-correction-with-pretrained-bert" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="post-title" itemprop="name">
      Spelling Correction with The Pretrained BERT Model
    </h1>
    <div class="post-title-bar">
      <ul>
          
              <li>
                  <i class="fa fa-book"></i>
                  
                      <a href="/categories/Natural-Language-Processing/">Natural-Language-Processing</a>
                  
              </li>
          
        <li>
          <i class="fa fa-calendar"></i>  2020-07-18
        </li>
        <li>
          <i class="fa fa-eye"></i>
          <span id="busuanzi_value_page_pv"></span>
        </li>
      </ul>
    </div>
  

          
      </header>
    
    <div class="article-entry post-content" itemprop="articleBody">
      
            
            <p>&nbsp;</p>
<center>Steven Cheng</center>


<h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p><img src="https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2020/202007/20200718/0.png" alt=""></p>
<p>BERT (Bidirectional Encoder Representations from Transformers) is published by researchers at Google AI Language. It has caused a stir in the Machine Learning community by presenting state-of-the-art results in a wide variety of NLP tasks, including Question Answering, Natural Language Inference, and others. BERT’s key technical innovation is applying the bidirectional training of Transformer, a popular attention model, to language modelling. This is in contrast to previous efforts which looked at a text sequence either from left to right or combined left-to-right and right-to-left training. The results show that a language model which is bidirectionally trained can have a deeper sense of language context and flow than single-direction language models.</p>
<p>BERT makes use of Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. Transformer includes two separate mechanisms — an encoder that reads the text input and a decoder that produces a prediction for the task. As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it’s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word).</p>
<h3 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h3><ul>
<li>Import necessary libraris</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> pytesseract <span class="keyword">import</span> image_to_string</span><br><span class="line"><span class="keyword">from</span> enchant.checker <span class="keyword">import</span> SpellChecker</span><br><span class="line"><span class="keyword">from</span> difflib <span class="keyword">import</span> SequenceMatcher</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> pytorch_pretrained_bert <span class="keyword">import</span> BertTokenizer, BertForMaskedLM</span><br></pre></td></tr></table></figure>

<ul>
<li>Process images by using OCR</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">imagename = <span class="string">'1.png'</span></span><br><span class="line">pil_img = Image.open(imagename)</span><br><span class="line">text = image_to_string(pil_img)</span><br><span class="line">text_original = str(text)</span><br><span class="line"></span><br><span class="line">print(text)</span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line">plt.imshow(np.asarray(pil_img))</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2020/202007/20200718/1.png" alt=""></p>
<p>Output:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">national economy gained momentum in recent weeks as con@gmer spending</span><br><span class="line">Strengthened, manufacturing activity cont@™ed to rise, and producers</span><br><span class="line">scheduled more investment in plant and equipment.</span><br></pre></td></tr></table></figure>

<ul>
<li>Process text and mask incorrect words</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># text cleanup</span></span><br><span class="line">rep = &#123;<span class="string">'\n'</span>: <span class="string">' '</span>,</span><br><span class="line">       <span class="string">'\\'</span>: <span class="string">' '</span>,</span><br><span class="line">       <span class="string">'\"'</span>: <span class="string">'"'</span>,</span><br><span class="line">       <span class="string">'-'</span>: <span class="string">' '</span>,</span><br><span class="line">       <span class="string">'"'</span>: <span class="string">' " '</span>,</span><br><span class="line">       <span class="string">','</span>:<span class="string">' , '</span>,</span><br><span class="line">       <span class="string">'.'</span>:<span class="string">' . '</span>,</span><br><span class="line">       <span class="string">'!'</span>:<span class="string">' ! '</span>,</span><br><span class="line">       <span class="string">'?'</span>:<span class="string">' ? '</span>,</span><br><span class="line">       <span class="string">"n't"</span>: <span class="string">" not"</span>,</span><br><span class="line">       <span class="string">"'ll"</span>: <span class="string">" will"</span>,</span><br><span class="line">       <span class="string">'*'</span>:<span class="string">' * '</span>,</span><br><span class="line">       <span class="string">'('</span>: <span class="string">' ( '</span>,</span><br><span class="line">       <span class="string">')'</span>: <span class="string">' ) '</span>,</span><br><span class="line">       <span class="string">"s'"</span>: <span class="string">"s '"</span>&#125;</span><br><span class="line"></span><br><span class="line">rep = dict((re.escape(k), v) <span class="keyword">for</span> k, v <span class="keyword">in</span> rep.items())</span><br><span class="line">pattern = re.compile(<span class="string">"|"</span>.join(rep.keys()))</span><br><span class="line">text = pattern.sub(<span class="keyword">lambda</span> m: rep[re.escape(m.group(<span class="number">0</span>))], text)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_personslist</span><span class="params">(text)</span>:</span></span><br><span class="line">    personslist = []</span><br><span class="line">    <span class="keyword">for</span> sent <span class="keyword">in</span> nltk.sent_tokenize(text):</span><br><span class="line">        <span class="keyword">for</span> chunk <span class="keyword">in</span> nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):</span><br><span class="line">            <span class="keyword">if</span> isinstance(chunk, nltk.tree.Tree) <span class="keyword">and</span> chunk.label() == <span class="string">'PERSON'</span>:</span><br><span class="line">                personslist.insert(<span class="number">0</span>, (chunk.leaves()[<span class="number">0</span>][<span class="number">0</span>]))</span><br><span class="line">    <span class="keyword">return</span> list(set(personslist))</span><br><span class="line">personslist = get_personslist(text)</span><br><span class="line">ignorewords = personslist + [<span class="string">"!"</span>, <span class="string">","</span>, <span class="string">"."</span>, <span class="string">"\""</span>, <span class="string">"?"</span>, <span class="string">'('</span>, <span class="string">')'</span>, <span class="string">'*'</span>, <span class="string">"''"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># use SpellChecker to find incorrect words</span></span><br><span class="line">d = SpellChecker(<span class="string">"en_US"</span>)</span><br><span class="line">words = text.split()</span><br><span class="line">incorrectwords = [w <span class="keyword">for</span> w <span class="keyword">in</span> words <span class="keyword">if</span> <span class="keyword">not</span> d.check(w) <span class="keyword">and</span> w <span class="keyword">not</span> <span class="keyword">in</span> ignorewords]</span><br><span class="line"></span><br><span class="line"><span class="comment"># use SpellChecker to get suggested replacements</span></span><br><span class="line">suggestedwords = [d.suggest(w) <span class="keyword">for</span> w <span class="keyword">in</span> incorrectwords]</span><br><span class="line"></span><br><span class="line"><span class="comment"># replace incorrect words with [MASK]</span></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> incorrectwords:</span><br><span class="line">    text = text.replace(w, <span class="string">'[MASK]'</span>)</span><br><span class="line"></span><br><span class="line">print(text)</span><br></pre></td></tr></table></figure>

<p>Output:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">national economy gained momentum in recent weeks as [MASK] spending Strengthened ,  manufacturing activity [MASK] to rise ,  and producers  scheduled more investment in plant and equipment .</span><br></pre></td></tr></table></figure>

<h3 id="Use-the-pretrained-BERT-model-to-predict-words"><a href="#Use-the-pretrained-BERT-model-to-predict-words" class="headerlink" title="Use the pretrained BERT model to predict words"></a>Use the pretrained BERT model to predict words</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tokenize text</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-uncased'</span>)</span><br><span class="line">tokenized_text = tokenizer.tokenize(text)</span><br><span class="line">indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)</span><br><span class="line">MASKIDS = [i <span class="keyword">for</span> i, e <span class="keyword">in</span> enumerate(tokenized_text) <span class="keyword">if</span> e == <span class="string">'[MASK]'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the segments tensors</span></span><br><span class="line">segs = [i <span class="keyword">for</span> i, e <span class="keyword">in</span> enumerate(tokenized_text) <span class="keyword">if</span> e == <span class="string">"."</span>]</span><br><span class="line">segments_ids = []</span><br><span class="line">prev = <span class="number">-1</span></span><br><span class="line"><span class="keyword">for</span> k, s <span class="keyword">in</span> enumerate(segs):</span><br><span class="line">    segments_ids = segments_ids + [k] * (s-prev)</span><br><span class="line">    prev = s</span><br><span class="line">segments_ids = segments_ids + [len(segs)] * (len(tokenized_text) - len(segments_ids))</span><br><span class="line">segments_tensors = torch.tensor([segments_ids])</span><br><span class="line"></span><br><span class="line"><span class="comment"># prepare Torch inputs</span></span><br><span class="line">tokens_tensors = torch.tensor([indexed_tokens])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load pre-trained model</span></span><br><span class="line">model = BertForMaskedLM.from_pretrained(<span class="string">'bert-base-uncased'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict all tokens</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    predictions = model(tokens_tensors, segments_tensors)</span><br></pre></td></tr></table></figure>

<ul>
<li>Match with proposals from SpellChecker</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_word</span><span class="params">(text_original, predictions, MASKIDS)</span>:</span></span><br><span class="line">    pred_words = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(MASKIDS)):</span><br><span class="line">        preds = torch.topk(predictions[<span class="number">0</span>, MASKIDS[i]], k=<span class="number">50</span>)</span><br><span class="line">        indices = preds.indices.tolist()</span><br><span class="line">        pred_list = tokenizer.convert_ids_to_tokens(indices)</span><br><span class="line">        sugg_list = suggestedwords[i]</span><br><span class="line">        sim_max = <span class="number">0</span></span><br><span class="line">        predicted_token = <span class="string">''</span></span><br><span class="line">        <span class="keyword">for</span> word1 <span class="keyword">in</span> pred_list:</span><br><span class="line">            <span class="keyword">for</span> word2 <span class="keyword">in</span> sugg_list:</span><br><span class="line">                s = SequenceMatcher(<span class="literal">None</span>, word1, word2).ratio()</span><br><span class="line">                <span class="keyword">if</span> s <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> s &gt; sim_max:</span><br><span class="line">                    sim_max = s</span><br><span class="line">                    predicted_token = word1</span><br><span class="line">        text_original = text_original.replace(<span class="string">'[MASK]'</span>, predicted_token, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> text_original</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">text_refined = predict_word(text, predictions, MASKIDS)</span><br><span class="line">print(text_refined)</span><br></pre></td></tr></table></figure>

<p>Output:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">national economy gained momentum in recent weeks as consumer spending Strengthened ,  manufacturing activity continued to rise ,  and producers  scheduled more investment in plant and equipment .</span><br></pre></td></tr></table></figure>

            <div class="post-copyright">
    <div class="content">
            <footer>
            <b>END</b
        </footer>
    </div>
</div>

      
        
            

        
    </div>
    <footer class="article-footer">
        
        
<div class="post-share">
    <a href="javascript:;" target="_blank" rel="noopener" id="share-sub" class="post-share-fab">
        <i class="fa fa-share-alt"></i>
    </a>
    <div class="post-share-list" id="share-list">
        <ul class="share-icons">
          <li>
            <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://steven-cheng-com.github.io/2020/07/18/spelling-correction-with-pretrained-bert/&title=《Spelling Correction with The Pretrained BERT Model》 — 双子塔科技&pic=https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2020/202007/20200718/0.png" data-title="Weibo">
              <i class="fa fa-weibo"></i>
            </a>
          </li>
          <li>
            <a class="weixin share-sns" id="wxFab" href="javascript:;" target="_blank" rel="noopener" data-title="Wechat">
              <i class="fa fa-weixin"></i>
            </a>
          </li>
          <li>
            <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://steven-cheng-com.github.io/2020/07/18/spelling-correction-with-pretrained-bert/&title=《Spelling Correction with The Pretrained BERT Model》 — 双子塔科技&source=梦想.自信.成就" data-title="QQ">
              <i class="fa fa-qq"></i>
            </a>
          </li>
          <li>
            <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://steven-cheng-com.github.io/2020/07/18/spelling-correction-with-pretrained-bert/" data-title="Facebook">
              <i class="fa fa-facebook"></i>
            </a>
          </li>
          <li>
            <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Spelling Correction with The Pretrained BERT Model》 — 双子塔科技&url=https://steven-cheng-com.github.io/2020/07/18/spelling-correction-with-pretrained-bert/&via=https://steven-cheng-com.github.io" data-title="Twitter">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
          <li>
            <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://steven-cheng-com.github.io/2020/07/18/spelling-correction-with-pretrained-bert/" data-title="Google+">
              <i class="fa fa-google-plus"></i>
            </a>
          </li>
        </ul>
     </div>
</div>
<div class="post-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;" target="_blank" rel="noopener" id="wxShare-close">×</a>
    <p>Scan QRcode and Share</p>
    <img src="//api.qrserver.com/v1/create-qr-code/?data=https://steven-cheng-com.github.io/2020/07/18/spelling-correction-with-pretrained-bert/" alt="Wecaht QRcode">
</div>

<div class="mask"></div>

        
        <ul class="article-footer-menu">
            
            
  <li class="article-footer-tags">
    <i class="fa fa-tags"></i>
      
    <a href="/tags/BERT/" class="color5">BERT</a>
      
    <a href="/tags/Spelling-Correction/" class="color5">Spelling-Correction</a>
      
  </li>

        </ul>
        
    </footer>
  </div>
</article>


    <aside class="post-toc-pos post-toc-top" id="post-toc">
        <nav class="post-toc-wrap">
            <ol class="post-toc"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Intro"><span class="post-toc-text">Intro</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Demo"><span class="post-toc-text">Demo</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Use-the-pretrained-BERT-model-to-predict-words"><span class="post-toc-text">Use the pretrained BERT model to predict words</span></a></li></ol>
        </nav>
    </aside>
    

<nav id="article-nav">
  
    <a href="/2020/08/16/spelling-correction-with-python-spellchecker/" id="article-nav-newer" class="article-nav-link-wrap">

      <span class="article-nav-title">
        <i class="fa fa-hand-o-left" aria-hidden="true"></i>
        
          Spelling Correction with Python Spellchecker
        
      </span>
    </a>
  
  
    <a href="/2020/06/19/Spelling-Corrector-from-Scratch/" id="article-nav-older" class="article-nav-link-wrap">
      <span class="article-nav-title">Spelling Corrector from Scratch</span>
      <i class="fa fa-hand-o-right" aria-hidden="true"></i>
    </a>
  
</nav>



    
</section>
        
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info" class="inner">
      
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        访问数：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


      <p>
        
      &copy; 2021 双子塔科技<br>
      </p>
    </div>
  </div>
</footer>

    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
<script>
  var mihoConfig = {
      root: "https://steven-cheng-com.github.io",
      animate: true,
      isHome: false,
      share: true,
      reward: 0
  }
</script>
<div class="sidebar">
    <div id="sidebar-search" title="Search">
        <i class="fa fa-search"></i>
    </div>
    <div id="sidebar-category" title="Categories">
        <i class="fa fa-book"></i>
    </div>
    <div id="sidebar-tag" title="Tags">
        <i class="fa fa-tags"></i>
    </div>
    <div id="sidebar-top">
        <span class="sidebar-top-icon"><i class="fa fa-angle-up"></i></span>
    </div>
</div>
<div class="sidebar-menu-box" id="sidebar-menu-box">
    <div class="sidebar-menu-box-container">
        <div id="sidebar-menu-box-categories">
            <a class="category-link" href="/categories/Algorithm/">Algorithm</a><a class="category-link" href="/categories/Computer-Vision/">Computer-Vision</a><a class="category-link" href="/categories/Deep-Learning/">Deep-Learning</a><a class="category-link" href="/categories/Machine-Learning/">Machine-Learning</a><a class="category-link" href="/categories/Natural-Language-Processing/">Natural-Language-Processing</a><a class="category-link" href="/categories/Programming/">Programming</a><a class="category-link" href="/categories/System/">System</a>
        </div>
        <div id="sidebar-menu-box-tags">
            <a href="/tags/Algorithm/" style="font-size: 13.33px;">Algorithm</a> <a href="/tags/Azure/" style="font-size: 10px;">Azure</a> <a href="/tags/BERT/" style="font-size: 16.67px;">BERT</a> <a href="/tags/Bagging/" style="font-size: 10px;">Bagging</a> <a href="/tags/Bias/" style="font-size: 10px;">Bias</a> <a href="/tags/Classification/" style="font-size: 10px;">Classification</a> <a href="/tags/Classifier/" style="font-size: 10px;">Classifier</a> <a href="/tags/Code/" style="font-size: 10px;">Code</a> <a href="/tags/Computer-Vision/" style="font-size: 13.33px;">Computer-Vision</a> <a href="/tags/Data-Science/" style="font-size: 13.33px;">Data-Science</a> <a href="/tags/Decision-Tree/" style="font-size: 10px;">Decision-Tree</a> <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep-Learning</a> <a href="/tags/Distance/" style="font-size: 10px;">Distance</a> <a href="/tags/Ensemble/" style="font-size: 10px;">Ensemble</a> <a href="/tags/Grammatical-Error-Correction/" style="font-size: 10px;">Grammatical-Error-Correction</a> <a href="/tags/Hierarchical-Clustering/" style="font-size: 10px;">Hierarchical-Clustering</a> <a href="/tags/Jupyter/" style="font-size: 10px;">Jupyter</a> <a href="/tags/Keras/" style="font-size: 10px;">Keras</a> <a href="/tags/Levenshtein/" style="font-size: 10px;">Levenshtein</a> <a href="/tags/Linear/" style="font-size: 10px;">Linear</a> <a href="/tags/Machine-Learning/" style="font-size: 16.67px;">Machine-Learning</a> <a href="/tags/Merge/" style="font-size: 10px;">Merge</a> <a href="/tags/MongoDB/" style="font-size: 10px;">MongoDB</a> <a href="/tags/Natural-Language-Processing/" style="font-size: 13.33px;">Natural-Language-Processing</a> <a href="/tags/OCR/" style="font-size: 10px;">OCR</a> <a href="/tags/Object-Detection/" style="font-size: 13.33px;">Object-Detection</a> <a href="/tags/Overfitting/" style="font-size: 10px;">Overfitting</a> <a href="/tags/Pre-trained-Model/" style="font-size: 10px;">Pre-trained-Model</a> <a href="/tags/Programming/" style="font-size: 10px;">Programming</a> <a href="/tags/PyCharm/" style="font-size: 10px;">PyCharm</a> <a href="/tags/PyTorch/" style="font-size: 10px;">PyTorch</a> <a href="/tags/Python/" style="font-size: 20px;">Python</a> <a href="/tags/R/" style="font-size: 10px;">R</a> <a href="/tags/Regression/" style="font-size: 10px;">Regression</a> <a href="/tags/Scikit-Learn/" style="font-size: 10px;">Scikit-Learn</a> <a href="/tags/Shortcuts/" style="font-size: 10px;">Shortcuts</a> <a href="/tags/Silhouette/" style="font-size: 10px;">Silhouette</a> <a href="/tags/Sklearn/" style="font-size: 10px;">Sklearn</a> <a href="/tags/Sort/" style="font-size: 10px;">Sort</a> <a href="/tags/Spelling-Correction/" style="font-size: 20px;">Spelling-Correction</a> <a href="/tags/Table-Detection/" style="font-size: 10px;">Table-Detection</a> <a href="/tags/TenforFlow/" style="font-size: 10px;">TenforFlow</a> <a href="/tags/Transformers/" style="font-size: 10px;">Transformers</a> <a href="/tags/Tree/" style="font-size: 20px;">Tree</a> <a href="/tags/VM/" style="font-size: 10px;">VM</a> <a href="/tags/Variance/" style="font-size: 10px;">Variance</a>
        </div>
    </div>
    <a href="javascript:;" target="_blank" rel="noopener" class="sidebar-menu-box-close">&times;</a>
</div>
<div class="mobile-header-menu-nav" id="mobile-header-menu-nav">
    <div class="mobile-header-menu-container">
        <span class="title">Menus</span>
        <ul class="mobile-header-menu-navbar">
            
            <li>
                <a  href="/">
                    <i class="fa fa-home"></i><span>首页</span>
                </a>
            </li>
            
            <li>
                <a  href="/archives">
                    <i class="fa fa-archives"></i><span>归档</span>
                </a>
            </li>
            
            <li>
                <a  href="/about">
                    <i class="fa fa-about"></i><span>关于</span>
                </a>
            </li>
            
        </ul>
    </div>
    <div class="mobile-header-tag-container">
        <span class="title">Tags</span>
        <div id="mobile-header-container-tags">
            <a href="/tags/Algorithm/" style="font-size: 13.33px;">Algorithm</a> <a href="/tags/Azure/" style="font-size: 10px;">Azure</a> <a href="/tags/BERT/" style="font-size: 16.67px;">BERT</a> <a href="/tags/Bagging/" style="font-size: 10px;">Bagging</a> <a href="/tags/Bias/" style="font-size: 10px;">Bias</a> <a href="/tags/Classification/" style="font-size: 10px;">Classification</a> <a href="/tags/Classifier/" style="font-size: 10px;">Classifier</a> <a href="/tags/Code/" style="font-size: 10px;">Code</a> <a href="/tags/Computer-Vision/" style="font-size: 13.33px;">Computer-Vision</a> <a href="/tags/Data-Science/" style="font-size: 13.33px;">Data-Science</a> <a href="/tags/Decision-Tree/" style="font-size: 10px;">Decision-Tree</a> <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep-Learning</a> <a href="/tags/Distance/" style="font-size: 10px;">Distance</a> <a href="/tags/Ensemble/" style="font-size: 10px;">Ensemble</a> <a href="/tags/Grammatical-Error-Correction/" style="font-size: 10px;">Grammatical-Error-Correction</a> <a href="/tags/Hierarchical-Clustering/" style="font-size: 10px;">Hierarchical-Clustering</a> <a href="/tags/Jupyter/" style="font-size: 10px;">Jupyter</a> <a href="/tags/Keras/" style="font-size: 10px;">Keras</a> <a href="/tags/Levenshtein/" style="font-size: 10px;">Levenshtein</a> <a href="/tags/Linear/" style="font-size: 10px;">Linear</a> <a href="/tags/Machine-Learning/" style="font-size: 16.67px;">Machine-Learning</a> <a href="/tags/Merge/" style="font-size: 10px;">Merge</a> <a href="/tags/MongoDB/" style="font-size: 10px;">MongoDB</a> <a href="/tags/Natural-Language-Processing/" style="font-size: 13.33px;">Natural-Language-Processing</a> <a href="/tags/OCR/" style="font-size: 10px;">OCR</a> <a href="/tags/Object-Detection/" style="font-size: 13.33px;">Object-Detection</a> <a href="/tags/Overfitting/" style="font-size: 10px;">Overfitting</a> <a href="/tags/Pre-trained-Model/" style="font-size: 10px;">Pre-trained-Model</a> <a href="/tags/Programming/" style="font-size: 10px;">Programming</a> <a href="/tags/PyCharm/" style="font-size: 10px;">PyCharm</a> <a href="/tags/PyTorch/" style="font-size: 10px;">PyTorch</a> <a href="/tags/Python/" style="font-size: 20px;">Python</a> <a href="/tags/R/" style="font-size: 10px;">R</a> <a href="/tags/Regression/" style="font-size: 10px;">Regression</a> <a href="/tags/Scikit-Learn/" style="font-size: 10px;">Scikit-Learn</a> <a href="/tags/Shortcuts/" style="font-size: 10px;">Shortcuts</a> <a href="/tags/Silhouette/" style="font-size: 10px;">Silhouette</a> <a href="/tags/Sklearn/" style="font-size: 10px;">Sklearn</a> <a href="/tags/Sort/" style="font-size: 10px;">Sort</a> <a href="/tags/Spelling-Correction/" style="font-size: 20px;">Spelling-Correction</a> <a href="/tags/Table-Detection/" style="font-size: 10px;">Table-Detection</a> <a href="/tags/TenforFlow/" style="font-size: 10px;">TenforFlow</a> <a href="/tags/Transformers/" style="font-size: 10px;">Transformers</a> <a href="/tags/Tree/" style="font-size: 20px;">Tree</a> <a href="/tags/VM/" style="font-size: 10px;">VM</a> <a href="/tags/Variance/" style="font-size: 10px;">Variance</a>
        </div>
    </div>
</div>
<div class="search-wrap">
    <span class="search-close">&times;</span>
        <a href="javascript:;" target="_blank" rel="noopener" class="header-icon waves-effect waves-circle waves-light" id="back">
            <i class="icon icon-lg icon-chevron-left"></i>
        </a>
        <input class="search-field" placeholder="Search..." id="keywords">
        <a id="search-submit" href="javascript:;" target="_blank" rel="noopener">
            <i class="fa fa-search"></i>
        </a>
    <div class="search-container" id="search-container">
        <ul class="search-result" id="search-result">
        </ul>
    </div>
</div>

<div id="search-tpl">
    <li class="search-result-item">
        <a href="{url}" class="search-item-li">
            <span class="search-item-li-title" title="{title}">{title}</span>
        </a>
    </li>
</div>
<script src="/js/search.js"></script>
<script src="/js/main.js"></script>


  <script src="//cdn.bootcss.com/particles.js/2.0.0/particles.min.js"></script>
  <div id="particles"></div>
  <script src="/js/particles.js"></script>







  <link rel="stylesheet" href="//cdn.bootcss.com/animate.css/3.5.0/animate.min.css">
  <script src="//cdn.bootcss.com/scrollReveal.js/3.0.5/scrollreveal.js"></script>
  <script src="/js/animate.js"></script>


  <script src="/js/pop-img.js"></script>
  <script>
     $(".article-entry p img").popImg();
  </script>

  </div>
</body>
</html>