<!doctype html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;center&gt;Stephen Cheng&lt;/center&gt;

&lt;h3 id=&#34;Intro&#34;&gt;&lt;a href=&#34;#Intro&#34; class=&#34;headerlink&#34; title=&#34;Intro&#34;&gt;&lt;/a&gt;Intro&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/steven-cheng-com/images/mast">
  <meta name="viewport" content="width=device-width, height=device-height, initial-scale=1.0, user-scalable=yes">
  <title>Automatic Clustering with Silhouette Analysis on Agglomerative Hierarchical Clustering | Stephen Cheng</title>

  <link rel="stylesheet" id="chosen-theme" href="https://rawgit.com/fiatjaf/classless/master/themes/plain/theme.css">
  <script>
let link = document.getElementById('chosen-theme')
let widget = document.createElement('div')
widget.style.position = 'absolute'
widget.style.right = '5px'
widget.style.top = '2px'
widget.style.background = 'beige'
widget.style.color = '#444'
widget.style.zIndex = 99
widget.style.padding = '4px 8px'
widget.innerHTML = `
<label>
  <p>No theme was set on <code>theme_config</code>.<br>
     Choose a theme from the list to experiment with it:</p>
  <select></select>
</label>
`
fetch('https://api.github.com/repos/fiatjaf/classless/contents/themes')
  .then(r => r.json())
  .then(files => {
    document.body.appendChild(widget)
    let select = document.querySelector('select')
    files
      .filter(f => f.type === 'dir')
      .forEach(f => {
        let option = document.createElement('option')
        option.value = f.name
        option.innerHTML = f.name
        select.appendChild(option)
      })
    let options = Array.from(select.querySelectorAll('option'))
    let chosen = options[parseInt(Math.random() * options.length)]
    chosen.selected = true
    link.href = link.href.replace(/themes\/[^\/]+/, `themes/${chosen.value}`)
    select.addEventListener('change', e => {
      let chosen = e.target.value
      link.href = link.href.replace(/themes\/[^\/]+/, `themes/${chosen}`)
    })
  })
  .catch(console.log)
  </script>

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
  <header role="banner">
      <a href="/">
        
          <img src="https://picsum.photos/640/480">
        
      </a>  
    <h1>
      <a href="/">Stephen Cheng</a>
    </h1>
    <aside>
      <p></p>
    </aside>
  </header>
  
  <main>
    <article>
  <header>
  
  <h1><a href="https://stephen-cheng.github.io/2020/03/22/automatic-clustering-with-silhouette-analysis-on-agglomerative-hierarchical-clustering/">Automatic Clustering with Silhouette Analysis on Agglomerative Hierarchical Clustering</a></h1>
  <aside>
    
    <time datetime="202020-03-22">
      March 22nd 2020
    </time>
    
    <ul>
    
      <li><a href="/tags/Hierarchical-Clustering/">Hierarchical-Clustering</a></li>
    
      <li><a href="/tags/Silhouette/">Silhouette</a></li>
    
    </ul>
  </aside>
</header>

  <div><p>&nbsp;</p>
<center>Stephen Cheng</center>

<h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p><img src="https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2020/202003/20200322/0.png" alt=""></p>
<p>Automatic clustering algorithms are algorithms that can perform clustering without prior knowledge of data sets, and determine the optimal number of clusters even in the presence of noise and outlier points.</p>
<h3 id="Silhouette-Analysis"><a href="#Silhouette-Analysis" class="headerlink" title="Silhouette Analysis"></a>Silhouette Analysis</h3><p><img src="https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2020/202003/20200322/1.png" alt=""></p>
<p>Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters. The silhouette score is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from âˆ’1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters. Thus silhouette analysis can be used to choose an optimal number for clusters automatically.</p>
<h3 id="Agglomerative-Hierarchical-Clustering"><a href="#Agglomerative-Hierarchical-Clustering" class="headerlink" title="Agglomerative Hierarchical Clustering"></a>Agglomerative Hierarchical Clustering</h3><p><img src="https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2020/202003/20200322/2.gif" alt=""></p>
<p>Hierarchical clustering algorithms fall into 2 branches: top-down or bottom-up. Bottom-up algorithms treat each data point as a single cluster at the outset and then successively merge (or agglomerate) pairs of clusters until all clusters have been merged into a single cluster that contains all data points. Bottom-up hierarchical clustering is therefore called hierarchical agglomerative clustering or HAC. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample.</p>
<p><b>The Steps of Hierarchical clustering:</b></p>
<ol>
<li><p>We start by treating each data point as a single cluster i.e. if there are N data points in our dataset then we have N clusters. We then select a distance metric that measures the distance between two clusters. As an example, we will use average linkage which defines the distance between two clusters to be the average distance between data points in the first cluster and data points in the second cluster.</p>
</li>
<li><p>On each iteration, two clusters are combined into one. The two clusters to be combined are selected as those with the smallest average linkage. I.e. according to our selected distance metric, these two clusters have the smallest distance between each other and therefore are the most similar and should be combined.</p>
</li>
<li><p>Step 2 is repeated until we reach the root of the tree that one cluster contains all data points. In this way we can select how many clusters we want in the end simply by choosing when to stop combining the clusters i.e. when we stop building the tree!</p>
</li>
</ol>
<h3 id="Automatic-Clustering"><a href="#Automatic-Clustering" class="headerlink" title="Automatic Clustering"></a>Automatic Clustering</h3><p>Since hierarchical clustering does not require us to specify the number of clusters and we can even select which number of clusters looks best since we are building a tree. Here we use the silhouette score to help us determine choosing the optimal number of clusters for clustering task automatically. An example of auto-clustering with silhouette analysis on agglomerative hierarchical clustering is shown as follows.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> AgglomerativeClustering</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler, normalize</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_score</span><br><span class="line"><span class="keyword">import</span> scipy.cluster.hierarchy <span class="keyword">as</span> shc</span><br></pre></td></tr></table></figure>
<p>Load and clean the data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X = pd.read_csv(<span class="string">'customer_info.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dropping the CUST_ID column from the data</span></span><br><span class="line">X = X.drop(<span class="string">'CUST_ID'</span>, axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Handling the missing values</span></span><br><span class="line">X.fillna(method =<span class="string">'ffill'</span>, inplace = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>Preprocess the data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scaling the data so that all the features become comparable</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_scaled = scaler.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalizing the data so that the data approximately  </span></span><br><span class="line"><span class="comment"># follows a Gaussian distribution</span></span><br><span class="line">X_normalized = normalize(X_scaled)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Converting the numpy array into a pandas DataFrame</span></span><br><span class="line">X_normalized = pd.DataFrame(X_normalized)</span><br></pre></td></tr></table></figure>

<p>Reduce the dimensionality:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pca = PCA(n_components = <span class="number">2</span>)</span><br><span class="line">X_new = pca.fit_transform(X_normalized)</span><br><span class="line">df_new = pd.DataFrame(X_new)</span><br><span class="line">df_new.columns = [<span class="string">'P1'</span>, <span class="string">'P2'</span>]</span><br></pre></td></tr></table></figure>

<p>Visualize the dendograms:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize =(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">plt.title(<span class="string">'Visualising Clustering'</span>)</span><br><span class="line">Dendrogram = shc.dendrogram((shc.linkage(df_new, method =<span class="string">'ward'</span>)))</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2020/202003/20200322/3.png" alt=""></p>
<p>Evaluate the clustering models:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">k = range(<span class="number">2</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">ac_list = [AgglomerativeClustering(n_clusters = i) <span class="keyword">for</span> i <span class="keyword">in</span> k]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Appending the silhouette scores</span></span><br><span class="line">silhouette_scores = &#123;&#125;</span><br><span class="line">silhouette_scores.fromkeys(k)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i,j <span class="keyword">in</span> enumerate(k):</span><br><span class="line">    silhouette_scores[j] = silhouette_score(df_new,</span><br><span class="line">                        ac_list[i].fit_predict(df_new))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plotting</span></span><br><span class="line">y = list(silhouette_scores.values())</span><br><span class="line">plt.bar(k, y)</span><br><span class="line">plt.xlabel(<span class="string">'Number of clusters'</span>, fontsize = <span class="number">20</span>)</span><br><span class="line">plt.ylabel(<span class="string">'S(i)'</span>, fontsize = <span class="number">20</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2020/202003/20200322/4.png" alt=""></p>
<p>From the result above, we can conclude that 3 clusters obtain the highest silhouette score so the optimal number of clusters is 3 in this case. The complete <code>code</code> and <code>dataset</code> can be found <a href="https://github.com/steven-cheng-com/auto_clustering_with_silhouette_analysis" target="_blank" rel="noopener">here</a></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] <a href="https://en.wikipedia.org/wiki/Automatic_clustering_algorithms" target="_blank" rel="noopener">Wikipedia-Automatic clustering algorithms</a><br>[2] <a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html" target="_blank" rel="noopener">https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html</a><br>[3] <a href="https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68" target="_blank" rel="noopener">The 5 Clustering Algorithms Data Scientists Need to Know</a></p>
</div>
</article>

  </main>
  <aside>
    <p>This is an `aside`, a useful place to put information of any kind: metadata about the site author (with or without pictures), brief information about the purposes of the site, a static collection of external links or anything else.</p>
<p>For some Classless themes pictures and data about the author may suit better in the `cover` and `description` site attributes (see `config.toml`), but most of the times that kind of information will fit here better.</p>

  </aside>
  <footer role="contentinfo">
    <p>A default site footer. If you don't know what to place here maybe you should just write your name plus the current year?</p>
<p>If you want to have a sitemap, a contact form or other complex stuff, most Classless themes will also handle it nicely. Or you can take a look at the <code>aside.html</code> partial.</p>

  </footer>
</body>
