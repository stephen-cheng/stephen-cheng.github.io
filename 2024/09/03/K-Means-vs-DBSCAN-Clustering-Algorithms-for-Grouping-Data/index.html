<!DOCTYPE html>
<html lang="en">

<head>

  <!-- Minima -->
  <!-- Hexo theme created by @adisaktijrs -->

  <!-- Basic Page Needs
  ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
  <meta charset="utf-8">

  
  <title>K-Means vs DBSCAN - Clustering Algorithms for Grouping Data</title>
  
  <link rel="canonical" href="https://stephen-cheng.github.io/2024/09/03/K-Means-vs-DBSCAN-Clustering-Algorithms-for-Grouping-Data/">
  
  <meta name="description" content="&amp;nbsp; Stephen Cheng  IntroClustering is a popular unsupervised machine learning technique used to identify groups of similar objects in a dataset. It">
  
  
  <meta name="keywords" content="AI, Tech, CS">
  
  <meta name="author" content="Stephen Cheng">
  
  
  
  <meta property="og:site_name" content="Stephen Cheng" />
  <meta property="og:type" content="article" />
  <meta property="og:title" content="K-Means vs DBSCAN - Clustering Algorithms for Grouping Data" />
  
  <meta property="og:description" content="&amp;nbsp; Stephen Cheng  IntroClustering is a popular unsupervised machine learning technique used to identify groups of similar objects in a dataset. It">
  
  <meta property="og:url" content="https://stephen-cheng.github.io/2024/09/03/K-Means-vs-DBSCAN-Clustering-Algorithms-for-Grouping-Data/" />

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="K-Means vs DBSCAN - Clustering Algorithms for Grouping Data">
  
  <meta name="twitter:description" content="&amp;nbsp; Stephen Cheng  IntroClustering is a popular unsupervised machine learning technique used to identify groups of similar objects in a dataset. It">
  
  
  
  
  <meta name="twitter:url" content="https://stephen-cheng.github.io/2024/09/03/K-Means-vs-DBSCAN-Clustering-Algorithms-for-Grouping-Data/" />

  <!-- Mobile Specific Metas
  ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Preload fonts
  ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
  <link rel="preload" href="/fonts/dm-serif-display-v4-latin-regular.woff2" as="font" type="font/woff2" crossorigin>
  <link rel="preload" href="/fonts/inter-v2-latin-regular.woff2" as="font" type="font/woff2" crossorigin>

  <!-- CSS
  ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
  
<link rel="stylesheet" href="/css/normalize.css">

  
<link rel="stylesheet" href="/css/skeleton.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
<link rel="stylesheet" href="/css/prism-dark.css">

  
<link rel="stylesheet" href="/css/prism-line-numbers.css">

  <!-- User css -->
  
  
<link rel="stylesheet" href="/css/user.css">

  

  <!-- Favicon
  ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
  <link rel="icon" type="image/png" href="/images/favicon.png">

  <!-- Custom Theme Color Style
  ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
  <style>
  a:not(.icon) {
    text-decoration-color: #0FA0CE;
    background-image: linear-gradient(
      to bottom,
      rgba(0, 0, 0, 0) 50%,
      #0FA0CE 50%
    );
  }
  blockquote {
    border-left: 8px solid #0FA0CE;
  }
  .nanobar .bar {
    background: #0FA0CE;
  }
  .button.button-primary:hover,
  button.button-primary:hover,
  input[type="submit"].button-primary:hover,
  input[type="reset"].button-primary:hover,
  input[type="button"].button-primary:hover,
  .button.button-primary:focus,
  button.button-primary:focus,
  input[type="submit"].button-primary:focus,
  input[type="reset"].button-primary:focus,
  input[type="button"].button-primary:focus {
    background-color: #0FA0CE;
    border-color: #0FA0CE;
  }
  input[type="email"]:focus,
  input[type="number"]:focus,
  input[type="search"]:focus,
  input[type="text"]:focus,
  input[type="tel"]:focus,
  input[type="url"]:focus,
  input[type="password"]:focus,
  textarea:focus,
  select:focus {
    border: 1px solid #0FA0CE;
  }
</style>

  <!-- Google Analytics (With Privacy Settings On)
  ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
  

  
  <script src="/js/pic.min.js" defer></script>
  

  

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
  <div class="container">
    <div class="row">
      <div>

        <div class="row">
  <div class="two columns" style="max-width: 50px">
    <h1 class="mt-2 mode">
      <div onclick=setDarkMode(true) id="darkBtn">üåë</div>
      <div onclick=setDarkMode(false) id="lightBtn" class=hidden>‚òÄÔ∏è</div>
      <script >
        if (localStorage.getItem('preferredTheme') == 'dark') {
          setDarkMode(true)
        }
        function setDarkMode(isDark) {
          var darkBtn = document.getElementById('darkBtn')
          var lightBtn = document.getElementById('lightBtn')
          if (isDark) {
            lightBtn.style.display = "block"
            darkBtn.style.display = "none"
            localStorage.setItem('preferredTheme', 'dark');
          } else {
            lightBtn.style.display = "none"
            darkBtn.style.display = "block"
            localStorage.removeItem('preferredTheme');
          }
          document.body.classList.toggle("darkmode");
        }
      </script>
    </h1>
  </div>

  <div class="six columns ml-1">
    <h1 class="mt-2">
      Stephen Cheng
    </h1>
  </div>

  <div class="twelve columns">
    <div class="row">
      <div class="nine columns left">
        <a href="/">Home</a>
        
          
          <a href="/archives" class="ml">Archives</a>
          
        
          
          <a href="/about" class="ml">About</a>
          
        
        
      </div>
    </div>
    <hr style="margin-bottom: 2.6rem">
  </div>
</div>

        <div class="trans">
            <h2>K-Means vs DBSCAN - Clustering Algorithms for Grouping Data</h2>

  <p>&nbsp;</p>
<center>Stephen Cheng</center>

<h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p>Clustering is a popular unsupervised machine learning technique used to identify groups of similar objects in a dataset. It has numerous applications in various fields, such as image recognition, customer segmentation, and anomaly detection. Two popular clustering algorithms are DBSCAN and K-Means. Each of these algorithms excels in different scenarios and has distinct advantages and limitations. In this guide, we will explore the key differences between DBSCAN and K-Means and how to implement them in Python using scikit-learn, a popular machine learning library. We will also discuss when to use each algorithm based on the characteristics of the dataset and the problem at hand. So let‚Äôs dive in!</p>
<h3 id="K-Means-Clustering-Algorithm"><a href="#K-Means-Clustering-Algorithm" class="headerlink" title="K-Means Clustering Algorithm"></a>K-Means Clustering Algorithm</h3><p>K-Means is a centroid-based algorithm that partitions data into k clusters based on the mean distance between points and their assigned centroid. The algorithm aims to minimize the sum of squared distances between each point and its assigned centroid. K-Means is widely used due to its simplicity and efficiency.</p>
<ul>
<li><p><strong>How K-Means Works</strong></p>
<ol>
<li>Choose K: Start by selecting the number of clusters <em>K</em>.</li>
<li>Initialize Centroids: Randomly place K centroids (one for each cluster) in the data space.</li>
<li>Assign Points to Clusters: Assign each data point to the nearest centroid based on Euclidean distance.</li>
<li>Update Centroids: Recalculate the centroids by taking the mean of all data points assigned to each cluster.</li>
<li>Repeat: Repeat the assignment and update steps until the centroids no longer change significantly (convergence).</li>
</ol>
</li>
<li><p><strong>Mathematical Representation</strong></p>
</li>
</ul>
<p>Given a set of data points X={x1,x2,‚Ä¶,xn}, K-Means aims to minimize the following objective function:</p>
<p><img src="https://raw.githubusercontent.com/stephen-cheng/images/refs/heads/master/blog/2024/0903_2.jpg" alt=""></p>
<p>Where:</p>
<p>  <em>K</em> is the number of clusters,<br>  <em>Ci</em> is the set of points in cluster <em>i</em>,<br>  <em>Œºi*‚Äã is the centroid of cluster *i</em>,<br>  <em>|x‚àíŒºi|¬≤</em> is the squared distance between point <em>x</em> and the centroid <em>Œºi</em>.</p>
<ul>
<li><p><strong>Advantages of K-Means</strong></p>
<ul>
<li>Simplicity: Easy to implement and computationally efficient, making it suitable for large datasets.</li>
<li>Scalability: K-Means performs well with a large number of data points and clusters.</li>
<li>Interpretability: The algorithm is intuitive, and the results are easy to interpret visually, especially in 2D space.</li>
</ul>
</li>
<li><p><strong>Limitations of K-Means</strong></p>
<ul>
<li>Predefined Number of Clusters: The number of clusters <em>K</em> must be defined beforehand, which can be difficult to determine.</li>
<li>Sensitivity to Outliers: K-Means is highly sensitive to outliers, as they can disproportionately affect the position of centroids.</li>
<li>Non-Spherical Clusters: K-Means assumes clusters are spherical and evenly sized, making it ineffective for complex, irregularly shaped clusters.</li>
</ul>
</li>
<li><p><strong>Implementation in Python</strong></p>
</li>
</ul>
<p>To implement K-Means in Python, we can use the scikit-learn library. Here‚Äôs an example. We initialize a KMeans object with n_clusters (the number of clusters to form) set to 3 and fit the model on our dataset X.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="comment"># Create a KMeans instance with 3 clusters</span></span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># Fit the model to data</span></span><br><span class="line">kmeans.fit(X)</span><br><span class="line"><span class="comment"># Predict cluster labels for new data points</span></span><br><span class="line">labels = kmeans.predict(new_data)</span><br></pre></td></tr></table></figure>

<h3 id="DBSCAN-Clustering-Algorithm"><a href="#DBSCAN-Clustering-Algorithm" class="headerlink" title="DBSCAN Clustering Algorithm"></a>DBSCAN Clustering Algorithm</h3><p>DBSCAN stands for Density-Based Spatial Clustering of Applications with Noise. It is a density-based algorithm that groups together points that are close to each other based on a density criterion. Points that are not part of any cluster are considered noise. DBSCAN is particularly useful when dealing with datasets that have irregular shapes and different densities.</p>
<ul>
<li><p><strong>How DBSCAN Works</strong></p>
<ol>
<li>Choose Parameters: Set the distance threshold <em>Œµ</em> and the minimum number of points <em>MinPts</em> required to form a dense region (core point).</li>
<li>Core Points and Neighborhoods: Identify core points that have at least <em>MinPts</em> points within the <em>Œµ-radius</em> neighborhood.</li>
<li>Cluster Formation: Form clusters by connecting core points and their neighbors.</li>
<li>Handle Noise: Any point that is not part of a core point‚Äôs neighborhood and cannot be assigned to any cluster is classified as noise.</li>
</ol>
</li>
<li><p><strong>Mathematical Representation</strong></p>
</li>
</ul>
<p>A point <em>p</em> is core point if there are at least <em>MinPts</em> points within <em>Œµ-radius</em> neighborhood of <em>p</em>. Formally, the neighborhood <em>N(p)</em> of point <em>p</em> is defined as:</p>
<p><img src="https://raw.githubusercontent.com/stephen-cheng/images/refs/heads/master/blog/2024/0903_3.jpg" alt=""></p>
<p>where:</p>
<p>  <em>D</em> is the dataset,<br>  <em>dist(p,q)</em> is the distance between points <em>p</em> and <em>q</em>,<br>  <em>Œµ</em> is the maximum distance threshold.</p>
<ul>
<li><p><strong>Advantages of DBSCAN</strong></p>
<ul>
<li>No Need for Predefined Clusters: DBSCAN can automatically determine the number of clusters based on the density of data points.</li>
<li>Handles Noise and Outliers: DBSCAN can identify outliers as noise, making it robust to noisy data.</li>
<li>Clusters of Arbitrary Shapes: DBSCAN can identify clusters of various shapes, not just spherical ones, making it versatile for complex datasets.</li>
</ul>
</li>
<li><p><strong>Limitations of DBSCAN</strong></p>
<ul>
<li>Sensitive to Parameter Choice: The performance of DBSCAN heavily depends on the correct choice of <em>Œµ</em> and <em>MinPts</em>. Inappropriate parameter selection can lead to poor results.</li>
<li>Not Ideal for Varying Densities: DBSCAN struggles when clusters have varying densities, as it may merge dense and sparse regions into the same cluster.</li>
<li>High-Dimensional Data: DBSCAN‚Äôs performance degrades in high-dimensional data because the concept of distance becomes less meaningful in higher dimensions (the curse of dimensionality).</li>
</ul>
</li>
<li><p><strong>Implementation in Python</strong></p>
</li>
</ul>
<p>Let‚Äôs take a look at some Python code examples for implementing these algorithms. We first initialize a DBSCAN object with eps (the radius of neighborhood) set to 0.5 and min_samples (the minimum number of points required to form a dense region) set to 5. We then fit the model on our dataset X.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of using DBSCAN</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> DBSCAN</span><br><span class="line">dbscan = DBSCAN(eps=<span class="number">0.5</span>, min_samples=<span class="number">5</span>)</span><br><span class="line">dbscan.fit(X)</span><br></pre></td></tr></table></figure>

<h3 id="Differences-Between-K-Means-and-DBSCAN"><a href="#Differences-Between-K-Means-and-DBSCAN" class="headerlink" title="Differences Between K-Means and DBSCAN"></a>Differences Between K-Means and DBSCAN</h3><p><img src="https://raw.githubusercontent.com/stephen-cheng/images/refs/heads/master/blog/2024/0903_4.jpg" alt=""></p>
<ul>
<li><strong>Use K-Means if:</strong><ul>
<li>You Know the Number of Clusters: K-Means is a good choice when you already have an idea of how many clusters <em>K</em> exist in the data.</li>
<li>The Data is Well-Separated: If the clusters are compact and well-separated in Euclidean space, K-Means can efficiently identify them.</li>
<li>You Have a Large Dataset: K-Means is computationally efficient and can handle large datasets quickly.</li>
</ul>
</li>
</ul>
<p>Example: A customer segmentation task where the dataset is large and consists of distinct, evenly distributed clusters would be ideal for K-Means clustering.</p>
<ul>
<li><p><strong>Use DBSCAN if:</strong></p>
<ul>
<li>You Don‚Äôt Know the Number of Clusters: DBSCAN automatically identifies the number of clusters based on the density of the data, making it useful when you don‚Äôt have prior knowledge of <em>K</em>.</li>
<li>You Have Irregularly Shaped Clusters: DBSCAN excels at finding clusters that are non-spherical or complex in shape, such as clusters in spatial data.</li>
<li>You Want to Handle Outliers: If your dataset contains noise or outliers, DBSCAN can effectively separate noise from the dense regions of the data.</li>
</ul>
<p>Example: In a geographical mapping application, where data points (e.g., earthquake epicenters) are scattered irregularly and outliers need to be identified, DBSCAN is an ideal choice.</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/stephen-cheng/images/refs/heads/master/blog/2024/0903_1.jpg" alt=""></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>K-Means is a simple and fast algorithm that works well when the data is well-separated into clusters. However, it requires the number of clusters to be specified beforehand, which can be a challenge in real-world applications where the number of clusters is not known. On the other hand, DBSCAN is a more flexible algorithm that does not require the number of clusters to be specified beforehand. It can also handle noise and outliers well. However, it may not work well when the density of the data points varies greatly across different parts of the dataset. Both algorithms have their strengths and weaknesses, so it‚Äôs important to experiment with both and compare their results before making a final decision. By leveraging existing libraries such as scikit-learn in Python, you can easily apply these algorithms to your own datasets and gain valuable insights into your data.</p>

  <p><a class="classtest-link" href="/tags/Clustering/" rel="tag">Clustering</a>, <a class="classtest-link" href="/tags/DBSCAN/" rel="tag">DBSCAN</a>, <a class="classtest-link" href="/tags/Kmeans/" rel="tag">Kmeans</a> ‚Äî Sep 3, 2024</p>
  


        </div>
        <div class="row mt-2">
  <h3>Search</h3>
  <div><input id="search-text" title="search" class="search-text" type="text" placeholder="search......"></div>
  <div style="margin-top: 1.5rem;">
    <ul id="result"></ul>
  </div>
</div>
        <div class="row mt-2">
  
    <div class="eight columns">
      <p id="madewith">Made with ‚ù§Ô∏è and ‚òÄÔ∏è
        <!-- <a class="footer-link icon" href="https://hexo.io" target="_blank" style="text-decoration: none;" rel="noreferrer" aria-label="Hexo.io"> -->
        <!-- <svg class="hexo svg-hov" width="14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><title>Hexo.js</title><path d="M12 .007L1.57 6.056V18.05L12 23.995l10.43-6.049V5.952L12 .007zm4.798 17.105l-.939.521-.939-.521V12.94H9.08v4.172l-.94.521-.938-.521V6.89l.939-.521.939.521v4.172h5.84V6.89l.94-.521.938.521v10.222z"/></svg> -->
        <!-- </a> -->
        
        on <a href="https://en.wikipedia.org/wiki/Earth" target="_blank" rel="noreferrer">Earth</a>.</p>
        
    </div>

    <!-- Sepcial thanks to https://simpleicons.org/ for the icons -->
    <div class="four columns mb-3 posisi" >
      
      <a class="ml-0 footer-link icon" href="https://github.com/stephen-cheng" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="GitHub">
        <svg class="github svg-hov" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
      </a>
      

      
      <a class="ml-0 footer-link icon" href="https://linkedin.com/in/stephen-cheng" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="LinkedIn">
        <svg class="linkedin svg-hov" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>LinkedIn</title><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
      </a>
      

      

      
        <a class="ml-0 footer-link icon" href="https://facebook.com/stephen.cheeng" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="Facebook">
          <svg class="facebook svg-hov" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Facebook</title><path d="M8.258 4.458c0-0.144 0.02-0.455 0.06-0.931c0.043-0.477 0.223-0.976 0.546-1.5c0.32-0.522 0.839-0.991 1.561-1.406 C11.144 0.208 12.183 0 13.539 0h3.82v4.163h-2.797c-0.277 0-0.535 0.104-0.768 0.309c-0.231 0.205-0.35 0.4-0.35 0.581v2.59h3.914 c-0.041 0.507-0.086 1-0.138 1.476l-0.155 1.258c-0.062 0.425-0.125 0.819-0.187 1.182h-3.462v11.542H8.258V11.558H5.742V7.643 h2.516V4.458z"/></svg>
        </a>
      

      
      <a class="ml-0 footer-link icon" href="https://instagram.com/stephen.cheeng" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="Instagram">
        <svg class="instagram svg-hov" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Instagram</title><path d="M12 0C8.74 0 8.333.015 7.053.072 5.775.132 4.905.333 4.14.63c-.789.306-1.459.717-2.126 1.384S.935 3.35.63 4.14C.333 4.905.131 5.775.072 7.053.012 8.333 0 8.74 0 12s.015 3.667.072 4.947c.06 1.277.261 2.148.558 2.913.306.788.717 1.459 1.384 2.126.667.666 1.336 1.079 2.126 1.384.766.296 1.636.499 2.913.558C8.333 23.988 8.74 24 12 24s3.667-.015 4.947-.072c1.277-.06 2.148-.262 2.913-.558.788-.306 1.459-.718 2.126-1.384.666-.667 1.079-1.335 1.384-2.126.296-.765.499-1.636.558-2.913.06-1.28.072-1.687.072-4.947s-.015-3.667-.072-4.947c-.06-1.277-.262-2.149-.558-2.913-.306-.789-.718-1.459-1.384-2.126C21.319 1.347 20.651.935 19.86.63c-.765-.297-1.636-.499-2.913-.558C15.667.012 15.26 0 12 0zm0 2.16c3.203 0 3.585.016 4.85.071 1.17.055 1.805.249 2.227.415.562.217.96.477 1.382.896.419.42.679.819.896 1.381.164.422.36 1.057.413 2.227.057 1.266.07 1.646.07 4.85s-.015 3.585-.074 4.85c-.061 1.17-.256 1.805-.421 2.227-.224.562-.479.96-.899 1.382-.419.419-.824.679-1.38.896-.42.164-1.065.36-2.235.413-1.274.057-1.649.07-4.859.07-3.211 0-3.586-.015-4.859-.074-1.171-.061-1.816-.256-2.236-.421-.569-.224-.96-.479-1.379-.899-.421-.419-.69-.824-.9-1.38-.165-.42-.359-1.065-.42-2.235-.045-1.26-.061-1.649-.061-4.844 0-3.196.016-3.586.061-4.861.061-1.17.255-1.814.42-2.234.21-.57.479-.96.9-1.381.419-.419.81-.689 1.379-.898.42-.166 1.051-.361 2.221-.421 1.275-.045 1.65-.06 4.859-.06l.045.03zm0 3.678c-3.405 0-6.162 2.76-6.162 6.162 0 3.405 2.76 6.162 6.162 6.162 3.405 0 6.162-2.76 6.162-6.162 0-3.405-2.76-6.162-6.162-6.162zM12 16c-2.21 0-4-1.79-4-4s1.79-4 4-4 4 1.79 4 4-1.79 4-4 4zm7.846-10.405c0 .795-.646 1.44-1.44 1.44-.795 0-1.44-.646-1.44-1.44 0-.794.646-1.439 1.44-1.439.793-.001 1.44.645 1.44 1.439z"/></svg>
      </a>
      

      

    </div>
  
</div>

      </div>

    </div>

  </div>
  <script src="/js/nanobar.min.js"></script>

  <script>
    var options = {
      classname: 'nanobar',
      id: 'myNanobar'
    };
    var nanobar = new Nanobar(options);
    nanobar.go(30);
    nanobar.go(76);
    nanobar.go(100);
  </script>

</body>

</html>