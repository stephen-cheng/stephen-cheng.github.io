<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head>
    <meta charset="utf-8">
<title>Category: Machine-Learning - Stephen Cheng</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">




<meta name="description" content="Personal sharings about Tech & Work.">



<meta name="keywords" content="AI, Tech, CS">








<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">


<link rel="stylesheet" href="/css/style.css">


<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>



<meta name="generator" content="Hexo 4.2.1"></head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                <img src="/images/logo.png" alt="" height="28">
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item "
               href="/about">About</a>
            
            <a class="navbar-item "
               href="/contact">Contact</a>
            
            <a class="navbar-item "
               href="https://www.instagram.com">Instagram</a>
            
            <a class="navbar-item "
               href="https://www.linkedin.com">LinkedIn</a>
            
            <a class="navbar-item "
               href="https://www.facebook.com">Facebook</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            
            
        </div>
    </div>
</nav>

    <section class="section section-heading">
    <div class="container">
        <div class="content">
            <h5><i class="far fa-folder"></i>Machine-Learning</h5>
        </div>
    </div>
</section>
<section class="section">
    <div class="container">
    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2020/03/22/automatic-clustering-with-silhouette-analysis-on-agglomerative-hierarchical-clustering/" itemprop="url">Automatic Clustering with Silhouette Analysis on Agglomerative Hierarchical Clustering</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-03-22T14:32:33.000Z" itemprop="datePublished">Mar 22 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Machine-Learning/">Machine-Learning</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            6 minutes read (About 910 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <p>&nbsp;</p>
<center>Stephen Cheng</center>

<h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p><img src="https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2020/202003/20200322/0.png" alt=""></p>
<p>Automatic clustering algorithms are algorithms that can perform clustering without prior knowledge of data sets, and determine the optimal number of clusters even in the presence of noise and outlier points.</p>
<h3 id="Silhouette-Analysis"><a href="#Silhouette-Analysis" class="headerlink" title="Silhouette Analysis"></a>Silhouette Analysis</h3><p><img src="https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2020/202003/20200322/1.png" alt=""></p>
<p>Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters. The silhouette score is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters. Thus silhouette analysis can be used to choose an optimal number for clusters automatically.</p>
<h3 id="Agglomerative-Hierarchical-Clustering"><a href="#Agglomerative-Hierarchical-Clustering" class="headerlink" title="Agglomerative Hierarchical Clustering"></a>Agglomerative Hierarchical Clustering</h3><p><img src="https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2020/202003/20200322/2.gif" alt=""></p>
<p>Hierarchical clustering algorithms fall into 2 branches: top-down or bottom-up. Bottom-up algorithms treat each data point as a single cluster at the outset and then successively merge (or agglomerate) pairs of clusters until all clusters have been merged into a single cluster that contains all data points. Bottom-up hierarchical clustering is therefore called hierarchical agglomerative clustering or HAC. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample.</p>
<p><b>The Steps of Hierarchical clustering:</b></p>
<ol>
<li><p>We start by treating each data point as a single cluster i.e. if there are N data points in our dataset then we have N clusters. We then select a distance metric that measures the distance between two clusters. As an example, we will use average linkage which defines the distance between two clusters to be the average distance between data points in the first cluster and data points in the second cluster.</p>
</li>
<li><p>On each iteration, two clusters are combined into one. The two clusters to be combined are selected as those with the smallest average linkage. I.e. according to our selected distance metric, these two clusters have the smallest distance between each other and therefore are the most similar and should be combined.</p>
</li>
<li><p>Step 2 is repeated until we reach the root of the tree that one cluster contains all data points. In this way we can select how many clusters we want in the end simply by choosing when to stop combining the clusters i.e. when we stop building the tree!</p>
</li>
</ol>
<h3 id="Automatic-Clustering"><a href="#Automatic-Clustering" class="headerlink" title="Automatic Clustering"></a>Automatic Clustering</h3><p>Since hierarchical clustering does not require us to specify the number of clusters and we can even select which number of clusters looks best since we are building a tree. Here we use the silhouette score to help us determine choosing the optimal number of clusters for clustering task automatically. An example of auto-clustering with silhouette analysis on agglomerative hierarchical clustering is shown as follows.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> AgglomerativeClustering</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler, normalize</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_score</span><br><span class="line"><span class="keyword">import</span> scipy.cluster.hierarchy <span class="keyword">as</span> shc</span><br></pre></td></tr></table></figure>
<p>Load and clean the data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X = pd.read_csv(<span class="string">'customer_info.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dropping the CUST_ID column from the data</span></span><br><span class="line">X = X.drop(<span class="string">'CUST_ID'</span>, axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Handling the missing values</span></span><br><span class="line">X.fillna(method =<span class="string">'ffill'</span>, inplace = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>Preprocess the data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scaling the data so that all the features become comparable</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_scaled = scaler.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalizing the data so that the data approximately  </span></span><br><span class="line"><span class="comment"># follows a Gaussian distribution</span></span><br><span class="line">X_normalized = normalize(X_scaled)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Converting the numpy array into a pandas DataFrame</span></span><br><span class="line">X_normalized = pd.DataFrame(X_normalized)</span><br></pre></td></tr></table></figure>

<p>Reduce the dimensionality:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pca = PCA(n_components = <span class="number">2</span>)</span><br><span class="line">X_new = pca.fit_transform(X_normalized)</span><br><span class="line">df_new = pd.DataFrame(X_new)</span><br><span class="line">df_new.columns = [<span class="string">'P1'</span>, <span class="string">'P2'</span>]</span><br></pre></td></tr></table></figure>

<p>Visualize the dendograms:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize =(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">plt.title(<span class="string">'Visualising Clustering'</span>)</span><br><span class="line">Dendrogram = shc.dendrogram((shc.linkage(df_new, method =<span class="string">'ward'</span>)))</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2020/202003/20200322/3.png" alt=""></p>
<p>Evaluate the clustering models:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">k = range(<span class="number">2</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">ac_list = [AgglomerativeClustering(n_clusters = i) <span class="keyword">for</span> i <span class="keyword">in</span> k]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Appending the silhouette scores</span></span><br><span class="line">silhouette_scores = &#123;&#125;</span><br><span class="line">silhouette_scores.fromkeys(k)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i,j <span class="keyword">in</span> enumerate(k):</span><br><span class="line">    silhouette_scores[j] = silhouette_score(df_new,</span><br><span class="line">                        ac_list[i].fit_predict(df_new))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plotting</span></span><br><span class="line">y = list(silhouette_scores.values())</span><br><span class="line">plt.bar(k, y)</span><br><span class="line">plt.xlabel(<span class="string">'Number of clusters'</span>, fontsize = <span class="number">20</span>)</span><br><span class="line">plt.ylabel(<span class="string">'S(i)'</span>, fontsize = <span class="number">20</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2020/202003/20200322/4.png" alt=""></p>
<p>From the result above, we can conclude that 3 clusters obtain the highest silhouette score so the optimal number of clusters is 3 in this case. The complete <code>code</code> and <code>dataset</code> can be found <a href="https://github.com/steven-cheng-com/auto_clustering_with_silhouette_analysis" target="_blank" rel="noopener">here</a></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] <a href="https://en.wikipedia.org/wiki/Automatic_clustering_algorithms" target="_blank" rel="noopener">Wikipedia-Automatic clustering algorithms</a><br>[2] <a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html" target="_blank" rel="noopener">https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html</a><br>[3] <a href="https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68" target="_blank" rel="noopener">The 5 Clustering Algorithms Data Scientists Need to Know</a></p>

    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2019/06/18/Python-for-Data-Science-Cheat-Sheet-with-Scikit-Learn/" itemprop="url">Python for Data Science Cheat Sheet with Scikit-Learn</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2019-06-18T20:33:19.000Z" itemprop="datePublished">Jun 18 2019</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Machine-Learning/">Machine-Learning</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            6 minutes read (About 860 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <p>&nbsp;</p>
<center>Stephen Cheng</center>


<p><img src="https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2019/201906/20190618/0.png" alt=""></p>
<hr>
<h3 id="Intro-of-Scikit-Learn"><a href="#Intro-of-Scikit-Learn" class="headerlink" title="Intro of Scikit-Learn"></a>Intro of Scikit-Learn</h3><p>Scikit-learn is an open source Python library that implements a range of machine learning, data preprocessing, cross-validation and visualization algorithms using a unified interface.</p>
<p>The whole workflow of data science includes:</p>
<ul>
<li>Loading the data</li>
<li>Training and test data</li>
<li>Preprocessing ehe data</li>
<li>Create your model</li>
<li>Model fitting</li>
<li>Prediction</li>
<li>Evaluate your model’a performance</li>
<li>Tune your model</li>
</ul>
<p>Here I give you a basic example for reference.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn <span class="keyword">import</span> neighbors, datasets, preprocessing</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>iris = datasets.load_iris()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X, y = iris.data[:, :<span class="number">2</span>], iris.target</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">33</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>scaler = preprocessing.StandardScaler().fit(X_train)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_train = scaler.transform(X_train)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_test = scaler.transform(X_test)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>knn = neighbors.KNeighborsClassifier(n_neighbors=<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>knn.fit(X_train, y_train)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_pred = knn.predict(X_test)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>accuracy_score(y_test, y_pred)</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="Loading-The-Data"><a href="#Loading-The-Data" class="headerlink" title="Loading The Data"></a>Loading The Data</h3><p>Your data needs to be numeric and stored as NumPy arrays or SciPy sparse matrices. Other types that are convertible to numeric arrays, such as Pandas DataFrame, are also acceptable.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = np.random.random((<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = np.array([<span class="string">'M'</span>,<span class="string">'M'</span>,<span class="string">'F'</span>,<span class="string">'F'</span>,<span class="string">'M'</span>,<span class="string">'F'</span>,<span class="string">'M'</span>,<span class="string">'M'</span>,<span class="string">'F'</span>,<span class="string">'F'</span>,<span class="string">'F'</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X[X &lt; <span class="number">0.7</span>] = <span class="number">0</span></span><br></pre></td></tr></table></figure>

<hr>
<h3 id="Training-And-Test-Data"><a href="#Training-And-Test-Data" class="headerlink" title="Training And Test Data"></a>Training And Test Data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="Preprocessing-The-Data"><a href="#Preprocessing-The-Data" class="headerlink" title="Preprocessing The Data"></a>Preprocessing The Data</h3><h4 id="Imputing-Missing-Values"><a href="#Imputing-Missing-Values" class="headerlink" title="Imputing Missing Values"></a>Imputing Missing Values</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>imp = Imputer(missing_values=<span class="number">0</span>, strategy=<span class="string">'mean'</span>, axis=<span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>imp.fit_transform(X_train)</span><br></pre></td></tr></table></figure>

<h4 id="Standardization"><a href="#Standardization" class="headerlink" title="Standardization"></a>Standardization</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>scaler = StandardScaler().fit(X_train)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>standardized_X = scaler.transform(X_train)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>standardized_X_test = scaler.transform(X_test)</span><br></pre></td></tr></table></figure>

<h4 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Normalizer</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>scaler = Normalizer().fit(X_train)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>normalized_X = scaler.transform(X_train)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>normalized_X_test = scaler.transform(X_test)</span><br></pre></td></tr></table></figure>

<h4 id="Binarization"><a href="#Binarization" class="headerlink" title="Binarization"></a>Binarization</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Binarizer</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>binarizer = Binarizer(threshold=<span class="number">0.0</span>).fit(X)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>binary_X = binarizer.transform(X)</span><br></pre></td></tr></table></figure>

<h4 id="Encoding-Categorical-Features"><a href="#Encoding-Categorical-Features" class="headerlink" title="Encoding Categorical Features"></a>Encoding Categorical Features</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>enc = LabelEncoder()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = enc.fit_transform(y)</span><br></pre></td></tr></table></figure>

<h4 id="Generating-Polynomial-Features"><a href="#Generating-Polynomial-Features" class="headerlink" title="Generating Polynomial Features"></a>Generating Polynomial Features</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>poly = PolynomialFeatures(<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>poly.fit_transform(X)</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="Create-Your-Model"><a href="#Create-Your-Model" class="headerlink" title="Create Your Model"></a>Create Your Model</h3><h4 id="Supervised-Learning-Estimators"><a href="#Supervised-Learning-Estimators" class="headerlink" title="Supervised Learning Estimators"></a>Supervised Learning Estimators</h4><p><b>Linear Regression</b></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lr = LinearRegression(normalize=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p><b>Support Vector Machines (SVM)</b></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>svc = SVC(kernel=<span class="string">'linear'</span>)</span><br></pre></td></tr></table></figure>

<p><b>Naive Bayes</b></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>gnb = GaussianNB()</span><br></pre></td></tr></table></figure>

<p><b>KNN</b></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn <span class="keyword">import</span> neighbors</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>knn = neighbors.KNeighborsClassifier(n_neighbors=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<h4 id="Unsupervised-Learning-Estimators"><a href="#Unsupervised-Learning-Estimators" class="headerlink" title="Unsupervised Learning Estimators"></a>Unsupervised Learning Estimators</h4><p><b>Principal Component Analysis (PCA)</b></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pca = PCA(n_components=<span class="number">0.95</span>)</span><br></pre></td></tr></table></figure>

<p><b>K Means</b></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>k_means = KMeans(n_clusters=<span class="number">3</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="Model-Fitting"><a href="#Model-Fitting" class="headerlink" title="Model Fitting"></a>Model Fitting</h3><h4 id="Supervised-learning"><a href="#Supervised-learning" class="headerlink" title="Supervised learning"></a>Supervised learning</h4><p>Fit the model to the data.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>lr.fit(X, y)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>knn.fit(X_train, y_train)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>svc.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>

<h4 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h4><p>Fit the model to the data.<br>Fit to data, then transform it.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>k_means.fit(X_train)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pca_model = pca.fit_transform(X_train)</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="Prediction"><a href="#Prediction" class="headerlink" title="Prediction"></a>Prediction</h3><h4 id="Supervised-Estimators"><a href="#Supervised-Estimators" class="headerlink" title="Supervised Estimators"></a>Supervised Estimators</h4><p>Predict labels. Predict labels. Estimate probability of a label.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_pred = svc.predict(np.random.random((<span class="number">2</span>,<span class="number">5</span>)))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_pred = lr.predict(X_test)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_pred = knn.predict_proba(X_test)</span><br></pre></td></tr></table></figure>

<h4 id="Unsupervised-Estimators"><a href="#Unsupervised-Estimators" class="headerlink" title="Unsupervised Estimators"></a>Unsupervised Estimators</h4><p>Predict labels in clustering algos.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_pred = k_means.predict(X_test)</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="Evaluate-Your-Model’s-Performance"><a href="#Evaluate-Your-Model’s-Performance" class="headerlink" title="Evaluate Your Model’s Performance"></a>Evaluate Your Model’s Performance</h3><h4 id="Classification-Metrics"><a href="#Classification-Metrics" class="headerlink" title="Classification Metrics"></a>Classification Metrics</h4><p><b>Accuracy Score</b></p>
<p>Estimator score method. Metric scoring functions.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>knn.score(X_test, y_test)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>accuracy_score(y_test, y_pred)</span><br></pre></td></tr></table></figure>

<p><b>Classification Report</b></p>
<p>Precision, recall, f1-score and support.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(classification_report(y_test, y_pred))</span><br></pre></td></tr></table></figure>

<p><b>Confusion Matrix</b></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(confusion_matrix(y_test, y_pred))</span><br></pre></td></tr></table></figure>

<h4 id="Regression-Metrics"><a href="#Regression-Metrics" class="headerlink" title="Regression Metrics"></a>Regression Metrics</h4><p><b>Mean Absolute Error</b></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_true = [<span class="number">3</span>, <span class="number">-0.5</span>, <span class="number">2</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mean_absolute_error(y_true, y_pred)</span><br></pre></td></tr></table></figure>

<p><b>Mean Squared Error</b></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mean_squared_error(y_test, y_pred)</span><br></pre></td></tr></table></figure>

<p><b>R² Score</b></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r2_score(y_true, y_pred)</span><br></pre></td></tr></table></figure>

<h4 id="Clustering-Metrics"><a href="#Clustering-Metrics" class="headerlink" title="Clustering Metrics"></a>Clustering Metrics</h4><p><b>Adjusted Rand Index</b></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> adjusted_rand_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>adjusted_rand_score(y_true, y_pred)</span><br></pre></td></tr></table></figure>

<p><b>Homogeneity</b></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> homogeneity_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>homogeneity_score(y_true, y_pred)</span><br></pre></td></tr></table></figure>

<p><b>V-measure</b></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> v_measure_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>metrics.v_measure_score(y_true, y_pred)</span><br></pre></td></tr></table></figure>

<h4 id="Cross-Validation"><a href="#Cross-Validation" class="headerlink" title="Cross-Validation"></a>Cross-Validation</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(cross_val_score(knn, X_train, y_train, cv=<span class="number">4</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(cross_val_score(lr, X, y, cv=<span class="number">2</span>))</span><br></pre></td></tr></table></figure>

<h3 id="Tune-Your-Model"><a href="#Tune-Your-Model" class="headerlink" title="Tune Your Model"></a>Tune Your Model</h3><h4 id="Grid-Search"><a href="#Grid-Search" class="headerlink" title="Grid Search"></a>Grid Search</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.grid_search <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>params = &#123;<span class="string">"n_neighbors"</span>: np.arange(<span class="number">1</span>,<span class="number">3</span>), <span class="string">"metric"</span>: [<span class="string">"euclidean"</span>, <span class="string">"cityblock"</span>]&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>grid = GridSearchCV(estimator=knn, param_grid=params)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>grid.fit(X_train, y_train)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(grid.best_score_)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(grid.best_estimator_.n_neighbors)</span><br></pre></td></tr></table></figure>

<h4 id="Randomized-Parameter-Optimization"><a href="#Randomized-Parameter-Optimization" class="headerlink" title="Randomized Parameter Optimization"></a>Randomized Parameter Optimization</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.grid_search <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>params = &#123;<span class="string">"n_neighbors"</span>: range(<span class="number">1</span>,<span class="number">5</span>), <span class="string">"weights"</span>: [<span class="string">"uniform"</span>, <span class="string">"distance"</span>]&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rsearch = RandomizedSearchCV(estimator=knn,</span><br><span class="line">                                param_distributions=params,</span><br><span class="line">                                cv=<span class="number">4</span>, n_iter=<span class="number">8</span>,random_state=<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rsearch.fit(X_train, y_train)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(rsearch.best_score_)</span><br></pre></td></tr></table></figure>

    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2018/11/21/WhatIsBaggingandHowDoesItWork/" itemprop="url">What Is Bagging and How Does It Work?</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2018-11-21T18:42:41.000Z" itemprop="datePublished">Nov 21 2018</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Machine-Learning/">Machine-Learning</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            2 minutes read (About 226 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <p>&nbsp;</p>
<center>Stephen Cheng</center>


<h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p>Bagging is a technique used to reduce the variance of our predictions by combining the result of multiple classifiers modeled on different sub-samples of the same dataset. The following figure will make it clearer.</p>
<p><img src="https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2018/201811/20181121/0.png" alt=""></p>
<h3 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h3><p>The steps followed in bagging are:</p>
<p><b>1) Create Multiple DataSets</b></p>
<ul>
<li>Sampling is done with replacement on the original data and new datasets are formed.</li>
<li>The new data sets can have a fraction of the columns as well as rows, which are generally hyper-parameters in a bagging model.</li>
<li>Taking row and column fractions less than 1 helps in making robust models, less prone to overfitting.</li>
</ul>
<p><b>2) Build Multiple Classifiers</b></p>
<ul>
<li>Classifiers are built on each data set.</li>
<li>Generally the same classifier is modeled on each dataset and predictions are made.</li>
</ul>
<p><b>3) Combine Classifiers</b></p>
<ul>
<li>The predictions of all the classifiers are combined using a mean, median or mode value depending on the problem at hand.</li>
<li>The combined values are generally more robust than a single model.</li>
</ul>
<p>Note that, here the number of models built is not a hyper-parameters. Higher number of models are always better or may give similar performance than lower numbers. It can be theoretically shown that the variance of the combined predictions are reduced to 1/n (n: number of classifiers) of the original variance, under some assumptions.</p>

    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2018/10/10/WhatAreEnsembleMethodsinTreeBasedModelling/" itemprop="url">What Are Ensemble Methods in Tree Based Modelling?</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2018-10-10T19:33:00.000Z" itemprop="datePublished">Oct 10 2018</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Machine-Learning/">Machine-Learning</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            2 minutes read (About 242 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <p>&nbsp;</p>
<center>Stephen Cheng</center>


<h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p>The literary meaning of word ‘ensemble’ is group. Ensemble methods involve group of predictive models to achieve a better accuracy and model stability. Ensemble methods are known to impart supreme boost to tree based models.</p>
<h3 id="Bias-amp-Variance"><a href="#Bias-amp-Variance" class="headerlink" title="Bias &amp; Variance"></a>Bias &amp; Variance</h3><p>Like every other model, a tree based model also suffers from the plague of bias and variance. Bias means, ‘how much on an average are the predicted values different from the actual value.’ Variance means, ‘how different will the predictions of the model be at the same point if different samples are taken from the same population’.</p>
<p>You build a small tree and you will get a model with low variance and high bias. How do you manage to balance the trade off between bias and variance ?</p>
<p>Normally, as you increase the complexity of your model, you will see a reduction in prediction error due to lower bias in the model. As you continue to make your model more complex, you end up over-fitting your model and your model will start suffering from high variance.</p>
<p>A champion model should maintain a balance between these two types of errors. This is known as the trade-off management of bias-variance errors. Ensemble learning is one way to execute this trade off analysis.</p>
<p><img src="https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2018/201810/20181010/0.png" alt=""></p>
<p>Some of the commonly used ensemble methods include: Bagging, Boosting and Stacking.</p>

    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2018/08/07/DecisionTreesImplementationinRandPython/" itemprop="url">Decision Trees Implementation in R and Python</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2018-08-07T06:58:03.000Z" itemprop="datePublished">Aug 7 2018</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Machine-Learning/">Machine-Learning</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            2 minutes read (About 250 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <p>&nbsp;</p>
<center>Stephen Cheng</center>


<h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p>For R users and Python users, decision tree is quite easy to implement. Let’s quickly look at the set of codes which can get you started with this algorithm. For ease of use, I’ve shared standard codes where you’ll need to replace your dataset name and variables to get started.</p>
<h3 id="R"><a href="#R" class="headerlink" title="R"></a>R</h3><p>For R users, there are multiple packages available to implement decision tree such as ctree, rpart, tree etc.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">library</span>(rpart)</span><br><span class="line">&gt; x &lt;- cbind(x_train,y_train)</span><br><span class="line"><span class="comment"># grow tree</span></span><br><span class="line">&gt; fit &lt;- rpart(y_train ~ ., data = x,method=<span class="string">"class"</span>)</span><br><span class="line">&gt; summary(fit)</span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">&gt; predicted= predict(fit,x_test)</span><br><span class="line">&gt; <span class="keyword">library</span>(rpart)</span><br><span class="line">&gt; x &lt;- cbind(x_train,y_train)</span><br><span class="line"><span class="comment"># grow tree</span></span><br><span class="line">&gt; fit &lt;- rpart(y_train ~ ., data = x,method=<span class="string">"class"</span>)</span><br><span class="line">&gt; summary(fit)</span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">&gt; predicted= predict(fit,x_test)</span><br></pre></td></tr></table></figure>

<p>In the code above:</p>
<ul>
<li>y_train – represents dependent variable.</li>
<li>x_train – represents independent variable</li>
<li>x – represents training data.</li>
</ul>
<h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><p>For Python users, below is the code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Import necessary libraries like pandas, numpy...</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></span><br><span class="line"><span class="comment"># Create tree object</span></span><br><span class="line"><span class="comment"># for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini  </span></span><br><span class="line">model = tree.DecisionTreeClassifier(criterion=<span class="string">'gini'</span>)</span><br><span class="line"><span class="comment"># model = tree.DecisionTreeRegressor() for regression</span></span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line">model.score(X, y)</span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= model.predict(x_test)</span><br></pre></td></tr></table></figure>

    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2018/07/27/AreTreeBasedModelsBetterThanLinearModels/" itemprop="url">Are Tree Based Models Better than Linear Models?</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2018-07-27T16:44:50.000Z" itemprop="datePublished">Jul 27 2018</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Machine-Learning/">Machine-Learning</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            a minute read (About 163 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <p>&nbsp;</p>
<center>Stephen Cheng</center>


<h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p>“If I can use logistic regression for classification problems and linear regression for regression problems, why is there a need to use trees”? Many of us have this question. And, this is a valid one too.</p>
<h3 id="How-to-choose-a-proper-algorithms"><a href="#How-to-choose-a-proper-algorithms" class="headerlink" title="How to choose a proper algorithms?"></a>How to choose a proper algorithms?</h3><p>Actually, you can use any algorithm. It is dependent on the type of problem you are solving. Let’s look at some key factors which will help you to decide which algorithm to use:</p>
<p>1) If the relationship between dependent &amp; independent variable is well approximated by a linear model, linear regression will outperform tree based model.</p>
<p>2) If there is a high non-linearity &amp; complex relationship between dependent &amp; independent variables, a tree model will outperform a classical regression method.</p>
<p>3) If you need to build a model which is easy to explain to people, a decision tree model will always do better than a linear model. Decision tree models are even simpler to interpret than linear regression!</p>

    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2018/06/06/WhatAreTheKeyParametersofTreeModellingandHowtoAvoidOver-fittinginDecisionTrees/" itemprop="url">What Are The Key Parameters of Tree Modelling and How to Avoid Over-fitting in Decision Trees?</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2018-06-06T14:37:33.000Z" itemprop="datePublished">Jun 6 2018</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Machine-Learning/">Machine-Learning</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            5 minutes read (About 811 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <p>&nbsp;</p>
<center>Stephen Cheng</center>


<h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p>Overfitting is one of the key challenges faced while modeling decision trees. If there is no limit set of a decision tree, it will give you 100% accuracy on training set because in the worse case it will end up making 1 leaf for each observation. Thus, preventing overfitting is pivotal while modeling a decision tree and it can be done in 2 ways:</p>
<p>1) Setting constraints on tree size.<br>2) Tree pruning.</p>
<p>Let’s discuss both of these briefly.</p>
<h3 id="Setting-Constraints-on-Tree-Size"><a href="#Setting-Constraints-on-Tree-Size" class="headerlink" title="Setting Constraints on Tree Size"></a>Setting Constraints on Tree Size</h3><p>This can be done by using various parameters which are used to define a tree. First, let‘s look at the general structure of a decision tree.</p>
<p><img src="https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2018/201806/20180606/0.png" alt=""></p>
<p>The parameters used for defining a tree are further explained below. The parameters described below are irrespective of tool. It is important to understand the role of parameters used in tree modeling. These parameters are available in R &amp; Python.</p>
<h4><center>Minimum samples for a node split</center></h4>

<p>1) Defines the minimum number of samples (or observations) which are required in a node to be considered for splitting.<br>2) Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.<br>3) Too high values can lead to under-fitting hence, it should be tuned using CV.</p>
<h4><center>Minimum samples for a terminal node (leaf)</center></h4>

<p>1) Defines the minimum samples (or observations) required in a terminal node or leaf.<br>2) Used to control over-fitting similar to min_samples_split.<br>3) Generally lower values should be chosen for imbalanced class problems because the regions in which the minority class will be in majority will be very small.</p>
<h4><center>Maximum depth of tree (vertical depth)</center></h4>

<p>1) The maximum depth of a tree.<br>2) Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.<br>3) Should be tuned using CV.</p>
<h4><center>Maximum number of terminal nodes</center></h4>

<p>1) The maximum number of terminal nodes or leaves in a tree.<br>2) Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.</p>
<h4><center>Maximum features to consider for split</center></h4>

<p>1) The number of features to consider while searching for a best split. These will be randomly selected.<br>2) As a thumb-rule, square root of the total number of features works great but we should check up to 30-40% of the total number of features.<br>3) Higher values can lead to over-fitting but depends on case to case.</p>
<h3 id="Tree-Pruning"><a href="#Tree-Pruning" class="headerlink" title="Tree Pruning"></a>Tree Pruning</h3><p>As discussed earlier, the technique of setting constraint is a greedy-approach. In other words, it will check for the best split instantaneously and move forward until one of the specified stopping condition is reached. Let’s consider the following case when you’re driving.</p>
<p><img src="https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2018/201806/20180606/1.png" alt=""></p>
<p>There are 2 lanes:</p>
<p>1) A lane with cars moving at 80km/h.<br>2) A lane with trucks moving at 30km/h.</p>
<p>At this instant, you are the yellow car and you have 2 choices:</p>
<p>1) Take a left and overtake the other 2 cars quickly.<br>2) Keep moving in the present lane.</p>
<p>Let’s analyze these choice. In the former choice, you’ll immediately overtake the car ahead and reach behind the truck and start moving at 30 km/h, looking for an opportunity to move back right. All cars originally behind you move ahead in the meanwhile. This would be the optimum choice if your objective is to maximize the distance covered in next say 10 seconds. In the later choice, you sale through at same speed, cross trucks and then overtake maybe depending on situation ahead. Greedy you!</p>
<p>This is exactly the difference between normal decision tree &amp; pruning. A decision tree with constraints won’t see the truck ahead and adopt a greedy approach by taking a left. On the other hand if we use pruning, we in effect look at a few steps ahead and make a choice.</p>
<h4><center>How to prune?</center></h4>

<p>So we know pruning is better. But how to implement it in decision tree? The idea is simple.</p>
<p>1) We first make the decision tree to a large depth.<br>2) Then we start at the bottom and start removing leaves which are giving us negative returns when compared from the top.<br>3) Suppose a split is giving us a gain of say -10 (loss of 10) and then the next split on that gives us a gain of 20. A simple decision tree will stop at step 1 but in pruning, we will see that the overall gain is +10 and keep both leaves.</p>
<h4><center>Note</center></h4>

<p>Note that sklearn’s decision tree classifier does not currently support pruning. Advanced packages like xgboost have adopted tree pruning in their implementation. But the library rpart in R, provides a function to prune. Good for R users!</p>

    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2018/05/06/RegressionTreesvs-ClassificationTrees/" itemprop="url">Regression Trees vs. Classification Trees</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2018-05-06T13:52:39.000Z" itemprop="datePublished">May 6 2018</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Machine-Learning/">Machine-Learning</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            3 minutes read (About 382 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <p>&nbsp;</p>
<center>Stephen Cheng</center>


<h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p>We all know that the terminal nodes (or leaves) lies at the bottom of the decision tree. This means that decision trees are typically drawn upside down such that leaves are the bottom &amp; roots are the tops (shown below).</p>
<p><img src="https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2018/201805/20180506/0.png" alt=""></p>
<p>Both the trees work almost similar to each other, let’s look at the primary differences &amp; similarity between classification and regression trees:</p>
<p>1) Regression trees are used when dependent variable is continuous. Classification trees are used when dependent variable is categorical.</p>
<p>2) In case of regression tree, the value obtained by terminal nodes in the training data is the mean response of observation falling in that region. Thus, if an unseen data observation falls in that region, we’ll make its prediction with mean value.</p>
<p>3) In case of classification tree, the value (class) obtained by terminal node in the training data is the mode of observations falling in that region. Thus, if an unseen data observation falls in that region, we’ll make its prediction with mode value.</p>
<p>4) Both the trees divide the predictor space (independent variables) into distinct and non-overlapping regions. For the sake of simplicity, you can think of these regions as high dimensional boxes or boxes.</p>
<p>5) Both the trees follow a top-down greedy approach known as recursive binary splitting. We call it as ‘top-down’ because it begins from the top of tree when all the observations are available in a single region and successively splits the predictor space into two new branches down the tree. It is known as ‘greedy’ because, the algorithm cares (looks for best variable available) about only the current split, and not about future splits which will lead to a better tree.</p>
<p>6) This splitting process is continued until a user defined stopping criteria is reached. For example: we can tell the algorithm to stop once the number of observations per node becomes less than 50.</p>
<p>7) In both the cases, the splitting process results in fully grown trees until the stopping criteria is reached. But, the fully grown tree is likely to overfit data, leading to poor accuracy on unseen data. This bring ‘pruning’. Pruning is one of the technique used tackle overfitting.</p>

    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2018/04/15/WhatIsADecisionTreeHowDoesItWork/" itemprop="url">What Is A Decision Tree ? How Does It Work ?</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2018-04-15T22:09:03.000Z" itemprop="datePublished">Apr 15 2018</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Machine-Learning/">Machine-Learning</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            6 minutes read (About 859 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <p>&nbsp;</p>
<center>Stephen Cheng</center>


<h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p>Decision tree is a type of supervised learning algorithm (having a pre-defined target variable) that is mostly used in classification problems. It works for both categorical and continuous input and output variables. In this technique, the population or sample are split into two or more homogeneous sets (or sub-populations) based on most significant splitter differentiator in input variables.</p>
<h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>Say we have a sample of 30 students with three variables: Gender (Boy/Girl), Class(IX/X) and Height (5 to 6 ft). 15 out of these 30 play cricket in leisure time. Now, we want to create a model to predict who will play cricket during leisure period? In this problem, we need to segregate students who play cricket in their leisure time based on highly significant input variable among all three.</p>
<p>This is where decision tree helps, it will segregate the students based on all values of three variables and identify the variable, which creates the best homogeneous sets of students (which are heterogeneous to each other). In the snapshot below, you can see that variable Gender is able to identify best homogeneous sets compared to the other two variables.</p>
<p><img src="https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2018/201804/20180415/0.png" alt=""></p>
<p>As mentioned above, decision tree identifies the most significant variable and its value that gives best homogeneous sets of population. Now the question which arises is, how does it identify the variable and the split? To do this, decision tree uses various algorithms, which we will shall discuss in the following section.</p>
<h3 id="Types-of-Decision-Trees"><a href="#Types-of-Decision-Trees" class="headerlink" title="Types of Decision Trees"></a>Types of Decision Trees</h3><p>Types of decision tree are based on the type of target variable we have. It can be of two types:</p>
<p><b>1) Categorical Variable Decision Tree</b></p>
<p>This type of Decision Tree which has categorical target variable then it called as categorical variable decision tree. E.g., in above scenario of student problem, where the target variable was “Student will play cricket or not” i.e. YES or NO.</p>
<p><b>2) Continuous Variable Decision Tree</b></p>
<p>This type of Decision Tree has continuous target variable then it is called as Continuous Variable Decision Tree.</p>
<h3 id="Example-1"><a href="#Example-1" class="headerlink" title="Example"></a>Example</h3><p>Let’s say we have a problem to predict whether a customer will pay his renewal premium with an insurance company (yes/no). Here we know that income of a customer is a significant variable but insurance company does not have income details for all customers. Now, as we know this is an important variable, then we can build a decision tree to predict customer income based on occupation, age and various other variables. In this case, we are predicting values for continuous variable.</p>
<h3 id="Important-Terminology"><a href="#Important-Terminology" class="headerlink" title="Important Terminology"></a>Important Terminology</h3><p>Let’s look at the basic terminology used with Decision trees:</p>
<p><b>1) Root Node</b></p>
<p>It represents entire population or sample and this further gets divided into two or more homogeneous sets.</p>
<p><b>2) Splitting</b></p>
<p>It is a process of dividing a node into two or more sub-nodes.</p>
<p><b>3) Decision Node</b></p>
<p>When a sub-node splits into further sub-nodes, then it is called decision node.</p>
<p><b>4) Leaf/Terminal Node</b></p>
<p>Nodes do not split is called Leaf or Terminal node.</p>
<p><b>5) Pruning</b><br>When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.</p>
<p><b>6) Branch/Sub-Tree</b></p>
<p>A sub section of entire tree is called branch or sub-tree.</p>
<p><b>7) Parent and Child Node</b></p>
<p>A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node.</p>
<p><img src="https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2018/201804/20180415/1.png" alt=""></p>
<p>These above are the terms commonly used for decision trees. As we know that every algorithm has advantages and disadvantages, below are the important factors which one should know.</p>
<h3 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h3><p><b>1) Easy to Understand</b></p>
<p>Decision tree output is very easy to understand even for people from non-analytical background. It does not require any statistical knowledge to read and interpret them. Its graphical representation is very intuitive and users can easily relate their hypothesis.</p>
<p><b>2) Useful in Data Exploration</b></p>
<p>Decision tree is one of the fastest way to identify most significant variables and relation between two or more variables. With the help of decision trees, we can create new variables/features that has better power to predict target variable. For example, we are working on a problem where we have information available in hundreds of variables, there decision tree will help to identify most significant variable.</p>
<p><b>3) Less Data Cleaning Required</b></p>
<p>It requires less data cleaning compared to some other modeling techniques. It is not influenced by outliers and missing values to a fair degree.</p>
<p><b>4) Data Type Is Not a Constraint</b></p>
<p>It can handle both numerical and categorical variables.</p>
<p><b>5) Non Parametric Method</b></p>
<p>Decision tree is considered to be a non-parametric method. This means that decision trees have no assumptions about the space distribution and the classifier structure.</p>
<h3 id="Disadvantages"><a href="#Disadvantages" class="headerlink" title="Disadvantages"></a>Disadvantages</h3><p><b>1) Over Fitting</b></p>
<p>Over fitting is one of the most practical difficulty for decision tree models. This problem gets solved by setting constraints on model parameters and pruning (discussed in detailed below).</p>
<p><b>2) Not Fit for Continuous Variables</b></p>
<p>While working with continuous numerical variables, decision tree looses information when it categorizes variables in different categories.</p>

    
    </div>
    
    
</article>




    
    
    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2023 Stephen Cheng&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a
                        href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            
        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/" target="_blank" rel="noopener">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("en-AU");
</script>




<script src="/js/script.js"></script>


    
</body>
</html>