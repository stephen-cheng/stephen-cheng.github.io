<!doctype html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;center&gt;Stephen Cheng&lt;/center&gt;


&lt;h3 id=&#34;Intro&#34;&gt;&lt;a href=&#34;#Intro&#34; class=&#34;headerlink&#34; title=&#34;Intro&#34;&gt;&lt;/a&gt;Intro&lt;/h3&gt;&lt;p&gt;Overfitting is one of the key challenges faced while modeling decision ">
  <meta name="viewport" content="width=device-width, height=device-height, initial-scale=1.0, user-scalable=yes">
  <title>What Are The Key Parameters of Tree Modelling and How to Avoid Over-fitting in Decision Trees? | Stephen Cheng</title>

  <link rel="stylesheet" id="chosen-theme" href="https://rawgit.com/fiatjaf/classless/master/themes/plain/theme.css">
  <script>
let link = document.getElementById('chosen-theme')
let widget = document.createElement('div')
widget.style.position = 'absolute'
widget.style.right = '5px'
widget.style.top = '2px'
widget.style.background = 'beige'
widget.style.color = '#444'
widget.style.zIndex = 99
widget.style.padding = '4px 8px'
widget.innerHTML = `
<label>
  <p>No theme was set on <code>theme_config</code>.<br>
     Choose a theme from the list to experiment with it:</p>
  <select></select>
</label>
`
fetch('https://api.github.com/repos/fiatjaf/classless/contents/themes')
  .then(r => r.json())
  .then(files => {
    document.body.appendChild(widget)
    let select = document.querySelector('select')
    files
      .filter(f => f.type === 'dir')
      .forEach(f => {
        let option = document.createElement('option')
        option.value = f.name
        option.innerHTML = f.name
        select.appendChild(option)
      })
    let options = Array.from(select.querySelectorAll('option'))
    let chosen = options[parseInt(Math.random() * options.length)]
    chosen.selected = true
    link.href = link.href.replace(/themes\/[^\/]+/, `themes/${chosen.value}`)
    select.addEventListener('change', e => {
      let chosen = e.target.value
      link.href = link.href.replace(/themes\/[^\/]+/, `themes/${chosen}`)
    })
  })
  .catch(console.log)
  </script>

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
  <header role="banner">
      <a href="/">
        
          <img src="https://picsum.photos/640/480">
        
      </a>  
    <h1>
      <a href="/">Stephen Cheng</a>
    </h1>
    <aside>
      <p></p>
    </aside>
  </header>
  
  <main>
    <article>
  <header>
  
  <h1><a href="https://stephen-cheng.github.io/2018/06/06/WhatAreTheKeyParametersofTreeModellingandHowtoAvoidOver-fittinginDecisionTrees/">What Are The Key Parameters of Tree Modelling and How to Avoid Over-fitting in Decision Trees?</a></h1>
  <aside>
    
    <time datetime="182018-06-06">
      June 6th 2018
    </time>
    
    <ul>
    
      <li><a href="/tags/Tree/">Tree</a></li>
    
      <li><a href="/tags/Overfitting/">Overfitting</a></li>
    
    </ul>
  </aside>
</header>

  <div><p>&nbsp;</p>
<center>Stephen Cheng</center>


<h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p>Overfitting is one of the key challenges faced while modeling decision trees. If there is no limit set of a decision tree, it will give you 100% accuracy on training set because in the worse case it will end up making 1 leaf for each observation. Thus, preventing overfitting is pivotal while modeling a decision tree and it can be done in 2 ways:</p>
<p>1) Setting constraints on tree size.<br>2) Tree pruning.</p>
<p>Let’s discuss both of these briefly.</p>
<h3 id="Setting-Constraints-on-Tree-Size"><a href="#Setting-Constraints-on-Tree-Size" class="headerlink" title="Setting Constraints on Tree Size"></a>Setting Constraints on Tree Size</h3><p>This can be done by using various parameters which are used to define a tree. First, let‘s look at the general structure of a decision tree.</p>
<p><img src="https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2018/201806/20180606/0.png" alt=""></p>
<p>The parameters used for defining a tree are further explained below. The parameters described below are irrespective of tool. It is important to understand the role of parameters used in tree modeling. These parameters are available in R &amp; Python.</p>
<h4><center>Minimum samples for a node split</center></h4>

<p>1) Defines the minimum number of samples (or observations) which are required in a node to be considered for splitting.<br>2) Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.<br>3) Too high values can lead to under-fitting hence, it should be tuned using CV.</p>
<h4><center>Minimum samples for a terminal node (leaf)</center></h4>

<p>1) Defines the minimum samples (or observations) required in a terminal node or leaf.<br>2) Used to control over-fitting similar to min_samples_split.<br>3) Generally lower values should be chosen for imbalanced class problems because the regions in which the minority class will be in majority will be very small.</p>
<h4><center>Maximum depth of tree (vertical depth)</center></h4>

<p>1) The maximum depth of a tree.<br>2) Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.<br>3) Should be tuned using CV.</p>
<h4><center>Maximum number of terminal nodes</center></h4>

<p>1) The maximum number of terminal nodes or leaves in a tree.<br>2) Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.</p>
<h4><center>Maximum features to consider for split</center></h4>

<p>1) The number of features to consider while searching for a best split. These will be randomly selected.<br>2) As a thumb-rule, square root of the total number of features works great but we should check up to 30-40% of the total number of features.<br>3) Higher values can lead to over-fitting but depends on case to case.</p>
<h3 id="Tree-Pruning"><a href="#Tree-Pruning" class="headerlink" title="Tree Pruning"></a>Tree Pruning</h3><p>As discussed earlier, the technique of setting constraint is a greedy-approach. In other words, it will check for the best split instantaneously and move forward until one of the specified stopping condition is reached. Let’s consider the following case when you’re driving.</p>
<p><img src="https://raw.githubusercontent.com/steven-cheng-com/images/master/blog/2018/201806/20180606/1.png" alt=""></p>
<p>There are 2 lanes:</p>
<p>1) A lane with cars moving at 80km/h.<br>2) A lane with trucks moving at 30km/h.</p>
<p>At this instant, you are the yellow car and you have 2 choices:</p>
<p>1) Take a left and overtake the other 2 cars quickly.<br>2) Keep moving in the present lane.</p>
<p>Let’s analyze these choice. In the former choice, you’ll immediately overtake the car ahead and reach behind the truck and start moving at 30 km/h, looking for an opportunity to move back right. All cars originally behind you move ahead in the meanwhile. This would be the optimum choice if your objective is to maximize the distance covered in next say 10 seconds. In the later choice, you sale through at same speed, cross trucks and then overtake maybe depending on situation ahead. Greedy you!</p>
<p>This is exactly the difference between normal decision tree &amp; pruning. A decision tree with constraints won’t see the truck ahead and adopt a greedy approach by taking a left. On the other hand if we use pruning, we in effect look at a few steps ahead and make a choice.</p>
<h4><center>How to prune?</center></h4>

<p>So we know pruning is better. But how to implement it in decision tree? The idea is simple.</p>
<p>1) We first make the decision tree to a large depth.<br>2) Then we start at the bottom and start removing leaves which are giving us negative returns when compared from the top.<br>3) Suppose a split is giving us a gain of say -10 (loss of 10) and then the next split on that gives us a gain of 20. A simple decision tree will stop at step 1 but in pruning, we will see that the overall gain is +10 and keep both leaves.</p>
<h4><center>Note</center></h4>

<p>Note that sklearn’s decision tree classifier does not currently support pruning. Advanced packages like xgboost have adopted tree pruning in their implementation. But the library rpart in R, provides a function to prune. Good for R users!</p>
</div>
</article>

  </main>
  <aside>
    <p>This is an `aside`, a useful place to put information of any kind: metadata about the site author (with or without pictures), brief information about the purposes of the site, a static collection of external links or anything else.</p>
<p>For some Classless themes pictures and data about the author may suit better in the `cover` and `description` site attributes (see `config.toml`), but most of the times that kind of information will fit here better.</p>

  </aside>
  <footer role="contentinfo">
    <p>A default site footer. If you don't know what to place here maybe you should just write your name plus the current year?</p>
<p>If you want to have a sitemap, a contact form or other complex stuff, most Classless themes will also handle it nicely. Or you can take a look at the <code>aside.html</code> partial.</p>

  </footer>
</body>
